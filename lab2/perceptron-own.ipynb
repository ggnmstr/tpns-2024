{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a56405bc-9aeb-4e37-bba2-27fe4e6ca10a",
   "metadata": {},
   "source": [
    "Многослойный персептрон\n",
    "- Реализовать архитектуру многослойного персептрона. \n",
    "- Реализовать алгоритм обратного распространения ошибки для обучения сети;\n",
    "- Обучить на очищенном датасете  из 1-ой лабораторной. \n",
    "- Реализовать перцептрон - регрессор; \n",
    "- Обработать датасет с классификацией грибов по алгоритму из 1-й лабораторной работы.\n",
    "- Обучить перцептрон на датасете с классификацией грибов. \n",
    "- Реализовать классификатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6cdaa85-bc35-4981-85ce-165e88654b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import sklearn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac7b77-41a7-46f6-9350-b5479c8b6cc8",
   "metadata": {},
   "source": [
    "# Перцептрон - Регрессор (ноутбуки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da8fdb5c-7cd5-467c-a8f9-5bfad3210631",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = pd.read_csv(\"./Laptop_price.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943363bb-3979-442a-b2e6-8e2245014da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Brand             1000 non-null   object \n",
      " 1   Processor_Speed   1000 non-null   float64\n",
      " 2   RAM_Size          1000 non-null   int64  \n",
      " 3   Storage_Capacity  1000 non-null   int64  \n",
      " 4   Screen_Size       1000 non-null   float64\n",
      " 5   Weight            1000 non-null   float64\n",
      " 6   Price             1000 non-null   float64\n",
      "dtypes: float64(4), int64(2), object(1)\n",
      "memory usage: 54.8+ KB\n"
     ]
    }
   ],
   "source": [
    "ldf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2daf3433-3b0b-4f5f-b3b9-961db7fcb548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Asus', 'Acer', 'Lenovo', 'HP', 'Dell'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldf['Brand'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f7d1438-c82b-47fb-9de9-5ace3d1c65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = pd.get_dummies(ldf,columns=['Brand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "399b3e09-b33c-42eb-9ccf-d7b5a6944fa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processor_Speed</th>\n",
       "      <th>RAM_Size</th>\n",
       "      <th>Storage_Capacity</th>\n",
       "      <th>Screen_Size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Price</th>\n",
       "      <th>Brand_Acer</th>\n",
       "      <th>Brand_Asus</th>\n",
       "      <th>Brand_Dell</th>\n",
       "      <th>Brand_HP</th>\n",
       "      <th>Brand_Lenovo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.830296</td>\n",
       "      <td>16</td>\n",
       "      <td>512</td>\n",
       "      <td>11.185147</td>\n",
       "      <td>2.641094</td>\n",
       "      <td>17395.093065</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.912833</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>11.311372</td>\n",
       "      <td>3.260012</td>\n",
       "      <td>31607.605919</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.241627</td>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>11.853023</td>\n",
       "      <td>2.029061</td>\n",
       "      <td>9291.023542</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.806248</td>\n",
       "      <td>16</td>\n",
       "      <td>512</td>\n",
       "      <td>12.280360</td>\n",
       "      <td>4.573865</td>\n",
       "      <td>17436.728334</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.268097</td>\n",
       "      <td>32</td>\n",
       "      <td>1000</td>\n",
       "      <td>14.990877</td>\n",
       "      <td>4.193472</td>\n",
       "      <td>32917.990718</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Processor_Speed  RAM_Size  Storage_Capacity  Screen_Size    Weight  \\\n",
       "0         3.830296        16               512    11.185147  2.641094   \n",
       "1         2.912833         4              1000    11.311372  3.260012   \n",
       "2         3.241627         4               256    11.853023  2.029061   \n",
       "3         3.806248        16               512    12.280360  4.573865   \n",
       "4         3.268097        32              1000    14.990877  4.193472   \n",
       "\n",
       "          Price  Brand_Acer  Brand_Asus  Brand_Dell  Brand_HP  Brand_Lenovo  \n",
       "0  17395.093065       False        True       False     False         False  \n",
       "1  31607.605919        True       False       False     False         False  \n",
       "2   9291.023542       False       False       False     False          True  \n",
       "3  17436.728334        True       False       False     False         False  \n",
       "4  32917.990718        True       False       False     False         False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25ddaf36-346f-496b-b0b7-1bb2fee82cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ldf.drop(columns=['Price']).values\n",
    "y = ldf['Price'].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b9ebc8f-172d-4038-b14e-859ac43bb4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dfda518-024a-49bd-af32-686cf5341ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "401cd100-3cbf-4390-9820-0a62b9e36b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "#X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "#y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "#y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29cdc1c-ac5a-467f-8904-6582c56f8438",
   "metadata": {},
   "source": [
    "## Своя модель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bcc43c-07d4-44a3-b2e4-00ee77f3a86a",
   "metadata": {},
   "source": [
    "### Код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b6d5c7e-04c3-47dc-bb3a-22cdd7da5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Линейный слой\n",
    "class Linear:\n",
    "  def __init__(self, input_size=None, output_size=None):\n",
    "    self.X = None\n",
    "    self.input_size = input_size\n",
    "    self.output_size = output_size\n",
    "    self.W = np.random.randn(input_size, output_size)\n",
    "    self.bias = np.zeros((1, output_size))\n",
    "\n",
    "  def __call__(self, X):\n",
    "    self.X = X\n",
    "    result = np.dot(X, self.W) + self.bias\n",
    "    return result\n",
    "\n",
    "  def prop(self, grad_prev, learning_rate):\n",
    "    grad_X = np.dot(grad_prev, self.W.T)\n",
    "    grad_W = np.dot(self.X.T, grad_prev)\n",
    "    grad_bias = np.sum(grad_prev, axis=0)\n",
    "\n",
    "    self.W -= learning_rate * grad_W\n",
    "    self.bias -= learning_rate * grad_bias\n",
    "    return grad_X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24411111-261d-4c9e-9e3a-e599c9dee4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция активации\n",
    "class Sigmoid:\n",
    "  def __init__(self):\n",
    "    self.y = None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.y = self.sigmoid(x)\n",
    "    return self.y\n",
    "\n",
    "\n",
    "  def sigmoid(self,x):\n",
    "      return 1. / (1. + np.exp(-x))\n",
    "    \n",
    "  def sigmoid_deriv(self, x):\n",
    "      return x * (1 - x)\n",
    "    \n",
    "  def prop(self, grad_prev, learning_rate):\n",
    "    return grad_prev * self.sigmoid_deriv(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da8332b7-e371-4138-9392-7bac587664e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Перцептрон \n",
    "class MLP:\n",
    "  def __init__(self, layers, max_iter=100, batch_size=100, learning_rate=0.001):\n",
    "    self.layers = layers\n",
    "    self.max_iter = max_iter\n",
    "    self.batch_size = batch_size\n",
    "    self.learning_rate = learning_rate\n",
    "\n",
    "  @staticmethod\n",
    "  def score(y_true, y_pred):\n",
    "    if np.sum((y_true - np.mean(y_true)) ** 2) == 0:\n",
    "      return 1\n",
    "    return 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "\n",
    "\n",
    "  def predict(self, input_data):\n",
    "    prev_data = input_data\n",
    "    for layer in self.layers:\n",
    "      prev_data = layer(prev_data)\n",
    "    return prev_data\n",
    "\n",
    "  def _prop(self, grad_prev, learning_rate):\n",
    "    for layer in reversed(self.layers):\n",
    "      grad_prev = layer.prop(grad_prev, learning_rate)\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    for iter in range(self.max_iter):\n",
    "      permutation = np.random.permutation(len(X))\n",
    "      X_shuffled = X[permutation]\n",
    "      y_shuffled = y[permutation]\n",
    "\n",
    "      total_loss = 0.\n",
    "      for batch_first in range(0, len(X), self.batch_size):\n",
    "        batch_last = min(batch_first + self.batch_size, len(X))\n",
    "\n",
    "        X_batch = X_shuffled[batch_first:batch_last]\n",
    "        y_batch = y_shuffled[batch_first:batch_last]\n",
    "\n",
    "        y_pred = self.predict(X_batch)\n",
    "        total_loss += self._loss(y_batch, y_pred)\n",
    "        grad_prev = self._grad_loss(y_batch, y_pred)\n",
    "        self._prop(grad_prev, self.learning_rate)\n",
    "\n",
    "      print(f'iteration: {iter}, loss: {total_loss}')\n",
    "\n",
    "  @staticmethod\n",
    "  def _loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "  @staticmethod\n",
    "  def _grad_loss(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28ef3cdc-8040-4aa8-b9f2-0a3ac4af5959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "  def __init__(self):\n",
    "    self.mean = None\n",
    "    self.std = None\n",
    "\n",
    "  def fit_transform(self, data):\n",
    "    self.mean = np.mean(data, axis=0)\n",
    "    self.std = np.std(data, axis=0)\n",
    "    return (data - self.mean) / self.std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1f1d20-e048-495c-a401-5b89cd4ee544",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7e3a798-f4dd-41e2-a370-0d3e96ad4cd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Brand  Processor_Speed  RAM_Size  Storage_Capacity  Screen_Size    Weight  \\\n",
      "0    Asus         3.830296        16               512    11.185147  2.641094   \n",
      "1    Acer         2.912833         4              1000    11.311372  3.260012   \n",
      "2  Lenovo         3.241627         4               256    11.853023  2.029061   \n",
      "3    Acer         3.806248        16               512    12.280360  4.573865   \n",
      "4    Acer         3.268097        32              1000    14.990877  4.193472   \n",
      "\n",
      "          Price  \n",
      "0  17395.093065  \n",
      "1  31607.605919  \n",
      "2   9291.023542  \n",
      "3  17436.728334  \n",
      "4  32917.990718  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Laptop_price.csv')\n",
    "print(df.head())\n",
    "\n",
    "X = df[['Storage_Capacity', 'RAM_Size', 'Processor_Speed', 'Screen_Size', 'Weight']].values\n",
    "y = df['Price'].values.reshape(-1, 1)\n",
    "\n",
    "normalizerX = Normalizer()\n",
    "normalizerY = Normalizer()\n",
    "\n",
    "X = normalizerX.fit_transform(X)\n",
    "y = normalizerY.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cea4c16-3b64-40ea-8414-5db2498a6ccf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, loss: 18.53261801973367\n",
      "iteration: 1, loss: 18.14225683722818\n",
      "iteration: 2, loss: 17.77267800837324\n",
      "iteration: 3, loss: 17.414374384555018\n",
      "iteration: 4, loss: 17.06467939219755\n",
      "iteration: 5, loss: 16.724763616430057\n",
      "iteration: 6, loss: 16.39939710333265\n",
      "iteration: 7, loss: 16.08004346274014\n",
      "iteration: 8, loss: 15.767853486999007\n",
      "iteration: 9, loss: 15.467843309711494\n",
      "iteration: 10, loss: 15.174854985461053\n",
      "iteration: 11, loss: 14.885682631227775\n",
      "iteration: 12, loss: 14.608469859720495\n",
      "iteration: 13, loss: 14.339119339916657\n",
      "iteration: 14, loss: 14.075144730968647\n",
      "iteration: 15, loss: 13.816213439751854\n",
      "iteration: 16, loss: 13.566263672124958\n",
      "iteration: 17, loss: 13.32374507715716\n",
      "iteration: 18, loss: 13.083060774684165\n",
      "iteration: 19, loss: 12.852175321189492\n",
      "iteration: 20, loss: 12.625914037829286\n",
      "iteration: 21, loss: 12.405242717242968\n",
      "iteration: 22, loss: 12.18912621784666\n",
      "iteration: 23, loss: 11.978822228120436\n",
      "iteration: 24, loss: 11.77489883852256\n",
      "iteration: 25, loss: 11.574383713869484\n",
      "iteration: 26, loss: 11.380114184668354\n",
      "iteration: 27, loss: 11.189114712316435\n",
      "iteration: 28, loss: 11.004714480436853\n",
      "iteration: 29, loss: 10.82009647011239\n",
      "iteration: 30, loss: 10.647341579843687\n",
      "iteration: 31, loss: 10.473782493449736\n",
      "iteration: 32, loss: 10.30355076928966\n",
      "iteration: 33, loss: 10.136946549756525\n",
      "iteration: 34, loss: 9.97474977142582\n",
      "iteration: 35, loss: 9.81767892787047\n",
      "iteration: 36, loss: 9.662782240469303\n",
      "iteration: 37, loss: 9.513989848346426\n",
      "iteration: 38, loss: 9.366781577801023\n",
      "iteration: 39, loss: 9.223412083780572\n",
      "iteration: 40, loss: 9.080223593993171\n",
      "iteration: 41, loss: 8.942917400927792\n",
      "iteration: 42, loss: 8.808504462466292\n",
      "iteration: 43, loss: 8.677068508497134\n",
      "iteration: 44, loss: 8.548303135330901\n",
      "iteration: 45, loss: 8.421565827534728\n",
      "iteration: 46, loss: 8.29903111136067\n",
      "iteration: 47, loss: 8.179423346572351\n",
      "iteration: 48, loss: 8.06123386815103\n",
      "iteration: 49, loss: 7.943511942176426\n",
      "iteration: 50, loss: 7.831425872799686\n",
      "iteration: 51, loss: 7.720571495786127\n",
      "iteration: 52, loss: 7.612349069283553\n",
      "iteration: 53, loss: 7.505375668833313\n",
      "iteration: 54, loss: 7.402584352848317\n",
      "iteration: 55, loss: 7.29874390562221\n",
      "iteration: 56, loss: 7.200874002465125\n",
      "iteration: 57, loss: 7.101389095525479\n",
      "iteration: 58, loss: 7.005651659659682\n",
      "iteration: 59, loss: 6.912083260033896\n",
      "iteration: 60, loss: 6.822006843822256\n",
      "iteration: 61, loss: 6.729733666646595\n",
      "iteration: 62, loss: 6.643405596701669\n",
      "iteration: 63, loss: 6.554806759088191\n",
      "iteration: 64, loss: 6.469823158158762\n",
      "iteration: 65, loss: 6.3873140130641115\n",
      "iteration: 66, loss: 6.306210094072081\n",
      "iteration: 67, loss: 6.224654300728173\n",
      "iteration: 68, loss: 6.146061040643689\n",
      "iteration: 69, loss: 6.0686855055866795\n",
      "iteration: 70, loss: 5.992994086076266\n",
      "iteration: 71, loss: 5.9180265684389415\n",
      "iteration: 72, loss: 5.844726535683562\n",
      "iteration: 73, loss: 5.773325665664213\n",
      "iteration: 74, loss: 5.704181295320364\n",
      "iteration: 75, loss: 5.634509309814712\n",
      "iteration: 76, loss: 5.567424164884408\n",
      "iteration: 77, loss: 5.499964177588636\n",
      "iteration: 78, loss: 5.4348010413160015\n",
      "iteration: 79, loss: 5.371289829257909\n",
      "iteration: 80, loss: 5.307604760208798\n",
      "iteration: 81, loss: 5.245860796420947\n",
      "iteration: 82, loss: 5.184765191722309\n",
      "iteration: 83, loss: 5.126017093975889\n",
      "iteration: 84, loss: 5.066650819541902\n",
      "iteration: 85, loss: 5.008304495894488\n",
      "iteration: 86, loss: 4.951164721857107\n",
      "iteration: 87, loss: 4.897407552749176\n",
      "iteration: 88, loss: 4.840789667337229\n",
      "iteration: 89, loss: 4.787847710970612\n",
      "iteration: 90, loss: 4.7352215259733175\n",
      "iteration: 91, loss: 4.682146684101703\n",
      "iteration: 92, loss: 4.630610345438345\n",
      "iteration: 93, loss: 4.579950588023521\n",
      "iteration: 94, loss: 4.5328519078390395\n",
      "iteration: 95, loss: 4.483207166424064\n",
      "iteration: 96, loss: 4.433353798270601\n",
      "iteration: 97, loss: 4.387452317134616\n",
      "iteration: 98, loss: 4.340279757562029\n",
      "iteration: 99, loss: 4.295856602035206\n",
      "iteration: 100, loss: 4.249243692141393\n",
      "iteration: 101, loss: 4.205336823155364\n",
      "iteration: 102, loss: 4.16172253121351\n",
      "iteration: 103, loss: 4.118940872394953\n",
      "iteration: 104, loss: 4.076134323044935\n",
      "iteration: 105, loss: 4.034155824755163\n",
      "iteration: 106, loss: 3.9934134226106233\n",
      "iteration: 107, loss: 3.9526528342169263\n",
      "iteration: 108, loss: 3.9131458154310783\n",
      "iteration: 109, loss: 3.8742386169124883\n",
      "iteration: 110, loss: 3.835396155150501\n",
      "iteration: 111, loss: 3.7974801880088584\n",
      "iteration: 112, loss: 3.760595890438265\n",
      "iteration: 113, loss: 3.722896642821504\n",
      "iteration: 114, loss: 3.6866875399880685\n",
      "iteration: 115, loss: 3.65027546058828\n",
      "iteration: 116, loss: 3.614881879146805\n",
      "iteration: 117, loss: 3.5804976746051826\n",
      "iteration: 118, loss: 3.546044051479061\n",
      "iteration: 119, loss: 3.5121745348151503\n",
      "iteration: 120, loss: 3.4785455359483834\n",
      "iteration: 121, loss: 3.4464165971504337\n",
      "iteration: 122, loss: 3.4135226703099626\n",
      "iteration: 123, loss: 3.3813129334293683\n",
      "iteration: 124, loss: 3.349534882240586\n",
      "iteration: 125, loss: 3.3186947310235055\n",
      "iteration: 126, loss: 3.2874734102791154\n",
      "iteration: 127, loss: 3.257280044922009\n",
      "iteration: 128, loss: 3.2275899888211357\n",
      "iteration: 129, loss: 3.1976752349950437\n",
      "iteration: 130, loss: 3.169192015270747\n",
      "iteration: 131, loss: 3.140218348518356\n",
      "iteration: 132, loss: 3.111497214080817\n",
      "iteration: 133, loss: 3.083541210519997\n",
      "iteration: 134, loss: 3.0556453153044765\n",
      "iteration: 135, loss: 3.0284909332496213\n",
      "iteration: 136, loss: 3.0018185765397205\n",
      "iteration: 137, loss: 2.9759898112443874\n",
      "iteration: 138, loss: 2.948973228621412\n",
      "iteration: 139, loss: 2.9225613053455786\n",
      "iteration: 140, loss: 2.8979374559480084\n",
      "iteration: 141, loss: 2.8714811915243077\n",
      "iteration: 142, loss: 2.84678711375308\n",
      "iteration: 143, loss: 2.822735841760358\n",
      "iteration: 144, loss: 2.797925710321695\n",
      "iteration: 145, loss: 2.773686062029132\n",
      "iteration: 146, loss: 2.749795788289023\n",
      "iteration: 147, loss: 2.726195977983222\n",
      "iteration: 148, loss: 2.703425949887812\n",
      "iteration: 149, loss: 2.6812941162280373\n",
      "iteration: 150, loss: 2.6576091518291123\n",
      "iteration: 151, loss: 2.635608870920202\n",
      "iteration: 152, loss: 2.6133259488282543\n",
      "iteration: 153, loss: 2.5913196693145837\n",
      "iteration: 154, loss: 2.570046135599047\n",
      "iteration: 155, loss: 2.5483359777900945\n",
      "iteration: 156, loss: 2.5271275613581956\n",
      "iteration: 157, loss: 2.5061869461623667\n",
      "iteration: 158, loss: 2.48550514347763\n",
      "iteration: 159, loss: 2.4653433735676327\n",
      "iteration: 160, loss: 2.4453737802868343\n",
      "iteration: 161, loss: 2.425274435515481\n",
      "iteration: 162, loss: 2.4059688792705214\n",
      "iteration: 163, loss: 2.3864282031682222\n",
      "iteration: 164, loss: 2.367124078466163\n",
      "iteration: 165, loss: 2.3479618500227075\n",
      "iteration: 166, loss: 2.3295814558262284\n",
      "iteration: 167, loss: 2.3108454640643306\n",
      "iteration: 168, loss: 2.2924648397176814\n",
      "iteration: 169, loss: 2.27445515745746\n",
      "iteration: 170, loss: 2.256687926737877\n",
      "iteration: 171, loss: 2.238019948942445\n",
      "iteration: 172, loss: 2.22047829718879\n",
      "iteration: 173, loss: 2.2030781568013476\n",
      "iteration: 174, loss: 2.1860177445846904\n",
      "iteration: 175, loss: 2.1688001929553558\n",
      "iteration: 176, loss: 2.1521796593681484\n",
      "iteration: 177, loss: 2.1353686321427237\n",
      "iteration: 178, loss: 2.1187999024050344\n",
      "iteration: 179, loss: 2.102667440440223\n",
      "iteration: 180, loss: 2.0864358589561793\n",
      "iteration: 181, loss: 2.070566353653495\n",
      "iteration: 182, loss: 2.0550696475884243\n",
      "iteration: 183, loss: 2.039098776443475\n",
      "iteration: 184, loss: 2.0235115772971124\n",
      "iteration: 185, loss: 2.0079437108567664\n",
      "iteration: 186, loss: 1.9929618472823207\n",
      "iteration: 187, loss: 1.9782062054490652\n",
      "iteration: 188, loss: 1.9634408229740745\n",
      "iteration: 189, loss: 1.9487102858747836\n",
      "iteration: 190, loss: 1.9339375741477818\n",
      "iteration: 191, loss: 1.9199352246847314\n",
      "iteration: 192, loss: 1.9054269919194868\n",
      "iteration: 193, loss: 1.8910864359975235\n",
      "iteration: 194, loss: 1.877195870016273\n",
      "iteration: 195, loss: 1.8636059761336847\n",
      "iteration: 196, loss: 1.8497795906923433\n",
      "iteration: 197, loss: 1.8365008630154682\n",
      "iteration: 198, loss: 1.8228105821761984\n",
      "iteration: 199, loss: 1.809703839873829\n",
      "iteration: 200, loss: 1.7966872590532204\n",
      "iteration: 201, loss: 1.7834521818763338\n",
      "iteration: 202, loss: 1.7707558746186176\n",
      "iteration: 203, loss: 1.7585668340911234\n",
      "iteration: 204, loss: 1.7454117919237322\n",
      "iteration: 205, loss: 1.7328022433941668\n",
      "iteration: 206, loss: 1.7206891148009436\n",
      "iteration: 207, loss: 1.7082170257440563\n",
      "iteration: 208, loss: 1.6965526533581712\n",
      "iteration: 209, loss: 1.684347656102815\n",
      "iteration: 210, loss: 1.6722859229558875\n",
      "iteration: 211, loss: 1.6605499864055633\n",
      "iteration: 212, loss: 1.648819620040929\n",
      "iteration: 213, loss: 1.6373247924348937\n",
      "iteration: 214, loss: 1.6258421059699562\n",
      "iteration: 215, loss: 1.6147086030874134\n",
      "iteration: 216, loss: 1.6035413666590963\n",
      "iteration: 217, loss: 1.5923687020002617\n",
      "iteration: 218, loss: 1.5814891638914919\n",
      "iteration: 219, loss: 1.5706163025119053\n",
      "iteration: 220, loss: 1.5597460329698218\n",
      "iteration: 221, loss: 1.5491856648412334\n",
      "iteration: 222, loss: 1.5387491295692444\n",
      "iteration: 223, loss: 1.5282250910903032\n",
      "iteration: 224, loss: 1.5179165877454603\n",
      "iteration: 225, loss: 1.5078971071616423\n",
      "iteration: 226, loss: 1.4974136012952806\n",
      "iteration: 227, loss: 1.4873807083886845\n",
      "iteration: 228, loss: 1.4777632796080913\n",
      "iteration: 229, loss: 1.4678910683747253\n",
      "iteration: 230, loss: 1.4580995307847908\n",
      "iteration: 231, loss: 1.4481611258631892\n",
      "iteration: 232, loss: 1.4388409322056843\n",
      "iteration: 233, loss: 1.429145499177628\n",
      "iteration: 234, loss: 1.4199788230400243\n",
      "iteration: 235, loss: 1.4103733425340812\n",
      "iteration: 236, loss: 1.4014950098489363\n",
      "iteration: 237, loss: 1.3922173862638088\n",
      "iteration: 238, loss: 1.383138620128881\n",
      "iteration: 239, loss: 1.3746118763283146\n",
      "iteration: 240, loss: 1.365189099951031\n",
      "iteration: 241, loss: 1.3566162588563273\n",
      "iteration: 242, loss: 1.3477715167876907\n",
      "iteration: 243, loss: 1.3393491621272322\n",
      "iteration: 244, loss: 1.3305580068038467\n",
      "iteration: 245, loss: 1.3222185887599935\n",
      "iteration: 246, loss: 1.3138032340145722\n",
      "iteration: 247, loss: 1.305929721212228\n",
      "iteration: 248, loss: 1.2974200897773946\n",
      "iteration: 249, loss: 1.2892087407927089\n",
      "iteration: 250, loss: 1.2812227963611105\n",
      "iteration: 251, loss: 1.2736178945524799\n",
      "iteration: 252, loss: 1.2654038838430501\n",
      "iteration: 253, loss: 1.2574821179907154\n",
      "iteration: 254, loss: 1.2495613443594968\n",
      "iteration: 255, loss: 1.2419901181735564\n",
      "iteration: 256, loss: 1.234340009064138\n",
      "iteration: 257, loss: 1.2268617084326447\n",
      "iteration: 258, loss: 1.219347647343515\n",
      "iteration: 259, loss: 1.2121260207562758\n",
      "iteration: 260, loss: 1.2045416610103499\n",
      "iteration: 261, loss: 1.1973450190121748\n",
      "iteration: 262, loss: 1.190134515118979\n",
      "iteration: 263, loss: 1.1831518465820563\n",
      "iteration: 264, loss: 1.1759794987916092\n",
      "iteration: 265, loss: 1.1687997198051352\n",
      "iteration: 266, loss: 1.161934351708353\n",
      "iteration: 267, loss: 1.155129327906852\n",
      "iteration: 268, loss: 1.1482849354987372\n",
      "iteration: 269, loss: 1.141907219687788\n",
      "iteration: 270, loss: 1.1349682359746713\n",
      "iteration: 271, loss: 1.1284140207720585\n",
      "iteration: 272, loss: 1.1216319094839857\n",
      "iteration: 273, loss: 1.115545412922729\n",
      "iteration: 274, loss: 1.1093967463685728\n",
      "iteration: 275, loss: 1.102522861131764\n",
      "iteration: 276, loss: 1.0960901446899087\n",
      "iteration: 277, loss: 1.0901894699131707\n",
      "iteration: 278, loss: 1.0837227786318577\n",
      "iteration: 279, loss: 1.0775306601825778\n",
      "iteration: 280, loss: 1.0716054803634494\n",
      "iteration: 281, loss: 1.065755530658279\n",
      "iteration: 282, loss: 1.059588554637133\n",
      "iteration: 283, loss: 1.053695518264618\n",
      "iteration: 284, loss: 1.0479629579341598\n",
      "iteration: 285, loss: 1.0420079888851304\n",
      "iteration: 286, loss: 1.0364485565576138\n",
      "iteration: 287, loss: 1.030518093006793\n",
      "iteration: 288, loss: 1.0251045391690106\n",
      "iteration: 289, loss: 1.0195477307831762\n",
      "iteration: 290, loss: 1.0138271500777534\n",
      "iteration: 291, loss: 1.008658085239832\n",
      "iteration: 292, loss: 1.0028717200230526\n",
      "iteration: 293, loss: 0.9975615738649193\n",
      "iteration: 294, loss: 0.9921222967939947\n",
      "iteration: 295, loss: 0.9870469458299123\n",
      "iteration: 296, loss: 0.9817385314941574\n",
      "iteration: 297, loss: 0.9765548240180355\n",
      "iteration: 298, loss: 0.9713423311418955\n",
      "iteration: 299, loss: 0.9662023090072969\n",
      "iteration: 300, loss: 0.9612854474479959\n",
      "iteration: 301, loss: 0.9561869771815493\n",
      "iteration: 302, loss: 0.9513155013342585\n",
      "iteration: 303, loss: 0.9464845129418771\n",
      "iteration: 304, loss: 0.9415366943210065\n",
      "iteration: 305, loss: 0.9369910983786252\n",
      "iteration: 306, loss: 0.9321740606483329\n",
      "iteration: 307, loss: 0.9273338921843307\n",
      "iteration: 308, loss: 0.9226794384321727\n",
      "iteration: 309, loss: 0.9179349945955821\n",
      "iteration: 310, loss: 0.9135117199912208\n",
      "iteration: 311, loss: 0.9089836645740765\n",
      "iteration: 312, loss: 0.9042813855240892\n",
      "iteration: 313, loss: 0.8998238567482193\n",
      "iteration: 314, loss: 0.8955273019819501\n",
      "iteration: 315, loss: 0.8909891434288251\n",
      "iteration: 316, loss: 0.886675536279401\n",
      "iteration: 317, loss: 0.8823967305605089\n",
      "iteration: 318, loss: 0.8781948706022753\n",
      "iteration: 319, loss: 0.8740616002228409\n",
      "iteration: 320, loss: 0.8697671208673946\n",
      "iteration: 321, loss: 0.865591839932496\n",
      "iteration: 322, loss: 0.8617388025187191\n",
      "iteration: 323, loss: 0.8574828195599875\n",
      "iteration: 324, loss: 0.8536906359571629\n",
      "iteration: 325, loss: 0.8493543780408126\n",
      "iteration: 326, loss: 0.8455966851937853\n",
      "iteration: 327, loss: 0.8416259423589498\n",
      "iteration: 328, loss: 0.8377281003934975\n",
      "iteration: 329, loss: 0.8338627395002358\n",
      "iteration: 330, loss: 0.8300000762225122\n",
      "iteration: 331, loss: 0.8263644611500158\n",
      "iteration: 332, loss: 0.8224667006772496\n",
      "iteration: 333, loss: 0.8191130407331111\n",
      "iteration: 334, loss: 0.8150749148805777\n",
      "iteration: 335, loss: 0.8113929615181485\n",
      "iteration: 336, loss: 0.8077823318940243\n",
      "iteration: 337, loss: 0.8042083000439599\n",
      "iteration: 338, loss: 0.8007318415491743\n",
      "iteration: 339, loss: 0.7972834439718118\n",
      "iteration: 340, loss: 0.7938982931616794\n",
      "iteration: 341, loss: 0.7903245687508234\n",
      "iteration: 342, loss: 0.786839391633757\n",
      "iteration: 343, loss: 0.7836653190420602\n",
      "iteration: 344, loss: 0.7801164961590108\n",
      "iteration: 345, loss: 0.7770375533635587\n",
      "iteration: 346, loss: 0.7736812225733452\n",
      "iteration: 347, loss: 0.7703165780064072\n",
      "iteration: 348, loss: 0.7671782634140427\n",
      "iteration: 349, loss: 0.7639354680224659\n",
      "iteration: 350, loss: 0.7607786878494712\n",
      "iteration: 351, loss: 0.7576221126025697\n",
      "iteration: 352, loss: 0.7545232528369539\n",
      "iteration: 353, loss: 0.751342718903657\n",
      "iteration: 354, loss: 0.7483606764628116\n",
      "iteration: 355, loss: 0.7453268645262423\n",
      "iteration: 356, loss: 0.7424293435485616\n",
      "iteration: 357, loss: 0.7392814172835007\n",
      "iteration: 358, loss: 0.7365267628779343\n",
      "iteration: 359, loss: 0.7334335945390741\n",
      "iteration: 360, loss: 0.7306145173548725\n",
      "iteration: 361, loss: 0.7277618609053609\n",
      "iteration: 362, loss: 0.7250184091498473\n",
      "iteration: 363, loss: 0.7221044128237755\n",
      "iteration: 364, loss: 0.7193504195019169\n",
      "iteration: 365, loss: 0.7165980157073019\n",
      "iteration: 366, loss: 0.7137763437155776\n",
      "iteration: 367, loss: 0.7111407953581297\n",
      "iteration: 368, loss: 0.708423385685733\n",
      "iteration: 369, loss: 0.7058008096980227\n",
      "iteration: 370, loss: 0.7031343473554714\n",
      "iteration: 371, loss: 0.7005897157728768\n",
      "iteration: 372, loss: 0.6978602205609664\n",
      "iteration: 373, loss: 0.6954212113634002\n",
      "iteration: 374, loss: 0.6928788324666184\n",
      "iteration: 375, loss: 0.6903590649075647\n",
      "iteration: 376, loss: 0.6877760442427094\n",
      "iteration: 377, loss: 0.6853683553518802\n",
      "iteration: 378, loss: 0.6830422650727006\n",
      "iteration: 379, loss: 0.6804617180835699\n",
      "iteration: 380, loss: 0.6779434575838468\n",
      "iteration: 381, loss: 0.6757449291542258\n",
      "iteration: 382, loss: 0.6734889300579107\n",
      "iteration: 383, loss: 0.6709050204273388\n",
      "iteration: 384, loss: 0.6687527756908906\n",
      "iteration: 385, loss: 0.6663505017476805\n",
      "iteration: 386, loss: 0.6640074676023532\n",
      "iteration: 387, loss: 0.6617670463535095\n",
      "iteration: 388, loss: 0.6595589959695648\n",
      "iteration: 389, loss: 0.6573281586781656\n",
      "iteration: 390, loss: 0.6550380045699746\n",
      "iteration: 391, loss: 0.6529236586917171\n",
      "iteration: 392, loss: 0.6507543251233326\n",
      "iteration: 393, loss: 0.6486706751746498\n",
      "iteration: 394, loss: 0.6465046112436567\n",
      "iteration: 395, loss: 0.6445168478786052\n",
      "iteration: 396, loss: 0.6423632159756489\n",
      "iteration: 397, loss: 0.640230726049444\n",
      "iteration: 398, loss: 0.6381626869537831\n",
      "iteration: 399, loss: 0.6361505417161246\n",
      "iteration: 400, loss: 0.6341691546422449\n",
      "iteration: 401, loss: 0.6321718391731235\n",
      "iteration: 402, loss: 0.6301504552249959\n",
      "iteration: 403, loss: 0.6282115031766461\n",
      "iteration: 404, loss: 0.6264151371918123\n",
      "iteration: 405, loss: 0.6243519427858653\n",
      "iteration: 406, loss: 0.6225641787721711\n",
      "iteration: 407, loss: 0.6204863325249149\n",
      "iteration: 408, loss: 0.6187344727730427\n",
      "iteration: 409, loss: 0.6168388429170173\n",
      "iteration: 410, loss: 0.6149273875435328\n",
      "iteration: 411, loss: 0.6131482211350582\n",
      "iteration: 412, loss: 0.6113858778969339\n",
      "iteration: 413, loss: 0.6095220951679743\n",
      "iteration: 414, loss: 0.6077228898412472\n",
      "iteration: 415, loss: 0.6060454241822432\n",
      "iteration: 416, loss: 0.6042903757572325\n",
      "iteration: 417, loss: 0.6024553059295595\n",
      "iteration: 418, loss: 0.6007611082634292\n",
      "iteration: 419, loss: 0.599124587925439\n",
      "iteration: 420, loss: 0.5974353753789703\n",
      "iteration: 421, loss: 0.5956981665768464\n",
      "iteration: 422, loss: 0.5940152648323097\n",
      "iteration: 423, loss: 0.5924158828183411\n",
      "iteration: 424, loss: 0.5909139725947727\n",
      "iteration: 425, loss: 0.5892597797366852\n",
      "iteration: 426, loss: 0.5875285592556434\n",
      "iteration: 427, loss: 0.5860177418704228\n",
      "iteration: 428, loss: 0.5844348984492989\n",
      "iteration: 429, loss: 0.5828761137823179\n",
      "iteration: 430, loss: 0.5812351676757763\n",
      "iteration: 431, loss: 0.5798762464823448\n",
      "iteration: 432, loss: 0.5781771338064307\n",
      "iteration: 433, loss: 0.5768486020151066\n",
      "iteration: 434, loss: 0.5751651783266613\n",
      "iteration: 435, loss: 0.573753525489431\n",
      "iteration: 436, loss: 0.5722103218250316\n",
      "iteration: 437, loss: 0.5708972084897772\n",
      "iteration: 438, loss: 0.5693146011224972\n",
      "iteration: 439, loss: 0.5679229380802489\n",
      "iteration: 440, loss: 0.5664434910658572\n",
      "iteration: 441, loss: 0.5650382908528799\n",
      "iteration: 442, loss: 0.5636923987876402\n",
      "iteration: 443, loss: 0.5622986289341463\n",
      "iteration: 444, loss: 0.56083548497637\n",
      "iteration: 445, loss: 0.5595824231487081\n",
      "iteration: 446, loss: 0.558135067695772\n",
      "iteration: 447, loss: 0.5568200996334663\n",
      "iteration: 448, loss: 0.5555101647519568\n",
      "iteration: 449, loss: 0.5542975666777912\n",
      "iteration: 450, loss: 0.5528334437119554\n",
      "iteration: 451, loss: 0.5516026785505632\n",
      "iteration: 452, loss: 0.5502096476676236\n",
      "iteration: 453, loss: 0.5489198654405808\n",
      "iteration: 454, loss: 0.5477044607896449\n",
      "iteration: 455, loss: 0.5464815492849967\n",
      "iteration: 456, loss: 0.545293904330214\n",
      "iteration: 457, loss: 0.5440754599307801\n",
      "iteration: 458, loss: 0.542686867974451\n",
      "iteration: 459, loss: 0.5415490175308232\n",
      "iteration: 460, loss: 0.5403327250244794\n",
      "iteration: 461, loss: 0.5390455515696841\n",
      "iteration: 462, loss: 0.5378890061059185\n",
      "iteration: 463, loss: 0.5368526283869698\n",
      "iteration: 464, loss: 0.5355515631300176\n",
      "iteration: 465, loss: 0.534373579057145\n",
      "iteration: 466, loss: 0.5332114699671036\n",
      "iteration: 467, loss: 0.5320780057669806\n",
      "iteration: 468, loss: 0.5309275383807524\n",
      "iteration: 469, loss: 0.5298322149399608\n",
      "iteration: 470, loss: 0.5286924108435693\n",
      "iteration: 471, loss: 0.5276598750496839\n",
      "iteration: 472, loss: 0.5265699776872752\n",
      "iteration: 473, loss: 0.525488375251795\n",
      "iteration: 474, loss: 0.5244413544694521\n",
      "iteration: 475, loss: 0.5233131322311616\n",
      "iteration: 476, loss: 0.5221654277599168\n",
      "iteration: 477, loss: 0.5212727815881015\n",
      "iteration: 478, loss: 0.5201956910083815\n",
      "iteration: 479, loss: 0.519025365071545\n",
      "iteration: 480, loss: 0.5180317958330473\n",
      "iteration: 481, loss: 0.5170289202797957\n",
      "iteration: 482, loss: 0.5159616676744684\n",
      "iteration: 483, loss: 0.5150089848276842\n",
      "iteration: 484, loss: 0.5139441819972937\n",
      "iteration: 485, loss: 0.5130321991280433\n",
      "iteration: 486, loss: 0.5120561913594867\n",
      "iteration: 487, loss: 0.5109695366181743\n",
      "iteration: 488, loss: 0.5100194071350641\n",
      "iteration: 489, loss: 0.509094980266195\n",
      "iteration: 490, loss: 0.5081642520720369\n",
      "iteration: 491, loss: 0.507164388962575\n",
      "iteration: 492, loss: 0.5062049856791329\n",
      "iteration: 493, loss: 0.505369831584545\n",
      "iteration: 494, loss: 0.5043314128422811\n",
      "iteration: 495, loss: 0.5034155897259815\n",
      "iteration: 496, loss: 0.5025861840522777\n",
      "iteration: 497, loss: 0.501673762878663\n",
      "iteration: 498, loss: 0.5008560644800825\n",
      "iteration: 499, loss: 0.4997957077951436\n",
      "iteration: 500, loss: 0.498968216370935\n",
      "iteration: 501, loss: 0.497984287366138\n",
      "iteration: 502, loss: 0.4971765843391787\n",
      "iteration: 503, loss: 0.49628618301997945\n",
      "iteration: 504, loss: 0.4954381302065994\n",
      "iteration: 505, loss: 0.49454483725543263\n",
      "iteration: 506, loss: 0.49368526957812053\n",
      "iteration: 507, loss: 0.4928654028540919\n",
      "iteration: 508, loss: 0.49204764702611925\n",
      "iteration: 509, loss: 0.49121231090650025\n",
      "iteration: 510, loss: 0.4905686883175042\n",
      "iteration: 511, loss: 0.48953852357225425\n",
      "iteration: 512, loss: 0.48877779747406336\n",
      "iteration: 513, loss: 0.48789349097601875\n",
      "iteration: 514, loss: 0.4872783702975163\n",
      "iteration: 515, loss: 0.48635322887111304\n",
      "iteration: 516, loss: 0.4855922317913886\n",
      "iteration: 517, loss: 0.4848825704128704\n",
      "iteration: 518, loss: 0.48402153257556924\n",
      "iteration: 519, loss: 0.4831215330997613\n",
      "iteration: 520, loss: 0.48238340427647963\n",
      "iteration: 521, loss: 0.48163939398666034\n",
      "iteration: 522, loss: 0.4808753361287417\n",
      "iteration: 523, loss: 0.48026385374866387\n",
      "iteration: 524, loss: 0.47936402401756506\n",
      "iteration: 525, loss: 0.47867018004110923\n",
      "iteration: 526, loss: 0.4778269437492915\n",
      "iteration: 527, loss: 0.47708566397966157\n",
      "iteration: 528, loss: 0.4764558751926222\n",
      "iteration: 529, loss: 0.4756120831797895\n",
      "iteration: 530, loss: 0.47510575617277634\n",
      "iteration: 531, loss: 0.474186061683073\n",
      "iteration: 532, loss: 0.4736414925977154\n",
      "iteration: 533, loss: 0.4727374917395484\n",
      "iteration: 534, loss: 0.47209158882413366\n",
      "iteration: 535, loss: 0.4714128869817021\n",
      "iteration: 536, loss: 0.4706665028773715\n",
      "iteration: 537, loss: 0.47001830753982543\n",
      "iteration: 538, loss: 0.46931973983492004\n",
      "iteration: 539, loss: 0.46866625914252963\n",
      "iteration: 540, loss: 0.46805417049244497\n",
      "iteration: 541, loss: 0.46728720690773656\n",
      "iteration: 542, loss: 0.46661595854028187\n",
      "iteration: 543, loss: 0.4661459853775745\n",
      "iteration: 544, loss: 0.4653305196940618\n",
      "iteration: 545, loss: 0.4646407501846953\n",
      "iteration: 546, loss: 0.4639598233553902\n",
      "iteration: 547, loss: 0.46336996257643437\n",
      "iteration: 548, loss: 0.4626372639024513\n",
      "iteration: 549, loss: 0.46204642608791274\n",
      "iteration: 550, loss: 0.46138127728114064\n",
      "iteration: 551, loss: 0.4607316443486307\n",
      "iteration: 552, loss: 0.46013460901390635\n",
      "iteration: 553, loss: 0.4594717194054762\n",
      "iteration: 554, loss: 0.45894033786144917\n",
      "iteration: 555, loss: 0.4582465841355106\n",
      "iteration: 556, loss: 0.45764178098451364\n",
      "iteration: 557, loss: 0.45711249896407063\n",
      "iteration: 558, loss: 0.4564602649834184\n",
      "iteration: 559, loss: 0.45585960037384465\n",
      "iteration: 560, loss: 0.4551970023219454\n",
      "iteration: 561, loss: 0.4547093511188255\n",
      "iteration: 562, loss: 0.454065105723628\n",
      "iteration: 563, loss: 0.4534424561358285\n",
      "iteration: 564, loss: 0.452836164744063\n",
      "iteration: 565, loss: 0.45234783611151475\n",
      "iteration: 566, loss: 0.45185531414787106\n",
      "iteration: 567, loss: 0.4511369281138937\n",
      "iteration: 568, loss: 0.4506037738318318\n",
      "iteration: 569, loss: 0.4500279529540866\n",
      "iteration: 570, loss: 0.4493756759258176\n",
      "iteration: 571, loss: 0.4488365437194306\n",
      "iteration: 572, loss: 0.4482751598169046\n",
      "iteration: 573, loss: 0.44772558646073723\n",
      "iteration: 574, loss: 0.4471332065580128\n",
      "iteration: 575, loss: 0.44662748906515004\n",
      "iteration: 576, loss: 0.4461605941234617\n",
      "iteration: 577, loss: 0.44560943305847855\n",
      "iteration: 578, loss: 0.44500488027191926\n",
      "iteration: 579, loss: 0.44454650016977976\n",
      "iteration: 580, loss: 0.4439294218085577\n",
      "iteration: 581, loss: 0.443445413899625\n",
      "iteration: 582, loss: 0.44287240722005017\n",
      "iteration: 583, loss: 0.44224488622605906\n",
      "iteration: 584, loss: 0.44176609398270444\n",
      "iteration: 585, loss: 0.44125713419144097\n",
      "iteration: 586, loss: 0.4407736022948249\n",
      "iteration: 587, loss: 0.44017444301631004\n",
      "iteration: 588, loss: 0.4396149718097514\n",
      "iteration: 589, loss: 0.4392165251030257\n",
      "iteration: 590, loss: 0.4387066201121546\n",
      "iteration: 591, loss: 0.438125774562263\n",
      "iteration: 592, loss: 0.4376613244421777\n",
      "iteration: 593, loss: 0.43712014823380885\n",
      "iteration: 594, loss: 0.43662414566204977\n",
      "iteration: 595, loss: 0.4361229670447556\n",
      "iteration: 596, loss: 0.43564153747895873\n",
      "iteration: 597, loss: 0.4351469854563319\n",
      "iteration: 598, loss: 0.4346690771452098\n",
      "iteration: 599, loss: 0.43420805220743497\n",
      "iteration: 600, loss: 0.43363704510210693\n",
      "iteration: 601, loss: 0.43324793509955806\n",
      "iteration: 602, loss: 0.43276064761815325\n",
      "iteration: 603, loss: 0.4322696786490244\n",
      "iteration: 604, loss: 0.4318034557324142\n",
      "iteration: 605, loss: 0.4313529956310873\n",
      "iteration: 606, loss: 0.43083187829109687\n",
      "iteration: 607, loss: 0.4303321386321788\n",
      "iteration: 608, loss: 0.4298910917230466\n",
      "iteration: 609, loss: 0.42939777234409116\n",
      "iteration: 610, loss: 0.4289340502620647\n",
      "iteration: 611, loss: 0.42848273864006736\n",
      "iteration: 612, loss: 0.4280347565515203\n",
      "iteration: 613, loss: 0.4275819826491102\n",
      "iteration: 614, loss: 0.4271659852916269\n",
      "iteration: 615, loss: 0.42664341768998937\n",
      "iteration: 616, loss: 0.4261590898707105\n",
      "iteration: 617, loss: 0.4257249733820749\n",
      "iteration: 618, loss: 0.4252919401829254\n",
      "iteration: 619, loss: 0.4249451150955256\n",
      "iteration: 620, loss: 0.42439728227387197\n",
      "iteration: 621, loss: 0.42394546720320214\n",
      "iteration: 622, loss: 0.42360149082945675\n",
      "iteration: 623, loss: 0.42305201228819855\n",
      "iteration: 624, loss: 0.42266871101035447\n",
      "iteration: 625, loss: 0.4222112622713359\n",
      "iteration: 626, loss: 0.4217349531041392\n",
      "iteration: 627, loss: 0.42142518615707836\n",
      "iteration: 628, loss: 0.4209793546401305\n",
      "iteration: 629, loss: 0.4205123271516537\n",
      "iteration: 630, loss: 0.42010195041609355\n",
      "iteration: 631, loss: 0.4196444436364075\n",
      "iteration: 632, loss: 0.4192268260890822\n",
      "iteration: 633, loss: 0.4187803837894764\n",
      "iteration: 634, loss: 0.41835053562971813\n",
      "iteration: 635, loss: 0.41801508174211666\n",
      "iteration: 636, loss: 0.41756679661132434\n",
      "iteration: 637, loss: 0.41719398948007497\n",
      "iteration: 638, loss: 0.4166992828115544\n",
      "iteration: 639, loss: 0.41633777207610945\n",
      "iteration: 640, loss: 0.4158921835067687\n",
      "iteration: 641, loss: 0.4155483645596133\n",
      "iteration: 642, loss: 0.41510043139397074\n",
      "iteration: 643, loss: 0.4146620972209135\n",
      "iteration: 644, loss: 0.4144173084991954\n",
      "iteration: 645, loss: 0.41385530572375406\n",
      "iteration: 646, loss: 0.4134552809100076\n",
      "iteration: 647, loss: 0.41310507978126565\n",
      "iteration: 648, loss: 0.4127175010503682\n",
      "iteration: 649, loss: 0.4124019499053029\n",
      "iteration: 650, loss: 0.41190286782713614\n",
      "iteration: 651, loss: 0.4114629558619136\n",
      "iteration: 652, loss: 0.4111664838333139\n",
      "iteration: 653, loss: 0.4107452329427713\n",
      "iteration: 654, loss: 0.41034023904403555\n",
      "iteration: 655, loss: 0.4098901086846282\n",
      "iteration: 656, loss: 0.40962530987237544\n",
      "iteration: 657, loss: 0.40913590793460314\n",
      "iteration: 658, loss: 0.40878081311969394\n",
      "iteration: 659, loss: 0.4084026621495398\n",
      "iteration: 660, loss: 0.40798926482881687\n",
      "iteration: 661, loss: 0.4076209430881998\n",
      "iteration: 662, loss: 0.40727249840809165\n",
      "iteration: 663, loss: 0.4068706083368497\n",
      "iteration: 664, loss: 0.4065120859601484\n",
      "iteration: 665, loss: 0.4060805616642796\n",
      "iteration: 666, loss: 0.4057524845929688\n",
      "iteration: 667, loss: 0.4053773578370902\n",
      "iteration: 668, loss: 0.40500533409271006\n",
      "iteration: 669, loss: 0.4046642375831856\n",
      "iteration: 670, loss: 0.4043701333055939\n",
      "iteration: 671, loss: 0.4040019440016932\n",
      "iteration: 672, loss: 0.4035229158263463\n",
      "iteration: 673, loss: 0.4031789876001802\n",
      "iteration: 674, loss: 0.40284290476497\n",
      "iteration: 675, loss: 0.40246196854532\n",
      "iteration: 676, loss: 0.4021691732937798\n",
      "iteration: 677, loss: 0.4017990815650099\n",
      "iteration: 678, loss: 0.4014148356676214\n",
      "iteration: 679, loss: 0.40104386857444946\n",
      "iteration: 680, loss: 0.4006966585032743\n",
      "iteration: 681, loss: 0.40032094674167557\n",
      "iteration: 682, loss: 0.3999512693049551\n",
      "iteration: 683, loss: 0.39957290404912715\n",
      "iteration: 684, loss: 0.39927190886707564\n",
      "iteration: 685, loss: 0.39888240149203374\n",
      "iteration: 686, loss: 0.39852466349271587\n",
      "iteration: 687, loss: 0.398248004367943\n",
      "iteration: 688, loss: 0.3979047689656437\n",
      "iteration: 689, loss: 0.39747561241950197\n",
      "iteration: 690, loss: 0.3971172928867241\n",
      "iteration: 691, loss: 0.39678465880230096\n",
      "iteration: 692, loss: 0.39645884113897545\n",
      "iteration: 693, loss: 0.39619837938747127\n",
      "iteration: 694, loss: 0.3957652925953737\n",
      "iteration: 695, loss: 0.39542610895284935\n",
      "iteration: 696, loss: 0.3950752345116714\n",
      "iteration: 697, loss: 0.39478660952368\n",
      "iteration: 698, loss: 0.39440513105943753\n",
      "iteration: 699, loss: 0.39408894074095024\n",
      "iteration: 700, loss: 0.3937748643710284\n",
      "iteration: 701, loss: 0.39335988949450207\n",
      "iteration: 702, loss: 0.39306912269055083\n",
      "iteration: 703, loss: 0.392687145806842\n",
      "iteration: 704, loss: 0.392385839633531\n",
      "iteration: 705, loss: 0.39216402130038414\n",
      "iteration: 706, loss: 0.39172567731452845\n",
      "iteration: 707, loss: 0.3913875550613113\n",
      "iteration: 708, loss: 0.39111974375361436\n",
      "iteration: 709, loss: 0.39073670661671245\n",
      "iteration: 710, loss: 0.39042646964556194\n",
      "iteration: 711, loss: 0.3901619115406708\n",
      "iteration: 712, loss: 0.38984493744267545\n",
      "iteration: 713, loss: 0.38950695579790795\n",
      "iteration: 714, loss: 0.3892288389055464\n",
      "iteration: 715, loss: 0.38879755879286676\n",
      "iteration: 716, loss: 0.3884585900954462\n",
      "iteration: 717, loss: 0.38823443675311775\n",
      "iteration: 718, loss: 0.3877789007494049\n",
      "iteration: 719, loss: 0.38757122119983944\n",
      "iteration: 720, loss: 0.38729241135701487\n",
      "iteration: 721, loss: 0.38686139297750505\n",
      "iteration: 722, loss: 0.38657421960142274\n",
      "iteration: 723, loss: 0.38630358766094697\n",
      "iteration: 724, loss: 0.38587478994452046\n",
      "iteration: 725, loss: 0.38557501099576114\n",
      "iteration: 726, loss: 0.38531124654576787\n",
      "iteration: 727, loss: 0.38499122433460464\n",
      "iteration: 728, loss: 0.38468937432343725\n",
      "iteration: 729, loss: 0.3843241259426854\n",
      "iteration: 730, loss: 0.38408104029788537\n",
      "iteration: 731, loss: 0.3837248579266654\n",
      "iteration: 732, loss: 0.38339513422139815\n",
      "iteration: 733, loss: 0.38315775635185517\n",
      "iteration: 734, loss: 0.38277502336041763\n",
      "iteration: 735, loss: 0.3824700194399345\n",
      "iteration: 736, loss: 0.3822300326640419\n",
      "iteration: 737, loss: 0.3818870674680273\n",
      "iteration: 738, loss: 0.38156601791795947\n",
      "iteration: 739, loss: 0.38124913176195036\n",
      "iteration: 740, loss: 0.3810509919842525\n",
      "iteration: 741, loss: 0.38068221686163045\n",
      "iteration: 742, loss: 0.38034267877664346\n",
      "iteration: 743, loss: 0.3800694472179808\n",
      "iteration: 744, loss: 0.3797680008096716\n",
      "iteration: 745, loss: 0.3794166608495671\n",
      "iteration: 746, loss: 0.37915357256745597\n",
      "iteration: 747, loss: 0.3787980221175637\n",
      "iteration: 748, loss: 0.3785329106746864\n",
      "iteration: 749, loss: 0.37834406328872067\n",
      "iteration: 750, loss: 0.3779754466813396\n",
      "iteration: 751, loss: 0.3776703899117006\n",
      "iteration: 752, loss: 0.3773991393302135\n",
      "iteration: 753, loss: 0.3771031721312994\n",
      "iteration: 754, loss: 0.37673909851801624\n",
      "iteration: 755, loss: 0.37644422551540585\n",
      "iteration: 756, loss: 0.37621361820694893\n",
      "iteration: 757, loss: 0.37590689533221766\n",
      "iteration: 758, loss: 0.37558401189985435\n",
      "iteration: 759, loss: 0.3753484803122562\n",
      "iteration: 760, loss: 0.3750658911834273\n",
      "iteration: 761, loss: 0.3747868478605321\n",
      "iteration: 762, loss: 0.3744430551027058\n",
      "iteration: 763, loss: 0.374167050161764\n",
      "iteration: 764, loss: 0.3738327446335904\n",
      "iteration: 765, loss: 0.37351118622053325\n",
      "iteration: 766, loss: 0.37322985381138324\n",
      "iteration: 767, loss: 0.3730196878955466\n",
      "iteration: 768, loss: 0.37276221223391226\n",
      "iteration: 769, loss: 0.3723749165900659\n",
      "iteration: 770, loss: 0.3721398349206012\n",
      "iteration: 771, loss: 0.3718608566156411\n",
      "iteration: 772, loss: 0.37153392493639337\n",
      "iteration: 773, loss: 0.37132883748455386\n",
      "iteration: 774, loss: 0.3709810874018453\n",
      "iteration: 775, loss: 0.3707388918936435\n",
      "iteration: 776, loss: 0.3703840140067371\n",
      "iteration: 777, loss: 0.3701900091539294\n",
      "iteration: 778, loss: 0.3698451441843162\n",
      "iteration: 779, loss: 0.3695534370128671\n",
      "iteration: 780, loss: 0.3693984427076702\n",
      "iteration: 781, loss: 0.36902673962370225\n",
      "iteration: 782, loss: 0.3687805118421341\n",
      "iteration: 783, loss: 0.3684432204722013\n",
      "iteration: 784, loss: 0.3682149696659754\n",
      "iteration: 785, loss: 0.3678881748050553\n",
      "iteration: 786, loss: 0.36762118920177234\n",
      "iteration: 787, loss: 0.36730348390151185\n",
      "iteration: 788, loss: 0.36705790631553703\n",
      "iteration: 789, loss: 0.3668417414928591\n",
      "iteration: 790, loss: 0.3665370061367703\n",
      "iteration: 791, loss: 0.3662472789123353\n",
      "iteration: 792, loss: 0.3659866347635937\n",
      "iteration: 793, loss: 0.3656420054373095\n",
      "iteration: 794, loss: 0.3653858229284887\n",
      "iteration: 795, loss: 0.3652552634631948\n",
      "iteration: 796, loss: 0.36480711029584784\n",
      "iteration: 797, loss: 0.3646242037959634\n",
      "iteration: 798, loss: 0.3643083929178234\n",
      "iteration: 799, loss: 0.36402598090674376\n",
      "iteration: 800, loss: 0.36377551524670976\n",
      "iteration: 801, loss: 0.3635143230872205\n",
      "iteration: 802, loss: 0.36320221550809945\n",
      "iteration: 803, loss: 0.36291726503707655\n",
      "iteration: 804, loss: 0.3626937818060273\n",
      "iteration: 805, loss: 0.3624746551400275\n",
      "iteration: 806, loss: 0.36217451744754753\n",
      "iteration: 807, loss: 0.36195855857557807\n",
      "iteration: 808, loss: 0.3616236267196718\n",
      "iteration: 809, loss: 0.36148321624825386\n",
      "iteration: 810, loss: 0.3610690219974624\n",
      "iteration: 811, loss: 0.360804721496465\n",
      "iteration: 812, loss: 0.3605424923676229\n",
      "iteration: 813, loss: 0.360365403835354\n",
      "iteration: 814, loss: 0.36002735107251654\n",
      "iteration: 815, loss: 0.3597894795582261\n",
      "iteration: 816, loss: 0.3594424172444376\n",
      "iteration: 817, loss: 0.3592754387464255\n",
      "iteration: 818, loss: 0.359120224010939\n",
      "iteration: 819, loss: 0.3587074276948094\n",
      "iteration: 820, loss: 0.3584215002382869\n",
      "iteration: 821, loss: 0.3582012293598122\n",
      "iteration: 822, loss: 0.3579406823423013\n",
      "iteration: 823, loss: 0.35766094867975884\n",
      "iteration: 824, loss: 0.357452631531939\n",
      "iteration: 825, loss: 0.35710508330327995\n",
      "iteration: 826, loss: 0.35693021991519125\n",
      "iteration: 827, loss: 0.3566081142695996\n",
      "iteration: 828, loss: 0.35637738798969637\n",
      "iteration: 829, loss: 0.35605079231462367\n",
      "iteration: 830, loss: 0.3559502234847507\n",
      "iteration: 831, loss: 0.355618560010006\n",
      "iteration: 832, loss: 0.3553019710227932\n",
      "iteration: 833, loss: 0.35506424747038895\n",
      "iteration: 834, loss: 0.3547781335171977\n",
      "iteration: 835, loss: 0.3546115682576324\n",
      "iteration: 836, loss: 0.354327066800882\n",
      "iteration: 837, loss: 0.354029083558149\n",
      "iteration: 838, loss: 0.35373814289036043\n",
      "iteration: 839, loss: 0.35355354697972785\n",
      "iteration: 840, loss: 0.3532848988419262\n",
      "iteration: 841, loss: 0.35304976879865835\n",
      "iteration: 842, loss: 0.3527171237204975\n",
      "iteration: 843, loss: 0.35252772217245265\n",
      "iteration: 844, loss: 0.35221015366243774\n",
      "iteration: 845, loss: 0.35196907964592294\n",
      "iteration: 846, loss: 0.3517288673881118\n",
      "iteration: 847, loss: 0.3514937579067515\n",
      "iteration: 848, loss: 0.3512782655878826\n",
      "iteration: 849, loss: 0.35099923330085925\n",
      "iteration: 850, loss: 0.35071456606329166\n",
      "iteration: 851, loss: 0.3504960942511694\n",
      "iteration: 852, loss: 0.3502531617195084\n",
      "iteration: 853, loss: 0.34997012273602346\n",
      "iteration: 854, loss: 0.3497278387974751\n",
      "iteration: 855, loss: 0.349505888395063\n",
      "iteration: 856, loss: 0.34926481924577424\n",
      "iteration: 857, loss: 0.34902117363388707\n",
      "iteration: 858, loss: 0.34869447204509363\n",
      "iteration: 859, loss: 0.34853529817792595\n",
      "iteration: 860, loss: 0.3482375666672734\n",
      "iteration: 861, loss: 0.3479864060398459\n",
      "iteration: 862, loss: 0.34775372738446303\n",
      "iteration: 863, loss: 0.34751271879058954\n",
      "iteration: 864, loss: 0.34728070176498615\n",
      "iteration: 865, loss: 0.3470536975479682\n",
      "iteration: 866, loss: 0.3467782144094985\n",
      "iteration: 867, loss: 0.3465791142864884\n",
      "iteration: 868, loss: 0.34626858431255136\n",
      "iteration: 869, loss: 0.34606729898406896\n",
      "iteration: 870, loss: 0.3458031445095567\n",
      "iteration: 871, loss: 0.34562814307251855\n",
      "iteration: 872, loss: 0.3452612676272079\n",
      "iteration: 873, loss: 0.34508013606209187\n",
      "iteration: 874, loss: 0.3447981459832099\n",
      "iteration: 875, loss: 0.34453790362403997\n",
      "iteration: 876, loss: 0.3442973954536013\n",
      "iteration: 877, loss: 0.3440591825745443\n",
      "iteration: 878, loss: 0.34387137757898134\n",
      "iteration: 879, loss: 0.3437364060060826\n",
      "iteration: 880, loss: 0.3433929497281497\n",
      "iteration: 881, loss: 0.34313299087403204\n",
      "iteration: 882, loss: 0.3429714583475216\n",
      "iteration: 883, loss: 0.3427474424483886\n",
      "iteration: 884, loss: 0.3424335747817534\n",
      "iteration: 885, loss: 0.3421874841251444\n",
      "iteration: 886, loss: 0.34192559214186846\n",
      "iteration: 887, loss: 0.34174049847876214\n",
      "iteration: 888, loss: 0.3415438517111842\n",
      "iteration: 889, loss: 0.3412071782123296\n",
      "iteration: 890, loss: 0.3410037650384154\n",
      "iteration: 891, loss: 0.3407347618642576\n",
      "iteration: 892, loss: 0.3405277924282493\n",
      "iteration: 893, loss: 0.3402542729148615\n",
      "iteration: 894, loss: 0.34013677842976797\n",
      "iteration: 895, loss: 0.33982483337340313\n",
      "iteration: 896, loss: 0.3396151362064742\n",
      "iteration: 897, loss: 0.33940488241078093\n",
      "iteration: 898, loss: 0.3391486688325187\n",
      "iteration: 899, loss: 0.33886868665937536\n",
      "iteration: 900, loss: 0.33864596917363937\n",
      "iteration: 901, loss: 0.33838575711911045\n",
      "iteration: 902, loss: 0.3381455873459212\n",
      "iteration: 903, loss: 0.3379240682954793\n",
      "iteration: 904, loss: 0.33769061694762514\n",
      "iteration: 905, loss: 0.33748787105612943\n",
      "iteration: 906, loss: 0.337207202716729\n",
      "iteration: 907, loss: 0.33700807921971043\n",
      "iteration: 908, loss: 0.3367414572095056\n",
      "iteration: 909, loss: 0.3365710145672978\n",
      "iteration: 910, loss: 0.3363016284675009\n",
      "iteration: 911, loss: 0.33607802211013826\n",
      "iteration: 912, loss: 0.33580474962038687\n",
      "iteration: 913, loss: 0.33553890212022525\n",
      "iteration: 914, loss: 0.3353878273678896\n",
      "iteration: 915, loss: 0.33510257288204276\n",
      "iteration: 916, loss: 0.33499043071376855\n",
      "iteration: 917, loss: 0.3346992614230153\n",
      "iteration: 918, loss: 0.3344095195699368\n",
      "iteration: 919, loss: 0.3341921210309929\n",
      "iteration: 920, loss: 0.33406046230343944\n",
      "iteration: 921, loss: 0.3337280162588965\n",
      "iteration: 922, loss: 0.33350519100982634\n",
      "iteration: 923, loss: 0.3332981592584891\n",
      "iteration: 924, loss: 0.3330848895854766\n",
      "iteration: 925, loss: 0.3328718476218314\n",
      "iteration: 926, loss: 0.33262944663933486\n",
      "iteration: 927, loss: 0.3323882622761797\n",
      "iteration: 928, loss: 0.3321402531732729\n",
      "iteration: 929, loss: 0.33192013811874754\n",
      "iteration: 930, loss: 0.3317289616941747\n",
      "iteration: 931, loss: 0.3315218372746967\n",
      "iteration: 932, loss: 0.33124330828728804\n",
      "iteration: 933, loss: 0.3310591442637199\n",
      "iteration: 934, loss: 0.330770644308463\n",
      "iteration: 935, loss: 0.3305860020464314\n",
      "iteration: 936, loss: 0.3303275090290054\n",
      "iteration: 937, loss: 0.3301585807480146\n",
      "iteration: 938, loss: 0.32993882337619873\n",
      "iteration: 939, loss: 0.32964515114357396\n",
      "iteration: 940, loss: 0.32942149074392435\n",
      "iteration: 941, loss: 0.32925576495690895\n",
      "iteration: 942, loss: 0.32899730489476214\n",
      "iteration: 943, loss: 0.3287740355702352\n",
      "iteration: 944, loss: 0.3285765129335888\n",
      "iteration: 945, loss: 0.32833599644531475\n",
      "iteration: 946, loss: 0.3281563475047403\n",
      "iteration: 947, loss: 0.32792021059298604\n",
      "iteration: 948, loss: 0.32766160662921434\n",
      "iteration: 949, loss: 0.32744291042774903\n",
      "iteration: 950, loss: 0.3271886881223019\n",
      "iteration: 951, loss: 0.32699320635337714\n",
      "iteration: 952, loss: 0.3267916724561648\n",
      "iteration: 953, loss: 0.32652187980589215\n",
      "iteration: 954, loss: 0.32634111208574934\n",
      "iteration: 955, loss: 0.32607532607010226\n",
      "iteration: 956, loss: 0.32592785504676913\n",
      "iteration: 957, loss: 0.32581127674518845\n",
      "iteration: 958, loss: 0.32547833246630986\n",
      "iteration: 959, loss: 0.32525658192422\n",
      "iteration: 960, loss: 0.32499486758435836\n",
      "iteration: 961, loss: 0.3248535576318215\n",
      "iteration: 962, loss: 0.324622578484823\n",
      "iteration: 963, loss: 0.32437868937686176\n",
      "iteration: 964, loss: 0.3241689432060931\n",
      "iteration: 965, loss: 0.32390649106885283\n",
      "iteration: 966, loss: 0.3237394092959534\n",
      "iteration: 967, loss: 0.32349241556770103\n",
      "iteration: 968, loss: 0.32326668303405076\n",
      "iteration: 969, loss: 0.32306971808992724\n",
      "iteration: 970, loss: 0.3228208290346505\n",
      "iteration: 971, loss: 0.3226549048900575\n",
      "iteration: 972, loss: 0.32238181677655814\n",
      "iteration: 973, loss: 0.32216947479142966\n",
      "iteration: 974, loss: 0.3220159963070734\n",
      "iteration: 975, loss: 0.3218035701714174\n",
      "iteration: 976, loss: 0.32165196044109445\n",
      "iteration: 977, loss: 0.32130560737091857\n",
      "iteration: 978, loss: 0.32113285517971274\n",
      "iteration: 979, loss: 0.3208619283258873\n",
      "iteration: 980, loss: 0.3207196578393871\n",
      "iteration: 981, loss: 0.32045377021798344\n",
      "iteration: 982, loss: 0.3202558803816462\n",
      "iteration: 983, loss: 0.3200303832940506\n",
      "iteration: 984, loss: 0.3198705305239399\n",
      "iteration: 985, loss: 0.3196335319932439\n",
      "iteration: 986, loss: 0.3194366260230984\n",
      "iteration: 987, loss: 0.3192128217313355\n",
      "iteration: 988, loss: 0.3189796899982643\n",
      "iteration: 989, loss: 0.3188185580880016\n",
      "iteration: 990, loss: 0.3185717446835129\n",
      "iteration: 991, loss: 0.31833940246401804\n",
      "iteration: 992, loss: 0.3181418267612205\n",
      "iteration: 993, loss: 0.31793454568640067\n",
      "iteration: 994, loss: 0.31780695685700755\n",
      "iteration: 995, loss: 0.31753680940487244\n",
      "iteration: 996, loss: 0.3172889649071296\n",
      "iteration: 997, loss: 0.3170618032446564\n",
      "iteration: 998, loss: 0.31689341784506947\n",
      "iteration: 999, loss: 0.3166924077569291\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = MLP([\n",
    "    Linear(X_train.shape[1], 10),\n",
    "    Sigmoid(),\n",
    "    Linear(10, 30),\n",
    "    Sigmoid(),\n",
    "    Linear(30, 1)\n",
    "], learning_rate=0.001, max_iter=1000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6520ba1c-492d-481d-8f64-e9460b61eaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533029253943877"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_exp = y_test.T[0]\n",
    "y_pred = model.predict(X_test).T[0]\n",
    "model.score(y_exp,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d23ba-2be4-4603-8a06-4072520b878b",
   "metadata": {},
   "source": [
    "# Перцептрон - Классификатор (грибы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4c00024-0af0-49aa-9e6f-e843a7b9f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = pd.read_csv(\"./mushroom/expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "990ab16f-135d-4b1c-9682-0a72853ed388",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.replace(\"?\", pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dbd2968-a815-4401-bff1-69467ea4963b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stalk-root    2480\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nans = gdf.isna().sum()\n",
    "print(nans[nans != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3ee3d5e-4251-48f0-bcd8-500f6787c9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63438/4140029336.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  gdf['stalk-root'].fillna(gdf['stalk-root'].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "gdf['stalk-root'].fillna(gdf['stalk-root'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4101510c-6d31-42de-b8a0-394e6d0ecc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "nans = gdf.isna().sum()\n",
    "print(nans[nans != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd8f24eb-7a45-4797-bc9c-08de9f03b593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class                                                     [EDIBLE, POISONOUS]\n",
       "cap-shape                      [CONVEX, FLAT, BELL, SUNKEN, KNOBBED, CONICAL]\n",
       "cap-surface                                 [SMOOTH, FIBROUS, SCALY, GROOVES]\n",
       "cap-color                   [WHITE, YELLOW, BROWN, GRAY, RED, PINK, PURPLE...\n",
       "brusies                                                         [BRUISES, NO]\n",
       "odor                        [ALMOND, ANISE, NONE, PUNGENT, CREOSOTE, FOUL,...\n",
       "gill-attachment                                              [FREE, ATTACHED]\n",
       "gill-spacing                                                 [CROWDED, CLOSE]\n",
       "gill-size                                                     [NARROW, BROAD]\n",
       "gill-color                  [WHITE, PINK, BROWN, GRAY, BLACK, CHOCOLATE, P...\n",
       "stalk-shape                                             [TAPERING, ENLARGING]\n",
       "stalk-root                                     [BULBOUS, CLUB, ROOTED, EQUAL]\n",
       "stalk-surface-above-ring                      [SMOOTH, FIBROUS, SILKY, SCALY]\n",
       "stalk-surface-below-ring                      [SMOOTH, SCALY, FIBROUS, SILKY]\n",
       "stalk-color-above-ring      [WHITE, PINK, GRAY, BUFF, BROWN, RED, CINNAMON...\n",
       "stalk-color-below-ring      [WHITE, PINK, GRAY, BUFF, BROWN, RED, YELLOW, ...\n",
       "veil-type                                                           [PARTIAL]\n",
       "veil-color                                     [WHITE, YELLOW, ORANGE, BROWN]\n",
       "ring-number                                                  [ONE, TWO, NONE]\n",
       "ring-type                         [PENDANT, EVANESCENT, LARGE, FLARING, NONE]\n",
       "spore-print-color           [PURPLE, BROWN, BLACK, CHOCOLATE, GREEN, WHITE...\n",
       "population                  [SEVERAL, SCATTERED, NUMEROUS, SOLITARY, ABUND...\n",
       "habitat                     [WOODS, MEADOWS, GRASSES, PATHS, URBAN, LEAVES...\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u =gdf.apply(lambda x: x.unique())\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df8529af-f2e7-4dc9-b85f-df8201c21017",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = gdf.drop(columns=['class'])\n",
    "y = gdf['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dabb26c0-57c4-44b7-9f40-afe26297431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [x for x in X.columns]\n",
    "ohe_features = [x for x in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed25501e-8299-4230-a746-163325c1dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_features:\n",
    "    if (len(X[col].unique()) == 2):\n",
    "        X[col] = (X[col] == X[col][0]).astype(int)\n",
    "        ohe_features.remove(col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ce15812-e40b-486b-897e-3052df7f0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "ring_mapping = {'NONE':0, 'ONE': 1, 'TWO': 2}\n",
    "X['ring-number'] = X['ring-number'].map(ring_mapping)\n",
    "ohe_features.remove('ring-number')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9d2daf9-79aa-42d3-a3b3-afa7301384cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>brusies</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>stalk-shape</th>\n",
       "      <th>stalk-root</th>\n",
       "      <th>stalk-surface-above-ring</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6733</th>\n",
       "      <td>FLAT</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>RED</td>\n",
       "      <td>0</td>\n",
       "      <td>FISHY</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BUFF</td>\n",
       "      <td>1</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PINK</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>EVANESCENT</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4185</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>0</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PINK</td>\n",
       "      <td>0</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>PINK</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>LARGE</td>\n",
       "      <td>CHOCOLATE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>1</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>1</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>PINK</td>\n",
       "      <td>PINK</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>SOLITARY</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8256</th>\n",
       "      <td>BELL</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>0</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>0</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>ORANGE</td>\n",
       "      <td>ORANGE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>ORANGE</td>\n",
       "      <td>1</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>LEAVES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8209</th>\n",
       "      <td>FLAT</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>1</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>0</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>PATHS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6643</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>0</td>\n",
       "      <td>SPICY</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BUFF</td>\n",
       "      <td>1</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>PINK</td>\n",
       "      <td>PINK</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>EVANESCENT</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7584</th>\n",
       "      <td>KNOBBED</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>RED</td>\n",
       "      <td>0</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BUFF</td>\n",
       "      <td>1</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PINK</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>EVANESCENT</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>PATHS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6330</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>0</td>\n",
       "      <td>FISHY</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BUFF</td>\n",
       "      <td>1</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>PINK</td>\n",
       "      <td>PINK</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>EVANESCENT</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>PATHS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>FLAT</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>1</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PURPLE</td>\n",
       "      <td>1</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>SOLITARY</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7855</th>\n",
       "      <td>FLAT</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>0</td>\n",
       "      <td>MUSTY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>0</td>\n",
       "      <td>CLUB</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>CINNAMON</td>\n",
       "      <td>CINNAMON</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>0</td>\n",
       "      <td>NONE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>CLUSTERED</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>ANISE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>0</td>\n",
       "      <td>CLUB</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>NUMEROUS</td>\n",
       "      <td>MEADOWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7963</th>\n",
       "      <td>BELL</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>0</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>0</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NUMEROUS</td>\n",
       "      <td>GRASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6788</th>\n",
       "      <td>FLAT</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>RED</td>\n",
       "      <td>0</td>\n",
       "      <td>SPICY</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BUFF</td>\n",
       "      <td>1</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>EVANESCENT</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>LEAVES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>FLAT</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>0</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PINK</td>\n",
       "      <td>0</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>PINK</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>LARGE</td>\n",
       "      <td>CHOCOLATE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>GRASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8179</th>\n",
       "      <td>BELL</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>0</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>0</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SOLITARY</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>1</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>1</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>PINK</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>SOLITARY</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2574</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>1</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>1</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>SOLITARY</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5750</th>\n",
       "      <td>BELL</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>GREEN</td>\n",
       "      <td>0</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>GREEN</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>MEADOWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>BELL</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>ALMOND</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>0</td>\n",
       "      <td>CLUB</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>SCATTERED</td>\n",
       "      <td>GRASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7709</th>\n",
       "      <td>KNOBBED</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>0</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BUFF</td>\n",
       "      <td>1</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>PINK</td>\n",
       "      <td>PINK</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>EVANESCENT</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>LEAVES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cap-shape cap-surface cap-color  brusies    odor  gill-attachment  \\\n",
       "6733      FLAT      SMOOTH       RED        0   FISHY                1   \n",
       "4185    CONVEX     FIBROUS      GRAY        0    FOUL                1   \n",
       "3022    CONVEX       SCALY     BROWN        1    NONE                1   \n",
       "8256      BELL      SMOOTH     BROWN        0    NONE                0   \n",
       "8209      FLAT      SMOOTH      GRAY        1    NONE                1   \n",
       "6643    CONVEX       SCALY     BROWN        0   SPICY                1   \n",
       "7584   KNOBBED       SCALY       RED        0    FOUL                1   \n",
       "6330    CONVEX      SMOOTH     BROWN        0   FISHY                1   \n",
       "3408      FLAT     FIBROUS     BROWN        1    NONE                1   \n",
       "7855      FLAT       SCALY     BROWN        0   MUSTY                0   \n",
       "538     CONVEX       SCALY     WHITE        1   ANISE                1   \n",
       "7963      BELL     FIBROUS      GRAY        0    NONE                1   \n",
       "6788      FLAT      SMOOTH       RED        0   SPICY                1   \n",
       "4843      FLAT     FIBROUS      GRAY        0    FOUL                1   \n",
       "8179      BELL       SCALY     BROWN        0    NONE                1   \n",
       "3016    CONVEX       SCALY     BROWN        1    NONE                1   \n",
       "2574    CONVEX     FIBROUS     BROWN        1    NONE                1   \n",
       "5750      BELL       SCALY     WHITE        1    NONE                1   \n",
       "241       BELL       SCALY     WHITE        1  ALMOND                1   \n",
       "7709   KNOBBED       SCALY     BROWN        0    FOUL                1   \n",
       "\n",
       "      gill-spacing  gill-size gill-color  stalk-shape stalk-root  \\\n",
       "6733             0          1       BUFF            1    BULBOUS   \n",
       "4185             0          0       PINK            0    BULBOUS   \n",
       "3022             0          0      BROWN            1    BULBOUS   \n",
       "8256             0          0      BROWN            0    BULBOUS   \n",
       "8209             0          0      WHITE            0    BULBOUS   \n",
       "6643             0          1       BUFF            1    BULBOUS   \n",
       "7584             0          1       BUFF            1    BULBOUS   \n",
       "6330             0          1       BUFF            1    BULBOUS   \n",
       "3408             0          0     PURPLE            1    BULBOUS   \n",
       "7855             0          0     YELLOW            0       CLUB   \n",
       "538              0          0      BROWN            0       CLUB   \n",
       "7963             1          0      WHITE            0    BULBOUS   \n",
       "6788             0          1       BUFF            1    BULBOUS   \n",
       "4843             0          0       PINK            0    BULBOUS   \n",
       "8179             0          0      WHITE            0    BULBOUS   \n",
       "3016             0          0      BROWN            1    BULBOUS   \n",
       "2574             0          0      BROWN            1    BULBOUS   \n",
       "5750             0          0      GREEN            0    BULBOUS   \n",
       "241              0          0      BLACK            0       CLUB   \n",
       "7709             0          1       BUFF            1    BULBOUS   \n",
       "\n",
       "     stalk-surface-above-ring stalk-surface-below-ring stalk-color-above-ring  \\\n",
       "6733                   SMOOTH                   SMOOTH                  WHITE   \n",
       "4185                    SILKY                    SILKY                   PINK   \n",
       "3022                   SMOOTH                   SMOOTH                   PINK   \n",
       "8256                   SMOOTH                   SMOOTH                 ORANGE   \n",
       "8209                   SMOOTH                   SMOOTH                  WHITE   \n",
       "6643                   SMOOTH                   SMOOTH                   PINK   \n",
       "7584                    SILKY                    SILKY                  WHITE   \n",
       "6330                    SILKY                   SMOOTH                   PINK   \n",
       "3408                   SMOOTH                   SMOOTH                  WHITE   \n",
       "7855                    SILKY                    SCALY               CINNAMON   \n",
       "538                    SMOOTH                   SMOOTH                  WHITE   \n",
       "7963                   SMOOTH                    SILKY                  WHITE   \n",
       "6788                   SMOOTH                    SILKY                  WHITE   \n",
       "4843                    SILKY                    SILKY                  BROWN   \n",
       "8179                    SCALY                    SCALY                  BROWN   \n",
       "3016                   SMOOTH                   SMOOTH                   PINK   \n",
       "2574                   SMOOTH                   SMOOTH                  WHITE   \n",
       "5750                   SMOOTH                   SMOOTH                  WHITE   \n",
       "241                    SMOOTH                   SMOOTH                  WHITE   \n",
       "7709                   SMOOTH                    SILKY                   PINK   \n",
       "\n",
       "     stalk-color-below-ring veil-type veil-color  ring-number   ring-type  \\\n",
       "6733                   PINK   PARTIAL      WHITE            1  EVANESCENT   \n",
       "4185                  BROWN   PARTIAL      WHITE            1       LARGE   \n",
       "3022                   PINK   PARTIAL      WHITE            1     PENDANT   \n",
       "8256                 ORANGE   PARTIAL     ORANGE            1     PENDANT   \n",
       "8209                  WHITE   PARTIAL      WHITE            2     PENDANT   \n",
       "6643                   PINK   PARTIAL      WHITE            1  EVANESCENT   \n",
       "7584                   PINK   PARTIAL      WHITE            1  EVANESCENT   \n",
       "6330                   PINK   PARTIAL      WHITE            1  EVANESCENT   \n",
       "3408                   GRAY   PARTIAL      WHITE            1     PENDANT   \n",
       "7855               CINNAMON   PARTIAL      WHITE            0        NONE   \n",
       "538                   WHITE   PARTIAL      WHITE            1     PENDANT   \n",
       "7963                  WHITE   PARTIAL      WHITE            2     PENDANT   \n",
       "6788                  WHITE   PARTIAL      WHITE            1  EVANESCENT   \n",
       "4843                   PINK   PARTIAL      WHITE            1       LARGE   \n",
       "8179                  BROWN   PARTIAL      WHITE            2     PENDANT   \n",
       "3016                  WHITE   PARTIAL      WHITE            1     PENDANT   \n",
       "2574                  WHITE   PARTIAL      WHITE            1     PENDANT   \n",
       "5750                  WHITE   PARTIAL      WHITE            2     PENDANT   \n",
       "241                   WHITE   PARTIAL      WHITE            1     PENDANT   \n",
       "7709                   PINK   PARTIAL      WHITE            1  EVANESCENT   \n",
       "\n",
       "     spore-print-color population  habitat  \n",
       "6733             WHITE    SEVERAL    WOODS  \n",
       "4185         CHOCOLATE    SEVERAL    WOODS  \n",
       "3022             BROWN   SOLITARY    WOODS  \n",
       "8256            YELLOW    SEVERAL   LEAVES  \n",
       "8209             WHITE    SEVERAL    PATHS  \n",
       "6643             WHITE    SEVERAL    WOODS  \n",
       "7584             WHITE    SEVERAL    PATHS  \n",
       "6330             WHITE    SEVERAL    PATHS  \n",
       "3408             BLACK   SOLITARY    WOODS  \n",
       "7855             WHITE  CLUSTERED    WOODS  \n",
       "538              BLACK   NUMEROUS  MEADOWS  \n",
       "7963             WHITE   NUMEROUS  GRASSES  \n",
       "6788             WHITE    SEVERAL   LEAVES  \n",
       "4843         CHOCOLATE    SEVERAL  GRASSES  \n",
       "8179             WHITE   SOLITARY    WOODS  \n",
       "3016             BLACK   SOLITARY    WOODS  \n",
       "2574             BROWN   SOLITARY    WOODS  \n",
       "5750             GREEN    SEVERAL  MEADOWS  \n",
       "241              BLACK  SCATTERED  GRASSES  \n",
       "7709             WHITE    SEVERAL   LEAVES  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "X.sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1f1f5e8-eef6-4bdf-8723-8493738022b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, columns=ohe_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4440bc93-b0b9-408e-acce-f5abaf0fe7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8416, 109)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brusies</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>stalk-shape</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>cap-shape_BELL</th>\n",
       "      <th>cap-shape_CONICAL</th>\n",
       "      <th>cap-shape_CONVEX</th>\n",
       "      <th>cap-shape_FLAT</th>\n",
       "      <th>cap-shape_KNOBBED</th>\n",
       "      <th>cap-shape_SUNKEN</th>\n",
       "      <th>cap-surface_FIBROUS</th>\n",
       "      <th>cap-surface_GROOVES</th>\n",
       "      <th>cap-surface_SCALY</th>\n",
       "      <th>cap-surface_SMOOTH</th>\n",
       "      <th>cap-color_BROWN</th>\n",
       "      <th>cap-color_BUFF</th>\n",
       "      <th>cap-color_CINNAMON</th>\n",
       "      <th>cap-color_GRAY</th>\n",
       "      <th>cap-color_GREEN</th>\n",
       "      <th>cap-color_PINK</th>\n",
       "      <th>cap-color_PURPLE</th>\n",
       "      <th>cap-color_RED</th>\n",
       "      <th>cap-color_WHITE</th>\n",
       "      <th>cap-color_YELLOW</th>\n",
       "      <th>odor_ALMOND</th>\n",
       "      <th>odor_ANISE</th>\n",
       "      <th>odor_CREOSOTE</th>\n",
       "      <th>odor_FISHY</th>\n",
       "      <th>odor_FOUL</th>\n",
       "      <th>odor_MUSTY</th>\n",
       "      <th>odor_NONE</th>\n",
       "      <th>odor_PUNGENT</th>\n",
       "      <th>odor_SPICY</th>\n",
       "      <th>gill-color_BLACK</th>\n",
       "      <th>gill-color_BROWN</th>\n",
       "      <th>gill-color_BUFF</th>\n",
       "      <th>gill-color_CHOCOLATE</th>\n",
       "      <th>gill-color_GRAY</th>\n",
       "      <th>gill-color_GREEN</th>\n",
       "      <th>gill-color_ORANGE</th>\n",
       "      <th>gill-color_PINK</th>\n",
       "      <th>gill-color_PURPLE</th>\n",
       "      <th>gill-color_RED</th>\n",
       "      <th>gill-color_WHITE</th>\n",
       "      <th>gill-color_YELLOW</th>\n",
       "      <th>stalk-root_BULBOUS</th>\n",
       "      <th>stalk-root_CLUB</th>\n",
       "      <th>stalk-root_EQUAL</th>\n",
       "      <th>stalk-root_ROOTED</th>\n",
       "      <th>stalk-surface-above-ring_FIBROUS</th>\n",
       "      <th>stalk-surface-above-ring_SCALY</th>\n",
       "      <th>stalk-surface-above-ring_SILKY</th>\n",
       "      <th>stalk-surface-above-ring_SMOOTH</th>\n",
       "      <th>stalk-surface-below-ring_FIBROUS</th>\n",
       "      <th>stalk-surface-below-ring_SCALY</th>\n",
       "      <th>stalk-surface-below-ring_SILKY</th>\n",
       "      <th>stalk-surface-below-ring_SMOOTH</th>\n",
       "      <th>stalk-color-above-ring_BROWN</th>\n",
       "      <th>stalk-color-above-ring_BUFF</th>\n",
       "      <th>stalk-color-above-ring_CINNAMON</th>\n",
       "      <th>stalk-color-above-ring_GRAY</th>\n",
       "      <th>stalk-color-above-ring_ORANGE</th>\n",
       "      <th>stalk-color-above-ring_PINK</th>\n",
       "      <th>stalk-color-above-ring_RED</th>\n",
       "      <th>stalk-color-above-ring_WHITE</th>\n",
       "      <th>stalk-color-above-ring_YELLOW</th>\n",
       "      <th>stalk-color-below-ring_BROWN</th>\n",
       "      <th>stalk-color-below-ring_BUFF</th>\n",
       "      <th>stalk-color-below-ring_CINNAMON</th>\n",
       "      <th>stalk-color-below-ring_GRAY</th>\n",
       "      <th>stalk-color-below-ring_ORANGE</th>\n",
       "      <th>stalk-color-below-ring_PINK</th>\n",
       "      <th>stalk-color-below-ring_RED</th>\n",
       "      <th>stalk-color-below-ring_WHITE</th>\n",
       "      <th>stalk-color-below-ring_YELLOW</th>\n",
       "      <th>veil-type_PARTIAL</th>\n",
       "      <th>veil-color_BROWN</th>\n",
       "      <th>veil-color_ORANGE</th>\n",
       "      <th>veil-color_WHITE</th>\n",
       "      <th>veil-color_YELLOW</th>\n",
       "      <th>ring-type_EVANESCENT</th>\n",
       "      <th>ring-type_FLARING</th>\n",
       "      <th>ring-type_LARGE</th>\n",
       "      <th>ring-type_NONE</th>\n",
       "      <th>ring-type_PENDANT</th>\n",
       "      <th>spore-print-color_BLACK</th>\n",
       "      <th>spore-print-color_BROWN</th>\n",
       "      <th>spore-print-color_BUFF</th>\n",
       "      <th>spore-print-color_CHOCOLATE</th>\n",
       "      <th>spore-print-color_GREEN</th>\n",
       "      <th>spore-print-color_ORANGE</th>\n",
       "      <th>spore-print-color_PURPLE</th>\n",
       "      <th>spore-print-color_WHITE</th>\n",
       "      <th>spore-print-color_YELLOW</th>\n",
       "      <th>population_ABUNDANT</th>\n",
       "      <th>population_CLUSTERED</th>\n",
       "      <th>population_NUMEROUS</th>\n",
       "      <th>population_SCATTERED</th>\n",
       "      <th>population_SEVERAL</th>\n",
       "      <th>population_SOLITARY</th>\n",
       "      <th>habitat_GRASSES</th>\n",
       "      <th>habitat_LEAVES</th>\n",
       "      <th>habitat_MEADOWS</th>\n",
       "      <th>habitat_PATHS</th>\n",
       "      <th>habitat_URBAN</th>\n",
       "      <th>habitat_WASTE</th>\n",
       "      <th>habitat_WOODS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7981</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4720</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6381</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2793</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      brusies  gill-attachment  gill-spacing  gill-size  stalk-shape  \\\n",
       "7981        0                1             1          0            0   \n",
       "4720        0                1             0          0            0   \n",
       "6381        0                1             0          1            1   \n",
       "2793        1                1             0          0            1   \n",
       "929         1                1             0          1            0   \n",
       "\n",
       "      ring-number  cap-shape_BELL  cap-shape_CONICAL  cap-shape_CONVEX  \\\n",
       "7981            2            True              False             False   \n",
       "4720            1           False              False              True   \n",
       "6381            1           False              False              True   \n",
       "2793            1           False              False              True   \n",
       "929             1           False              False              True   \n",
       "\n",
       "      cap-shape_FLAT  cap-shape_KNOBBED  cap-shape_SUNKEN  \\\n",
       "7981           False              False             False   \n",
       "4720           False              False             False   \n",
       "6381           False              False             False   \n",
       "2793           False              False             False   \n",
       "929            False              False             False   \n",
       "\n",
       "      cap-surface_FIBROUS  cap-surface_GROOVES  cap-surface_SCALY  \\\n",
       "7981                 True                False              False   \n",
       "4720                False                False               True   \n",
       "6381                False                False              False   \n",
       "2793                False                False               True   \n",
       "929                 False                False              False   \n",
       "\n",
       "      cap-surface_SMOOTH  cap-color_BROWN  cap-color_BUFF  cap-color_CINNAMON  \\\n",
       "7981               False            False           False               False   \n",
       "4720               False            False           False               False   \n",
       "6381                True             True           False               False   \n",
       "2793               False            False           False               False   \n",
       "929                 True             True           False               False   \n",
       "\n",
       "      cap-color_GRAY  cap-color_GREEN  cap-color_PINK  cap-color_PURPLE  \\\n",
       "7981            True            False           False             False   \n",
       "4720           False            False           False             False   \n",
       "6381           False            False           False             False   \n",
       "2793           False            False           False             False   \n",
       "929            False            False           False             False   \n",
       "\n",
       "      cap-color_RED  cap-color_WHITE  cap-color_YELLOW  odor_ALMOND  \\\n",
       "7981          False            False             False        False   \n",
       "4720          False            False              True        False   \n",
       "6381          False            False             False        False   \n",
       "2793           True            False             False        False   \n",
       "929           False            False             False        False   \n",
       "\n",
       "      odor_ANISE  odor_CREOSOTE  odor_FISHY  odor_FOUL  odor_MUSTY  odor_NONE  \\\n",
       "7981       False          False       False      False       False       True   \n",
       "4720       False          False       False       True       False      False   \n",
       "6381       False          False       False      False       False      False   \n",
       "2793       False          False       False      False       False       True   \n",
       "929        False          False       False      False       False      False   \n",
       "\n",
       "      odor_PUNGENT  odor_SPICY  gill-color_BLACK  gill-color_BROWN  \\\n",
       "7981         False       False             False             False   \n",
       "4720         False       False             False             False   \n",
       "6381         False        True             False             False   \n",
       "2793         False       False             False             False   \n",
       "929           True       False             False             False   \n",
       "\n",
       "      gill-color_BUFF  gill-color_CHOCOLATE  gill-color_GRAY  \\\n",
       "7981            False                 False            False   \n",
       "4720            False                  True            False   \n",
       "6381             True                 False            False   \n",
       "2793            False                 False            False   \n",
       "929             False                 False            False   \n",
       "\n",
       "      gill-color_GREEN  gill-color_ORANGE  gill-color_PINK  gill-color_PURPLE  \\\n",
       "7981             False              False             True              False   \n",
       "4720             False              False            False              False   \n",
       "6381             False              False            False              False   \n",
       "2793             False              False             True              False   \n",
       "929              False              False            False              False   \n",
       "\n",
       "      gill-color_RED  gill-color_WHITE  gill-color_YELLOW  stalk-root_BULBOUS  \\\n",
       "7981           False             False              False                True   \n",
       "4720           False             False              False                True   \n",
       "6381           False             False              False                True   \n",
       "2793           False             False              False                True   \n",
       "929            False              True              False               False   \n",
       "\n",
       "      stalk-root_CLUB  stalk-root_EQUAL  stalk-root_ROOTED  \\\n",
       "7981            False             False              False   \n",
       "4720            False             False              False   \n",
       "6381            False             False              False   \n",
       "2793            False             False              False   \n",
       "929             False              True              False   \n",
       "\n",
       "      stalk-surface-above-ring_FIBROUS  stalk-surface-above-ring_SCALY  \\\n",
       "7981                             False                           False   \n",
       "4720                             False                           False   \n",
       "6381                             False                           False   \n",
       "2793                             False                           False   \n",
       "929                              False                           False   \n",
       "\n",
       "      stalk-surface-above-ring_SILKY  stalk-surface-above-ring_SMOOTH  \\\n",
       "7981                            True                            False   \n",
       "4720                            True                            False   \n",
       "6381                            True                            False   \n",
       "2793                           False                             True   \n",
       "929                            False                             True   \n",
       "\n",
       "      stalk-surface-below-ring_FIBROUS  stalk-surface-below-ring_SCALY  \\\n",
       "7981                             False                           False   \n",
       "4720                             False                           False   \n",
       "6381                             False                           False   \n",
       "2793                             False                           False   \n",
       "929                              False                           False   \n",
       "\n",
       "      stalk-surface-below-ring_SILKY  stalk-surface-below-ring_SMOOTH  \\\n",
       "7981                           False                             True   \n",
       "4720                            True                            False   \n",
       "6381                            True                            False   \n",
       "2793                           False                             True   \n",
       "929                            False                             True   \n",
       "\n",
       "      stalk-color-above-ring_BROWN  stalk-color-above-ring_BUFF  \\\n",
       "7981                         False                        False   \n",
       "4720                         False                        False   \n",
       "6381                         False                        False   \n",
       "2793                         False                        False   \n",
       "929                          False                        False   \n",
       "\n",
       "      stalk-color-above-ring_CINNAMON  stalk-color-above-ring_GRAY  \\\n",
       "7981                            False                        False   \n",
       "4720                            False                        False   \n",
       "6381                            False                        False   \n",
       "2793                            False                        False   \n",
       "929                             False                        False   \n",
       "\n",
       "      stalk-color-above-ring_ORANGE  stalk-color-above-ring_PINK  \\\n",
       "7981                          False                        False   \n",
       "4720                          False                         True   \n",
       "6381                          False                        False   \n",
       "2793                          False                        False   \n",
       "929                           False                        False   \n",
       "\n",
       "      stalk-color-above-ring_RED  stalk-color-above-ring_WHITE  \\\n",
       "7981                       False                          True   \n",
       "4720                       False                         False   \n",
       "6381                       False                          True   \n",
       "2793                       False                          True   \n",
       "929                        False                          True   \n",
       "\n",
       "      stalk-color-above-ring_YELLOW  stalk-color-below-ring_BROWN  \\\n",
       "7981                          False                         False   \n",
       "4720                          False                          True   \n",
       "6381                          False                         False   \n",
       "2793                          False                         False   \n",
       "929                           False                         False   \n",
       "\n",
       "      stalk-color-below-ring_BUFF  stalk-color-below-ring_CINNAMON  \\\n",
       "7981                        False                            False   \n",
       "4720                        False                            False   \n",
       "6381                        False                            False   \n",
       "2793                        False                            False   \n",
       "929                         False                            False   \n",
       "\n",
       "      stalk-color-below-ring_GRAY  stalk-color-below-ring_ORANGE  \\\n",
       "7981                        False                          False   \n",
       "4720                        False                          False   \n",
       "6381                        False                          False   \n",
       "2793                        False                          False   \n",
       "929                         False                          False   \n",
       "\n",
       "      stalk-color-below-ring_PINK  stalk-color-below-ring_RED  \\\n",
       "7981                        False                       False   \n",
       "4720                        False                       False   \n",
       "6381                        False                       False   \n",
       "2793                         True                       False   \n",
       "929                         False                       False   \n",
       "\n",
       "      stalk-color-below-ring_WHITE  stalk-color-below-ring_YELLOW  \\\n",
       "7981                          True                          False   \n",
       "4720                         False                          False   \n",
       "6381                          True                          False   \n",
       "2793                         False                          False   \n",
       "929                           True                          False   \n",
       "\n",
       "      veil-type_PARTIAL  veil-color_BROWN  veil-color_ORANGE  \\\n",
       "7981               True             False              False   \n",
       "4720               True             False              False   \n",
       "6381               True             False              False   \n",
       "2793               True             False              False   \n",
       "929                True             False              False   \n",
       "\n",
       "      veil-color_WHITE  veil-color_YELLOW  ring-type_EVANESCENT  \\\n",
       "7981              True              False                 False   \n",
       "4720              True              False                 False   \n",
       "6381              True              False                  True   \n",
       "2793              True              False                 False   \n",
       "929               True              False                 False   \n",
       "\n",
       "      ring-type_FLARING  ring-type_LARGE  ring-type_NONE  ring-type_PENDANT  \\\n",
       "7981              False            False           False               True   \n",
       "4720              False             True           False              False   \n",
       "6381              False            False           False              False   \n",
       "2793              False            False           False               True   \n",
       "929               False            False           False               True   \n",
       "\n",
       "      spore-print-color_BLACK  spore-print-color_BROWN  \\\n",
       "7981                    False                    False   \n",
       "4720                    False                    False   \n",
       "6381                    False                    False   \n",
       "2793                     True                    False   \n",
       "929                      True                    False   \n",
       "\n",
       "      spore-print-color_BUFF  spore-print-color_CHOCOLATE  \\\n",
       "7981                   False                        False   \n",
       "4720                   False                         True   \n",
       "6381                   False                        False   \n",
       "2793                   False                        False   \n",
       "929                    False                        False   \n",
       "\n",
       "      spore-print-color_GREEN  spore-print-color_ORANGE  \\\n",
       "7981                    False                     False   \n",
       "4720                    False                     False   \n",
       "6381                    False                     False   \n",
       "2793                    False                     False   \n",
       "929                     False                     False   \n",
       "\n",
       "      spore-print-color_PURPLE  spore-print-color_WHITE  \\\n",
       "7981                     False                     True   \n",
       "4720                     False                    False   \n",
       "6381                     False                     True   \n",
       "2793                     False                    False   \n",
       "929                      False                    False   \n",
       "\n",
       "      spore-print-color_YELLOW  population_ABUNDANT  population_CLUSTERED  \\\n",
       "7981                     False                False                 False   \n",
       "4720                     False                False                 False   \n",
       "6381                     False                False                 False   \n",
       "2793                     False                False                 False   \n",
       "929                      False                False                 False   \n",
       "\n",
       "      population_NUMEROUS  population_SCATTERED  population_SEVERAL  \\\n",
       "7981                 True                 False               False   \n",
       "4720                False                 False               False   \n",
       "6381                False                 False                True   \n",
       "2793                False                 False                True   \n",
       "929                 False                  True               False   \n",
       "\n",
       "      population_SOLITARY  habitat_GRASSES  habitat_LEAVES  habitat_MEADOWS  \\\n",
       "7981                False             True           False            False   \n",
       "4720                 True             True           False            False   \n",
       "6381                False            False           False            False   \n",
       "2793                False            False           False            False   \n",
       "929                 False             True           False            False   \n",
       "\n",
       "      habitat_PATHS  habitat_URBAN  habitat_WASTE  habitat_WOODS  \n",
       "7981          False          False          False          False  \n",
       "4720          False          False          False          False  \n",
       "6381           True          False          False          False  \n",
       "2793          False          False          False           True  \n",
       "929           False          False          False          False  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e9ddcf0-5d7f-4f3f-a60d-66950daf7d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {'EDIBLE': 1, 'POISONOUS': 0}\n",
    "y = y.map(class_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07861aa9-358e-4fb1-b28e-0513e7d7766f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3766    1\n",
       "96      1\n",
       "8200    1\n",
       "3443    1\n",
       "5779    0\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1066bb44-89f3-463d-8a4d-974a902073ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      3\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "X = X.values\n",
    "y = y.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "#X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "#X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "#y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "#y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb8f5da-ca9a-4659-a29e-17659e5d7476",
   "metadata": {},
   "source": [
    "## Своя модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73b07a33-4eb6-467b-8b1b-33975531be59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63438/2261711917.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  gdf['stalk-root'].fillna(gdf['stalk-root'].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "gdf = pd.read_csv(\"./mushroom/expanded.csv\")\n",
    "gdf.replace(\"?\", pd.NA, inplace=True)\n",
    "gdf['stalk-root'].fillna(gdf['stalk-root'].mode()[0], inplace=True)\n",
    "u =gdf.apply(lambda x: x.unique())\n",
    "X = gdf.drop(columns=['class'])\n",
    "y = gdf['class']\n",
    "cat_features = [x for x in X.columns]\n",
    "ohe_features = [x for x in X.columns]\n",
    "for col in cat_features:\n",
    "    if (len(X[col].unique()) == 2):\n",
    "        X[col] = (X[col] == X[col][0]).astype(int)\n",
    "        ohe_features.remove(col)\n",
    "ring_mapping = {'NONE':0, 'ONE': 1, 'TWO': 2}\n",
    "X['ring-number'] = X['ring-number'].map(ring_mapping)\n",
    "ohe_features.remove('ring-number')\n",
    "X = pd.get_dummies(X, columns=ohe_features)\n",
    "class_mapping = {'EDIBLE': 1, 'POISONOUS': 0}\n",
    "y = y.map(class_mapping)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fc0aacb-f75e-4303-b1e7-0712582ad60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mushrooms = fetch_ucirepo(id=73)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0a8ac0a-2452-464e-b6dc-d5b9a91b902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63438/2629846134.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['stalk-root'].fillna('m', inplace=True)\n",
      "/tmp/ipykernel_63438/2629846134.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['stalk-root'].fillna('m', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df = mushrooms.data.features\n",
    "target = mushrooms.data.targets['poisonous']\n",
    "\n",
    "df['stalk-root'].fillna('m', inplace=True)\n",
    "X = df\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "X_encoded = encoder.fit_transform(X).toarray()\n",
    "y_encoded = target.apply(lambda x: 0 if x == 'p' else 1).values.reshape(-1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "565c5ffe-6d42-41c2-ba23-b53832151d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=65)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d571e004-e8e4-45b8-9135-4854dce42a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, loss: 26.285152511498136\n",
      "iteration: 1, loss: 20.382204679945374\n",
      "iteration: 2, loss: 16.235462386375627\n",
      "iteration: 3, loss: 12.694856336842244\n",
      "iteration: 4, loss: 10.345479299226346\n",
      "iteration: 5, loss: 8.929055780540645\n",
      "iteration: 6, loss: 8.038134317980655\n",
      "iteration: 7, loss: 7.444227284424475\n",
      "iteration: 8, loss: 7.015253125435622\n",
      "iteration: 9, loss: 6.688744967189931\n",
      "iteration: 10, loss: 6.423168543490324\n",
      "iteration: 11, loss: 6.19992965226865\n",
      "iteration: 12, loss: 6.001164616230405\n",
      "iteration: 13, loss: 5.820253132703173\n",
      "iteration: 14, loss: 5.65114408302831\n",
      "iteration: 15, loss: 5.487445354208369\n",
      "iteration: 16, loss: 5.328619796117897\n",
      "iteration: 17, loss: 5.170643607216995\n",
      "iteration: 18, loss: 5.016811607141028\n",
      "iteration: 19, loss: 4.86411898400066\n",
      "iteration: 20, loss: 4.711674589538198\n",
      "iteration: 21, loss: 4.567538287650464\n",
      "iteration: 22, loss: 4.424032045529015\n",
      "iteration: 23, loss: 4.285303584888445\n",
      "iteration: 24, loss: 4.150412966364809\n",
      "iteration: 25, loss: 4.0204756335952485\n",
      "iteration: 26, loss: 3.892476089461367\n",
      "iteration: 27, loss: 3.7677529130170946\n",
      "iteration: 28, loss: 3.6441774211236226\n",
      "iteration: 29, loss: 3.52298534869802\n",
      "iteration: 30, loss: 3.404379873836775\n",
      "iteration: 31, loss: 3.2860471794918245\n",
      "iteration: 32, loss: 3.1702614769594715\n",
      "iteration: 33, loss: 3.0563330677410443\n",
      "iteration: 34, loss: 2.9438954107670563\n",
      "iteration: 35, loss: 2.8342288038836565\n",
      "iteration: 36, loss: 2.7270057729298194\n",
      "iteration: 37, loss: 2.6240230373839277\n",
      "iteration: 38, loss: 2.522732786832673\n",
      "iteration: 39, loss: 2.4274075352921383\n",
      "iteration: 40, loss: 2.336101199466402\n",
      "iteration: 41, loss: 2.248991557875044\n",
      "iteration: 42, loss: 2.169151524702226\n",
      "iteration: 43, loss: 2.092722459935001\n",
      "iteration: 44, loss: 2.022064346600134\n",
      "iteration: 45, loss: 1.956275778324881\n",
      "iteration: 46, loss: 1.8956450715951851\n",
      "iteration: 47, loss: 1.8399755544846634\n",
      "iteration: 48, loss: 1.7867561659986755\n",
      "iteration: 49, loss: 1.738700953008213\n",
      "iteration: 50, loss: 1.693834911480535\n",
      "iteration: 51, loss: 1.651663545210982\n",
      "iteration: 52, loss: 1.6131866798965013\n",
      "iteration: 53, loss: 1.576360272922008\n",
      "iteration: 54, loss: 1.5417960103370743\n",
      "iteration: 55, loss: 1.509277256667357\n",
      "iteration: 56, loss: 1.4787808385322183\n",
      "iteration: 57, loss: 1.4498463032343563\n",
      "iteration: 58, loss: 1.422635324226413\n",
      "iteration: 59, loss: 1.3957913348343955\n",
      "iteration: 60, loss: 1.3706086704958038\n",
      "iteration: 61, loss: 1.3467076959586506\n",
      "iteration: 62, loss: 1.3237129814116622\n",
      "iteration: 63, loss: 1.301772650480931\n",
      "iteration: 64, loss: 1.28146199199643\n",
      "iteration: 65, loss: 1.2611219726957925\n",
      "iteration: 66, loss: 1.2421072176510883\n",
      "iteration: 67, loss: 1.223815722539942\n",
      "iteration: 68, loss: 1.2063947071902377\n",
      "iteration: 69, loss: 1.1899184829249223\n",
      "iteration: 70, loss: 1.1743917433918654\n",
      "iteration: 71, loss: 1.1595939836049052\n",
      "iteration: 72, loss: 1.1456701436613865\n",
      "iteration: 73, loss: 1.1317224969469988\n",
      "iteration: 74, loss: 1.1189596596364204\n",
      "iteration: 75, loss: 1.106879394048446\n",
      "iteration: 76, loss: 1.0946476548372868\n",
      "iteration: 77, loss: 1.083884569070617\n",
      "iteration: 78, loss: 1.0729100735346346\n",
      "iteration: 79, loss: 1.062496741716182\n",
      "iteration: 80, loss: 1.0526740434856876\n",
      "iteration: 81, loss: 1.043047747470172\n",
      "iteration: 82, loss: 1.0339058905136531\n",
      "iteration: 83, loss: 1.024793902407329\n",
      "iteration: 84, loss: 1.0162692616308784\n",
      "iteration: 85, loss: 1.0081962544789453\n",
      "iteration: 86, loss: 1.0000754951044946\n",
      "iteration: 87, loss: 0.9922246026836099\n",
      "iteration: 88, loss: 0.9846333808563634\n",
      "iteration: 89, loss: 0.9773580717248422\n",
      "iteration: 90, loss: 0.9702402596999762\n",
      "iteration: 91, loss: 0.9632419848498384\n",
      "iteration: 92, loss: 0.9563992965517015\n",
      "iteration: 93, loss: 0.9502574132325965\n",
      "iteration: 94, loss: 0.9436295945275676\n",
      "iteration: 95, loss: 0.9372853926122732\n",
      "iteration: 96, loss: 0.931549393790996\n",
      "iteration: 97, loss: 0.9252156925608999\n",
      "iteration: 98, loss: 0.9190958617710168\n",
      "iteration: 99, loss: 0.9136816520701729\n"
     ]
    }
   ],
   "source": [
    "model = MLP([\n",
    "    Linear(X_train.shape[1], 100),\n",
    "    Sigmoid(),\n",
    "    Linear(100, 1),\n",
    "    Sigmoid()\n",
    "], learning_rate=0.05, max_iter=100)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfc7076e-7942-4578-a5be-a8f495d5c463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9415309112703624"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_exp = y_test.T[0]\n",
    "y_pred = model.predict(X_test).T[0]\n",
    "model.score(y_exp, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab3409-c341-4dec-8768-aa25a4a315a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
