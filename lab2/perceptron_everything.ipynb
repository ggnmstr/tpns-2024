{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a56405bc-9aeb-4e37-bba2-27fe4e6ca10a",
   "metadata": {},
   "source": [
    "Многослойный персептрон\n",
    "- Реализовать архитектуру многослойного персептрона. \n",
    "- Реализовать алгоритм обратного распространения ошибки для обучения сети;\n",
    "- Обучить на очищенном датасете  из 1-ой лабораторной. \n",
    "- Реализовать перцептрон - регрессор; \n",
    "- Обработать датасет с классификацией грибов по алгоритму из 1-й лабораторной работы.\n",
    "- Обучить перцептрон на датасете с классификацией грибов. \n",
    "- Реализовать классификатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6cdaa85-bc35-4981-85ce-165e88654b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import sklearn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac7b77-41a7-46f6-9350-b5479c8b6cc8",
   "metadata": {},
   "source": [
    "# Перцептрон - Регрессор (ноутбуки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da8fdb5c-7cd5-467c-a8f9-5bfad3210631",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = pd.read_csv(\"./Laptop_price.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943363bb-3979-442a-b2e6-8e2245014da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Brand             1000 non-null   object \n",
      " 1   Processor_Speed   1000 non-null   float64\n",
      " 2   RAM_Size          1000 non-null   int64  \n",
      " 3   Storage_Capacity  1000 non-null   int64  \n",
      " 4   Screen_Size       1000 non-null   float64\n",
      " 5   Weight            1000 non-null   float64\n",
      " 6   Price             1000 non-null   float64\n",
      "dtypes: float64(4), int64(2), object(1)\n",
      "memory usage: 54.8+ KB\n"
     ]
    }
   ],
   "source": [
    "ldf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2daf3433-3b0b-4f5f-b3b9-961db7fcb548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Asus', 'Acer', 'Lenovo', 'HP', 'Dell'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldf['Brand'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f7d1438-c82b-47fb-9de9-5ace3d1c65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = pd.get_dummies(ldf,columns=['Brand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "399b3e09-b33c-42eb-9ccf-d7b5a6944fa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processor_Speed</th>\n",
       "      <th>RAM_Size</th>\n",
       "      <th>Storage_Capacity</th>\n",
       "      <th>Screen_Size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Price</th>\n",
       "      <th>Brand_Acer</th>\n",
       "      <th>Brand_Asus</th>\n",
       "      <th>Brand_Dell</th>\n",
       "      <th>Brand_HP</th>\n",
       "      <th>Brand_Lenovo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.830296</td>\n",
       "      <td>16</td>\n",
       "      <td>512</td>\n",
       "      <td>11.185147</td>\n",
       "      <td>2.641094</td>\n",
       "      <td>17395.093065</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.912833</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>11.311372</td>\n",
       "      <td>3.260012</td>\n",
       "      <td>31607.605919</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.241627</td>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>11.853023</td>\n",
       "      <td>2.029061</td>\n",
       "      <td>9291.023542</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.806248</td>\n",
       "      <td>16</td>\n",
       "      <td>512</td>\n",
       "      <td>12.280360</td>\n",
       "      <td>4.573865</td>\n",
       "      <td>17436.728334</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.268097</td>\n",
       "      <td>32</td>\n",
       "      <td>1000</td>\n",
       "      <td>14.990877</td>\n",
       "      <td>4.193472</td>\n",
       "      <td>32917.990718</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Processor_Speed  RAM_Size  Storage_Capacity  Screen_Size    Weight  \\\n",
       "0         3.830296        16               512    11.185147  2.641094   \n",
       "1         2.912833         4              1000    11.311372  3.260012   \n",
       "2         3.241627         4               256    11.853023  2.029061   \n",
       "3         3.806248        16               512    12.280360  4.573865   \n",
       "4         3.268097        32              1000    14.990877  4.193472   \n",
       "\n",
       "          Price  Brand_Acer  Brand_Asus  Brand_Dell  Brand_HP  Brand_Lenovo  \n",
       "0  17395.093065           0           1           0         0             0  \n",
       "1  31607.605919           1           0           0         0             0  \n",
       "2   9291.023542           0           0           0         0             1  \n",
       "3  17436.728334           1           0           0         0             0  \n",
       "4  32917.990718           1           0           0         0             0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25ddaf36-346f-496b-b0b7-1bb2fee82cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ldf.drop(columns=['Price']).values\n",
    "y = ldf['Price'].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b9ebc8f-172d-4038-b14e-859ac43bb4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dfda518-024a-49bd-af32-686cf5341ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "401cd100-3cbf-4390-9820-0a62b9e36b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae74c1d-1dbd-4aeb-9681-b9e1d8e2fbdb",
   "metadata": {},
   "source": [
    "## sklearn model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f1199e-4395-4185-bb7c-58d46726d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(hidden_layer_sizes=(10,10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e0ab39-a96e-49b3-8986-ae14a82f10c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/lib64/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(hidden_layer_sizes=(10, 10, 10))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(hidden_layer_sizes=(10, 10, 10))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(10, 10, 10))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be4a2d89-3f25-4c5d-bf91-fe13f16f3edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9953021036373197"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X=X_test,y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5694b186-2e9a-4fe5-a27f-f5c9cbc0f072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'batch_size': 'auto',\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (10, 10, 10),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 200,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': None,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': False,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29cdc1c-ac5a-467f-8904-6582c56f8438",
   "metadata": {},
   "source": [
    "## Своя модель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bcc43c-07d4-44a3-b2e4-00ee77f3a86a",
   "metadata": {},
   "source": [
    "### Код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b6d5c7e-04c3-47dc-bb3a-22cdd7da5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Линейный слой\n",
    "class Linear:\n",
    "  def __init__(self, input_size=None, output_size=None):\n",
    "    self.X = None\n",
    "    self.input_size = input_size\n",
    "    self.output_size = output_size\n",
    "    self.W = np.random.randn(input_size, output_size)\n",
    "    self.bias = np.zeros((1, output_size))\n",
    "\n",
    "  def __call__(self, X):\n",
    "    self.X = X\n",
    "    result = np.dot(X, self.W) + self.bias\n",
    "    return result\n",
    "\n",
    "  def prop(self, grad_prev, learning_rate):\n",
    "    grad_X = np.dot(grad_prev, self.W.T)\n",
    "    grad_W = np.dot(self.X.T, grad_prev)\n",
    "    grad_bias = np.sum(grad_prev, axis=0)\n",
    "\n",
    "    self.W -= learning_rate * grad_W\n",
    "    self.bias -= learning_rate * grad_bias\n",
    "    return grad_X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24411111-261d-4c9e-9e3a-e599c9dee4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция активации\n",
    "class Sigmoid:\n",
    "  def __init__(self):\n",
    "    self.y = None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.y = self.sigmoid(x)\n",
    "    return self.y\n",
    "\n",
    "\n",
    "  def sigmoid(self,x):\n",
    "      return 1. / (1. + np.exp(-x))\n",
    "    \n",
    "  def sigmoid_deriv(self, x):\n",
    "      return x * (1 - x)\n",
    "    \n",
    "  def prop(self, grad_prev, learning_rate):\n",
    "    return grad_prev * self.sigmoid_deriv(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da8332b7-e371-4138-9392-7bac587664e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Перцептрон \n",
    "class MLP:\n",
    "  def __init__(self, layers, max_iter=100, batch_size=100, learning_rate=0.001):\n",
    "    self.layers = layers\n",
    "    self.max_iter = max_iter\n",
    "    self.batch_size = batch_size\n",
    "    self.learning_rate = learning_rate\n",
    "\n",
    "  @staticmethod\n",
    "  def score(y_true, y_pred):\n",
    "    if np.sum((y_true - np.mean(y_true)) ** 2) == 0:\n",
    "      return 1\n",
    "    return 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "\n",
    "\n",
    "  def predict(self, input_data):\n",
    "    prev_data = input_data\n",
    "    for layer in self.layers:\n",
    "      prev_data = layer(prev_data)\n",
    "    return prev_data\n",
    "\n",
    "  def _prop(self, grad_prev, learning_rate):\n",
    "    for layer in reversed(self.layers):\n",
    "      grad_prev = layer.prop(grad_prev, learning_rate)\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    for iter in range(self.max_iter):\n",
    "      permutation = np.random.permutation(len(X))\n",
    "      X_shuffled = X[permutation]\n",
    "      y_shuffled = y[permutation]\n",
    "\n",
    "      total_loss = 0.\n",
    "      for batch_first in range(0, len(X), self.batch_size):\n",
    "        batch_last = min(batch_first + self.batch_size, len(X))\n",
    "\n",
    "        X_batch = X_shuffled[batch_first:batch_last]\n",
    "        y_batch = y_shuffled[batch_first:batch_last]\n",
    "\n",
    "        y_pred = self.predict(X_batch)\n",
    "        total_loss += self._loss(y_batch, y_pred)\n",
    "        grad_prev = self._grad_loss(y_batch, y_pred)\n",
    "        self._prop(grad_prev, self.learning_rate)\n",
    "\n",
    "      print(f'iteration: {iter}, loss: {total_loss}')\n",
    "\n",
    "  @staticmethod\n",
    "  def _loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "  @staticmethod\n",
    "  def _grad_loss(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28ef3cdc-8040-4aa8-b9f2-0a3ac4af5959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "  def __init__(self):\n",
    "    self.mean = None\n",
    "    self.std = None\n",
    "\n",
    "  def fit_transform(self, data):\n",
    "    self.mean = np.mean(data, axis=0)\n",
    "    self.std = np.std(data, axis=0)\n",
    "    return (data - self.mean) / self.std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1f1d20-e048-495c-a401-5b89cd4ee544",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7e3a798-f4dd-41e2-a370-0d3e96ad4cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Brand  Processor_Speed  RAM_Size  Storage_Capacity  Screen_Size    Weight  \\\n",
      "0    Asus         3.830296        16               512    11.185147  2.641094   \n",
      "1    Acer         2.912833         4              1000    11.311372  3.260012   \n",
      "2  Lenovo         3.241627         4               256    11.853023  2.029061   \n",
      "3    Acer         3.806248        16               512    12.280360  4.573865   \n",
      "4    Acer         3.268097        32              1000    14.990877  4.193472   \n",
      "\n",
      "          Price  \n",
      "0  17395.093065  \n",
      "1  31607.605919  \n",
      "2   9291.023542  \n",
      "3  17436.728334  \n",
      "4  32917.990718  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Laptop_price.csv')\n",
    "print(df.head())\n",
    "\n",
    "X = df[['Storage_Capacity', 'RAM_Size', 'Processor_Speed', 'Screen_Size', 'Weight']].values\n",
    "y = df['Price'].values.reshape(-1, 1)\n",
    "\n",
    "normalizerX = Normalizer()\n",
    "normalizerY = Normalizer()\n",
    "\n",
    "X = normalizerX.fit_transform(X)\n",
    "y = normalizerY.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cea4c16-3b64-40ea-8414-5db2498a6ccf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, loss: 13.272036227059246\n",
      "iteration: 1, loss: 11.470309664364787\n",
      "iteration: 2, loss: 10.196378574907325\n",
      "iteration: 3, loss: 9.286482918506685\n",
      "iteration: 4, loss: 8.623188728652629\n",
      "iteration: 5, loss: 8.129667123493846\n",
      "iteration: 6, loss: 7.756932509485049\n",
      "iteration: 7, loss: 7.463859523608862\n",
      "iteration: 8, loss: 7.227065382565684\n",
      "iteration: 9, loss: 7.02929083635966\n",
      "iteration: 10, loss: 6.858880552021258\n",
      "iteration: 11, loss: 6.70746626265999\n",
      "iteration: 12, loss: 6.571101726612962\n",
      "iteration: 13, loss: 6.4431852249450134\n",
      "iteration: 14, loss: 6.323746880532601\n",
      "iteration: 15, loss: 6.211078057610908\n",
      "iteration: 16, loss: 6.10024261581637\n",
      "iteration: 17, loss: 5.9961851164740825\n",
      "iteration: 18, loss: 5.893663854321205\n",
      "iteration: 19, loss: 5.794046889007545\n",
      "iteration: 20, loss: 5.696960128492919\n",
      "iteration: 21, loss: 5.60198574025409\n",
      "iteration: 22, loss: 5.509413428031145\n",
      "iteration: 23, loss: 5.420125764302803\n",
      "iteration: 24, loss: 5.331002732045797\n",
      "iteration: 25, loss: 5.24449306430407\n",
      "iteration: 26, loss: 5.159621883146678\n",
      "iteration: 27, loss: 5.076085499729541\n",
      "iteration: 28, loss: 4.994697292698217\n",
      "iteration: 29, loss: 4.914463383027801\n",
      "iteration: 30, loss: 4.836491061889515\n",
      "iteration: 31, loss: 4.759273926915812\n",
      "iteration: 32, loss: 4.684321793884831\n",
      "iteration: 33, loss: 4.610196194662008\n",
      "iteration: 34, loss: 4.537712496930228\n",
      "iteration: 35, loss: 4.465907878431177\n",
      "iteration: 36, loss: 4.396821460611541\n",
      "iteration: 37, loss: 4.3282403822290645\n",
      "iteration: 38, loss: 4.261507309649314\n",
      "iteration: 39, loss: 4.19550456475384\n",
      "iteration: 40, loss: 4.130675502147818\n",
      "iteration: 41, loss: 4.0681364588198985\n",
      "iteration: 42, loss: 4.005068227123287\n",
      "iteration: 43, loss: 3.944672739562565\n",
      "iteration: 44, loss: 3.885241069802201\n",
      "iteration: 45, loss: 3.826516445258836\n",
      "iteration: 46, loss: 3.768639745762847\n",
      "iteration: 47, loss: 3.712498076285735\n",
      "iteration: 48, loss: 3.656864359078683\n",
      "iteration: 49, loss: 3.6021972329561893\n",
      "iteration: 50, loss: 3.549202746060002\n",
      "iteration: 51, loss: 3.4969025317918785\n",
      "iteration: 52, loss: 3.445641701827623\n",
      "iteration: 53, loss: 3.3950270733870753\n",
      "iteration: 54, loss: 3.3457289168145596\n",
      "iteration: 55, loss: 3.297797466092261\n",
      "iteration: 56, loss: 3.249700159472306\n",
      "iteration: 57, loss: 3.203022474857006\n",
      "iteration: 58, loss: 3.1569547799874016\n",
      "iteration: 59, loss: 3.1116676018634823\n",
      "iteration: 60, loss: 3.0681862574864294\n",
      "iteration: 61, loss: 3.0254953476859345\n",
      "iteration: 62, loss: 2.9824766919463555\n",
      "iteration: 63, loss: 2.9405808560239706\n",
      "iteration: 64, loss: 2.899919894147611\n",
      "iteration: 65, loss: 2.8592132214878987\n",
      "iteration: 66, loss: 2.8202425012093797\n",
      "iteration: 67, loss: 2.7815038371635796\n",
      "iteration: 68, loss: 2.74277045322749\n",
      "iteration: 69, loss: 2.7051773782783726\n",
      "iteration: 70, loss: 2.668350414901413\n",
      "iteration: 71, loss: 2.632311921228992\n",
      "iteration: 72, loss: 2.5976652532643163\n",
      "iteration: 73, loss: 2.5623647027799876\n",
      "iteration: 74, loss: 2.5283572942709824\n",
      "iteration: 75, loss: 2.4953480185911587\n",
      "iteration: 76, loss: 2.4620854809285997\n",
      "iteration: 77, loss: 2.4295627471719334\n",
      "iteration: 78, loss: 2.398416248157195\n",
      "iteration: 79, loss: 2.3666440837682146\n",
      "iteration: 80, loss: 2.336432623656082\n",
      "iteration: 81, loss: 2.3063423685100535\n",
      "iteration: 82, loss: 2.2765958335289755\n",
      "iteration: 83, loss: 2.2479227794335097\n",
      "iteration: 84, loss: 2.2195863620629877\n",
      "iteration: 85, loss: 2.191614356690713\n",
      "iteration: 86, loss: 2.1646135254662404\n",
      "iteration: 87, loss: 2.1370807426602756\n",
      "iteration: 88, loss: 2.110674751441077\n",
      "iteration: 89, loss: 2.0846728109676174\n",
      "iteration: 90, loss: 2.058821187958691\n",
      "iteration: 91, loss: 2.033870057207039\n",
      "iteration: 92, loss: 2.009932664692269\n",
      "iteration: 93, loss: 1.9847744147835504\n",
      "iteration: 94, loss: 1.96139298214382\n",
      "iteration: 95, loss: 1.9381169565621013\n",
      "iteration: 96, loss: 1.9148057439536816\n",
      "iteration: 97, loss: 1.8924534243105022\n",
      "iteration: 98, loss: 1.8701309365682233\n",
      "iteration: 99, loss: 1.848434890850121\n",
      "iteration: 100, loss: 1.8273372889314317\n",
      "iteration: 101, loss: 1.806341156740368\n",
      "iteration: 102, loss: 1.7854218052381978\n",
      "iteration: 103, loss: 1.7650794499863383\n",
      "iteration: 104, loss: 1.745114208838123\n",
      "iteration: 105, loss: 1.7258073337532651\n",
      "iteration: 106, loss: 1.7064281293428223\n",
      "iteration: 107, loss: 1.6872948006340092\n",
      "iteration: 108, loss: 1.6686767146616608\n",
      "iteration: 109, loss: 1.650560645427324\n",
      "iteration: 110, loss: 1.6329473854536063\n",
      "iteration: 111, loss: 1.6151583341281979\n",
      "iteration: 112, loss: 1.5974291067563318\n",
      "iteration: 113, loss: 1.5806453151338442\n",
      "iteration: 114, loss: 1.5636987461244247\n",
      "iteration: 115, loss: 1.5475336103769095\n",
      "iteration: 116, loss: 1.5311345074128633\n",
      "iteration: 117, loss: 1.5152160955256448\n",
      "iteration: 118, loss: 1.49985264325754\n",
      "iteration: 119, loss: 1.4842713161824233\n",
      "iteration: 120, loss: 1.4692160436784891\n",
      "iteration: 121, loss: 1.4547618813420542\n",
      "iteration: 122, loss: 1.4397699778256583\n",
      "iteration: 123, loss: 1.4253502715113033\n",
      "iteration: 124, loss: 1.4113222972211654\n",
      "iteration: 125, loss: 1.397529345751647\n",
      "iteration: 126, loss: 1.3839907510285727\n",
      "iteration: 127, loss: 1.3704330603713164\n",
      "iteration: 128, loss: 1.3574269227037892\n",
      "iteration: 129, loss: 1.3442152079640604\n",
      "iteration: 130, loss: 1.331710842752544\n",
      "iteration: 131, loss: 1.3193203803314557\n",
      "iteration: 132, loss: 1.3068189458543626\n",
      "iteration: 133, loss: 1.294861744856489\n",
      "iteration: 134, loss: 1.2828715701527185\n",
      "iteration: 135, loss: 1.2712510010639697\n",
      "iteration: 136, loss: 1.2595924683281643\n",
      "iteration: 137, loss: 1.2482491368246982\n",
      "iteration: 138, loss: 1.2371671463205758\n",
      "iteration: 139, loss: 1.2261173647467571\n",
      "iteration: 140, loss: 1.2154536893617027\n",
      "iteration: 141, loss: 1.205021127118738\n",
      "iteration: 142, loss: 1.194487686650212\n",
      "iteration: 143, loss: 1.184254216713926\n",
      "iteration: 144, loss: 1.1741459086518709\n",
      "iteration: 145, loss: 1.1644335814581415\n",
      "iteration: 146, loss: 1.1546283404218824\n",
      "iteration: 147, loss: 1.1449863617169072\n",
      "iteration: 148, loss: 1.1355448203439071\n",
      "iteration: 149, loss: 1.1262219309211041\n",
      "iteration: 150, loss: 1.1171370783744183\n",
      "iteration: 151, loss: 1.10827304962517\n",
      "iteration: 152, loss: 1.0993239272716226\n",
      "iteration: 153, loss: 1.090735427088244\n",
      "iteration: 154, loss: 1.0822932543142256\n",
      "iteration: 155, loss: 1.0740212381507825\n",
      "iteration: 156, loss: 1.0656666315732242\n",
      "iteration: 157, loss: 1.0574614032863408\n",
      "iteration: 158, loss: 1.049436614484078\n",
      "iteration: 159, loss: 1.04167543614831\n",
      "iteration: 160, loss: 1.0338234011370915\n",
      "iteration: 161, loss: 1.0261232862786076\n",
      "iteration: 162, loss: 1.0186150676692627\n",
      "iteration: 163, loss: 1.011296688524699\n",
      "iteration: 164, loss: 1.004150962421729\n",
      "iteration: 165, loss: 0.9969776864035967\n",
      "iteration: 166, loss: 0.9899754677455064\n",
      "iteration: 167, loss: 0.9830815294588228\n",
      "iteration: 168, loss: 0.9762203739941293\n",
      "iteration: 169, loss: 0.9694636701508174\n",
      "iteration: 170, loss: 0.9628672879311118\n",
      "iteration: 171, loss: 0.9564485599120747\n",
      "iteration: 172, loss: 0.9500261070532264\n",
      "iteration: 173, loss: 0.9437456775699364\n",
      "iteration: 174, loss: 0.9375585934213653\n",
      "iteration: 175, loss: 0.9314461682221329\n",
      "iteration: 176, loss: 0.9255087894895491\n",
      "iteration: 177, loss: 0.9196440647281418\n",
      "iteration: 178, loss: 0.9140331758519479\n",
      "iteration: 179, loss: 0.9081404573913554\n",
      "iteration: 180, loss: 0.9024970805598078\n",
      "iteration: 181, loss: 0.89710103291143\n",
      "iteration: 182, loss: 0.8915621184121606\n",
      "iteration: 183, loss: 0.8862623921191952\n",
      "iteration: 184, loss: 0.8810310450148134\n",
      "iteration: 185, loss: 0.8759952151105652\n",
      "iteration: 186, loss: 0.870598683313063\n",
      "iteration: 187, loss: 0.8654692668511428\n",
      "iteration: 188, loss: 0.860554886588011\n",
      "iteration: 189, loss: 0.8556522645000996\n",
      "iteration: 190, loss: 0.8510189693105134\n",
      "iteration: 191, loss: 0.846088474595824\n",
      "iteration: 192, loss: 0.841427410339063\n",
      "iteration: 193, loss: 0.8368028937276766\n",
      "iteration: 194, loss: 0.832457256836691\n",
      "iteration: 195, loss: 0.828007224267385\n",
      "iteration: 196, loss: 0.8235911362300393\n",
      "iteration: 197, loss: 0.819111342545\n",
      "iteration: 198, loss: 0.8151087487038033\n",
      "iteration: 199, loss: 0.8106985809900025\n",
      "iteration: 200, loss: 0.8065091767684492\n",
      "iteration: 201, loss: 0.8025313734130622\n",
      "iteration: 202, loss: 0.7985368123579145\n",
      "iteration: 203, loss: 0.7946027140282156\n",
      "iteration: 204, loss: 0.7907502299493174\n",
      "iteration: 205, loss: 0.7869073226411113\n",
      "iteration: 206, loss: 0.7831819228402018\n",
      "iteration: 207, loss: 0.779561133484624\n",
      "iteration: 208, loss: 0.7758262453618534\n",
      "iteration: 209, loss: 0.7720825164495049\n",
      "iteration: 210, loss: 0.7686419598199763\n",
      "iteration: 211, loss: 0.7650303779755179\n",
      "iteration: 212, loss: 0.7617278834816062\n",
      "iteration: 213, loss: 0.7582931590193487\n",
      "iteration: 214, loss: 0.7547869826662332\n",
      "iteration: 215, loss: 0.7515166975425844\n",
      "iteration: 216, loss: 0.7483054376206683\n",
      "iteration: 217, loss: 0.7451860769437524\n",
      "iteration: 218, loss: 0.7418553683258022\n",
      "iteration: 219, loss: 0.7387657338532423\n",
      "iteration: 220, loss: 0.7357313647484032\n",
      "iteration: 221, loss: 0.7326572895878178\n",
      "iteration: 222, loss: 0.7297270252714056\n",
      "iteration: 223, loss: 0.7267104391280982\n",
      "iteration: 224, loss: 0.7238706065436139\n",
      "iteration: 225, loss: 0.7209405260273207\n",
      "iteration: 226, loss: 0.7182481238936661\n",
      "iteration: 227, loss: 0.71536493941116\n",
      "iteration: 228, loss: 0.7127444203702885\n",
      "iteration: 229, loss: 0.7098575732544115\n",
      "iteration: 230, loss: 0.7072821748329629\n",
      "iteration: 231, loss: 0.7046543572515123\n",
      "iteration: 232, loss: 0.702066566425307\n",
      "iteration: 233, loss: 0.6995896160945796\n",
      "iteration: 234, loss: 0.6970908512183898\n",
      "iteration: 235, loss: 0.6944976802512656\n",
      "iteration: 236, loss: 0.6919704566808652\n",
      "iteration: 237, loss: 0.689601072988796\n",
      "iteration: 238, loss: 0.6871667083822284\n",
      "iteration: 239, loss: 0.6848439275400144\n",
      "iteration: 240, loss: 0.6825659481169097\n",
      "iteration: 241, loss: 0.6802422890989607\n",
      "iteration: 242, loss: 0.6780450609025113\n",
      "iteration: 243, loss: 0.6757323005979613\n",
      "iteration: 244, loss: 0.673628841442815\n",
      "iteration: 245, loss: 0.6714919170102255\n",
      "iteration: 246, loss: 0.6692147136285326\n",
      "iteration: 247, loss: 0.6671407660509951\n",
      "iteration: 248, loss: 0.6651017295166404\n",
      "iteration: 249, loss: 0.6630056750187396\n",
      "iteration: 250, loss: 0.6609943622588411\n",
      "iteration: 251, loss: 0.6589976020558234\n",
      "iteration: 252, loss: 0.65701037554204\n",
      "iteration: 253, loss: 0.6550254665953722\n",
      "iteration: 254, loss: 0.653133673964372\n",
      "iteration: 255, loss: 0.6512167440768891\n",
      "iteration: 256, loss: 0.649301025341849\n",
      "iteration: 257, loss: 0.6474996912514824\n",
      "iteration: 258, loss: 0.6457084704302534\n",
      "iteration: 259, loss: 0.6438505578717559\n",
      "iteration: 260, loss: 0.6423987705680253\n",
      "iteration: 261, loss: 0.6403811406366632\n",
      "iteration: 262, loss: 0.638602842696166\n",
      "iteration: 263, loss: 0.6369023152461515\n",
      "iteration: 264, loss: 0.6352597472985247\n",
      "iteration: 265, loss: 0.6335388801233526\n",
      "iteration: 266, loss: 0.6319301628432543\n",
      "iteration: 267, loss: 0.6302421429506743\n",
      "iteration: 268, loss: 0.6286243021122696\n",
      "iteration: 269, loss: 0.6271259141304587\n",
      "iteration: 270, loss: 0.6254506577443514\n",
      "iteration: 271, loss: 0.6238686720380195\n",
      "iteration: 272, loss: 0.6223666353384741\n",
      "iteration: 273, loss: 0.6209989592759865\n",
      "iteration: 274, loss: 0.6193241934628991\n",
      "iteration: 275, loss: 0.6178064131200292\n",
      "iteration: 276, loss: 0.6166408982547888\n",
      "iteration: 277, loss: 0.6149653464088847\n",
      "iteration: 278, loss: 0.613490398639546\n",
      "iteration: 279, loss: 0.6122439413871913\n",
      "iteration: 280, loss: 0.6106429514874735\n",
      "iteration: 281, loss: 0.6093444511445605\n",
      "iteration: 282, loss: 0.6080228742223979\n",
      "iteration: 283, loss: 0.6067378554538304\n",
      "iteration: 284, loss: 0.605428028045605\n",
      "iteration: 285, loss: 0.6040665229663579\n",
      "iteration: 286, loss: 0.602681272269844\n",
      "iteration: 287, loss: 0.6013909319370956\n",
      "iteration: 288, loss: 0.60013593859038\n",
      "iteration: 289, loss: 0.5989322944683628\n",
      "iteration: 290, loss: 0.5976664981985502\n",
      "iteration: 291, loss: 0.5963561236366367\n",
      "iteration: 292, loss: 0.5951452936344347\n",
      "iteration: 293, loss: 0.5939483068821918\n",
      "iteration: 294, loss: 0.5928252361467117\n",
      "iteration: 295, loss: 0.591679201389493\n",
      "iteration: 296, loss: 0.5905608231372306\n",
      "iteration: 297, loss: 0.5894745365947657\n",
      "iteration: 298, loss: 0.5882413272747866\n",
      "iteration: 299, loss: 0.587156796842471\n",
      "iteration: 300, loss: 0.5859009996611613\n",
      "iteration: 301, loss: 0.5848453198811977\n",
      "iteration: 302, loss: 0.5838504440114609\n",
      "iteration: 303, loss: 0.5827345084487601\n",
      "iteration: 304, loss: 0.581712303828255\n",
      "iteration: 305, loss: 0.580668787277962\n",
      "iteration: 306, loss: 0.5794833098576045\n",
      "iteration: 307, loss: 0.5784320989919581\n",
      "iteration: 308, loss: 0.5775250752623717\n",
      "iteration: 309, loss: 0.576451267878309\n",
      "iteration: 310, loss: 0.5754420182494072\n",
      "iteration: 311, loss: 0.5745086848922966\n",
      "iteration: 312, loss: 0.5735194022308092\n",
      "iteration: 313, loss: 0.5725829204468477\n",
      "iteration: 314, loss: 0.5717412858889521\n",
      "iteration: 315, loss: 0.570570951581857\n",
      "iteration: 316, loss: 0.5696409889961921\n",
      "iteration: 317, loss: 0.568706281494131\n",
      "iteration: 318, loss: 0.5677366160152667\n",
      "iteration: 319, loss: 0.566836573215314\n",
      "iteration: 320, loss: 0.565938122415801\n",
      "iteration: 321, loss: 0.5650539453923737\n",
      "iteration: 322, loss: 0.5641019091273827\n",
      "iteration: 323, loss: 0.5632157142039365\n",
      "iteration: 324, loss: 0.5625467902714739\n",
      "iteration: 325, loss: 0.5617973731960484\n",
      "iteration: 326, loss: 0.5606551488653\n",
      "iteration: 327, loss: 0.5598694808373017\n",
      "iteration: 328, loss: 0.5590132322277136\n",
      "iteration: 329, loss: 0.5581114932431628\n",
      "iteration: 330, loss: 0.5573092366237308\n",
      "iteration: 331, loss: 0.556608807049359\n",
      "iteration: 332, loss: 0.5556886624924381\n",
      "iteration: 333, loss: 0.5548606044751863\n",
      "iteration: 334, loss: 0.5541096471744216\n",
      "iteration: 335, loss: 0.5532548886446351\n",
      "iteration: 336, loss: 0.5524332930836208\n",
      "iteration: 337, loss: 0.5517389294617547\n",
      "iteration: 338, loss: 0.5510305872398107\n",
      "iteration: 339, loss: 0.550148764397195\n",
      "iteration: 340, loss: 0.5495709266654402\n",
      "iteration: 341, loss: 0.5488179453316617\n",
      "iteration: 342, loss: 0.5479037077802069\n",
      "iteration: 343, loss: 0.5472317338728593\n",
      "iteration: 344, loss: 0.5466727707360843\n",
      "iteration: 345, loss: 0.5457587921305456\n",
      "iteration: 346, loss: 0.5450574391705597\n",
      "iteration: 347, loss: 0.5442518216123141\n",
      "iteration: 348, loss: 0.5435595838686003\n",
      "iteration: 349, loss: 0.5428719198808791\n",
      "iteration: 350, loss: 0.5421542818137566\n",
      "iteration: 351, loss: 0.5415025077515077\n",
      "iteration: 352, loss: 0.5407805700868247\n",
      "iteration: 353, loss: 0.5401563109718076\n",
      "iteration: 354, loss: 0.5394763447887451\n",
      "iteration: 355, loss: 0.5389184729369634\n",
      "iteration: 356, loss: 0.5380715573669415\n",
      "iteration: 357, loss: 0.5373738901651831\n",
      "iteration: 358, loss: 0.5367156060159273\n",
      "iteration: 359, loss: 0.5361230158371855\n",
      "iteration: 360, loss: 0.5354219093121876\n",
      "iteration: 361, loss: 0.5349641884747112\n",
      "iteration: 362, loss: 0.5341410134364413\n",
      "iteration: 363, loss: 0.5336004347108083\n",
      "iteration: 364, loss: 0.5329590660291773\n",
      "iteration: 365, loss: 0.5323256144776154\n",
      "iteration: 366, loss: 0.5317766732783361\n",
      "iteration: 367, loss: 0.5310411010211283\n",
      "iteration: 368, loss: 0.530434721444052\n",
      "iteration: 369, loss: 0.5299648343562822\n",
      "iteration: 370, loss: 0.5292850241920791\n",
      "iteration: 371, loss: 0.5285869638541578\n",
      "iteration: 372, loss: 0.5280125145415895\n",
      "iteration: 373, loss: 0.5273787254210236\n",
      "iteration: 374, loss: 0.5268231872271293\n",
      "iteration: 375, loss: 0.5262901235205484\n",
      "iteration: 376, loss: 0.5257077877541697\n",
      "iteration: 377, loss: 0.5251073942364591\n",
      "iteration: 378, loss: 0.524481628154381\n",
      "iteration: 379, loss: 0.5240301612984553\n",
      "iteration: 380, loss: 0.5234672995311206\n",
      "iteration: 381, loss: 0.5228375772206029\n",
      "iteration: 382, loss: 0.5222450097503372\n",
      "iteration: 383, loss: 0.5218193042494957\n",
      "iteration: 384, loss: 0.5211329921658338\n",
      "iteration: 385, loss: 0.5205817245408819\n",
      "iteration: 386, loss: 0.5200211330195688\n",
      "iteration: 387, loss: 0.5195300651623206\n",
      "iteration: 388, loss: 0.5192347211179311\n",
      "iteration: 389, loss: 0.5184575370662856\n",
      "iteration: 390, loss: 0.5178810960866324\n",
      "iteration: 391, loss: 0.5173452735941685\n",
      "iteration: 392, loss: 0.5169460116859297\n",
      "iteration: 393, loss: 0.5163556956004707\n",
      "iteration: 394, loss: 0.5159439844774004\n",
      "iteration: 395, loss: 0.5152683716250769\n",
      "iteration: 396, loss: 0.5148252904565673\n",
      "iteration: 397, loss: 0.5142536667334686\n",
      "iteration: 398, loss: 0.513792977809916\n",
      "iteration: 399, loss: 0.513232031217649\n",
      "iteration: 400, loss: 0.5127751030453841\n",
      "iteration: 401, loss: 0.5122246911687504\n",
      "iteration: 402, loss: 0.511746812083933\n",
      "iteration: 403, loss: 0.5112611930626262\n",
      "iteration: 404, loss: 0.5107842145066137\n",
      "iteration: 405, loss: 0.5102403713420466\n",
      "iteration: 406, loss: 0.5097234990430184\n",
      "iteration: 407, loss: 0.5092473900450369\n",
      "iteration: 408, loss: 0.5088122515266844\n",
      "iteration: 409, loss: 0.5083061478225303\n",
      "iteration: 410, loss: 0.507886594337066\n",
      "iteration: 411, loss: 0.5073782907095795\n",
      "iteration: 412, loss: 0.5069707135483311\n",
      "iteration: 413, loss: 0.506455840216629\n",
      "iteration: 414, loss: 0.5059837635889777\n",
      "iteration: 415, loss: 0.5055219164955231\n",
      "iteration: 416, loss: 0.5050045437341815\n",
      "iteration: 417, loss: 0.5045492750701961\n",
      "iteration: 418, loss: 0.5041805832651511\n",
      "iteration: 419, loss: 0.5036294834761363\n",
      "iteration: 420, loss: 0.5031574850589365\n",
      "iteration: 421, loss: 0.5027138791393885\n",
      "iteration: 422, loss: 0.5022273385048323\n",
      "iteration: 423, loss: 0.5018745187877814\n",
      "iteration: 424, loss: 0.5012930363705366\n",
      "iteration: 425, loss: 0.5009925767978612\n",
      "iteration: 426, loss: 0.5005324948560148\n",
      "iteration: 427, loss: 0.49995843847997146\n",
      "iteration: 428, loss: 0.4995462959411464\n",
      "iteration: 429, loss: 0.4990930761056561\n",
      "iteration: 430, loss: 0.4987492210319929\n",
      "iteration: 431, loss: 0.49828799821940883\n",
      "iteration: 432, loss: 0.49780689510580434\n",
      "iteration: 433, loss: 0.49739154464234064\n",
      "iteration: 434, loss: 0.4969832595771546\n",
      "iteration: 435, loss: 0.49665518979092943\n",
      "iteration: 436, loss: 0.4960474081219258\n",
      "iteration: 437, loss: 0.49568303848495415\n",
      "iteration: 438, loss: 0.4952872457251076\n",
      "iteration: 439, loss: 0.4949115404172202\n",
      "iteration: 440, loss: 0.49432048584891053\n",
      "iteration: 441, loss: 0.4940068602825616\n",
      "iteration: 442, loss: 0.49349140056911694\n",
      "iteration: 443, loss: 0.4931017480337216\n",
      "iteration: 444, loss: 0.49270296228149313\n",
      "iteration: 445, loss: 0.49241216751687095\n",
      "iteration: 446, loss: 0.491869002588554\n",
      "iteration: 447, loss: 0.49157175543678705\n",
      "iteration: 448, loss: 0.49104738568361506\n",
      "iteration: 449, loss: 0.4906493736016455\n",
      "iteration: 450, loss: 0.4902592024706955\n",
      "iteration: 451, loss: 0.48987123171040203\n",
      "iteration: 452, loss: 0.48937866103015204\n",
      "iteration: 453, loss: 0.4889790846345629\n",
      "iteration: 454, loss: 0.48859976265945665\n",
      "iteration: 455, loss: 0.48819032235727255\n",
      "iteration: 456, loss: 0.4878407710020837\n",
      "iteration: 457, loss: 0.4876028180341546\n",
      "iteration: 458, loss: 0.4870933190512027\n",
      "iteration: 459, loss: 0.4866552679635848\n",
      "iteration: 460, loss: 0.4861984115112708\n",
      "iteration: 461, loss: 0.48585592995464366\n",
      "iteration: 462, loss: 0.485393279530873\n",
      "iteration: 463, loss: 0.4849982392283903\n",
      "iteration: 464, loss: 0.48460517274778486\n",
      "iteration: 465, loss: 0.48423316501242214\n",
      "iteration: 466, loss: 0.4839331168570353\n",
      "iteration: 467, loss: 0.4834769905627535\n",
      "iteration: 468, loss: 0.48319849674747384\n",
      "iteration: 469, loss: 0.4826665800708865\n",
      "iteration: 470, loss: 0.48248506772514066\n",
      "iteration: 471, loss: 0.481976770257221\n",
      "iteration: 472, loss: 0.48164410230120114\n",
      "iteration: 473, loss: 0.4811961526014402\n",
      "iteration: 474, loss: 0.48086686942141915\n",
      "iteration: 475, loss: 0.48043929590131984\n",
      "iteration: 476, loss: 0.48019416952580607\n",
      "iteration: 477, loss: 0.47968429039359184\n",
      "iteration: 478, loss: 0.47935355371582206\n",
      "iteration: 479, loss: 0.4789398953783445\n",
      "iteration: 480, loss: 0.4785732353527501\n",
      "iteration: 481, loss: 0.4782012982930687\n",
      "iteration: 482, loss: 0.47777286375535877\n",
      "iteration: 483, loss: 0.4774466698077866\n",
      "iteration: 484, loss: 0.47707499248007645\n",
      "iteration: 485, loss: 0.4767335071388522\n",
      "iteration: 486, loss: 0.4764774276721711\n",
      "iteration: 487, loss: 0.47611866537278136\n",
      "iteration: 488, loss: 0.4755906060187985\n",
      "iteration: 489, loss: 0.4752193026292107\n",
      "iteration: 490, loss: 0.47488445116190897\n",
      "iteration: 491, loss: 0.47452174170299666\n",
      "iteration: 492, loss: 0.4741651042690704\n",
      "iteration: 493, loss: 0.4738355236762528\n",
      "iteration: 494, loss: 0.4735653410422386\n",
      "iteration: 495, loss: 0.47303809935176294\n",
      "iteration: 496, loss: 0.4727660781424822\n",
      "iteration: 497, loss: 0.47233648672528283\n",
      "iteration: 498, loss: 0.47198128574858317\n",
      "iteration: 499, loss: 0.47160295863680635\n",
      "iteration: 500, loss: 0.47144796177904136\n",
      "iteration: 501, loss: 0.4708873140822068\n",
      "iteration: 502, loss: 0.47061165886056344\n",
      "iteration: 503, loss: 0.4703331181209346\n",
      "iteration: 504, loss: 0.4699374490423181\n",
      "iteration: 505, loss: 0.4695544604880661\n",
      "iteration: 506, loss: 0.4691541330858952\n",
      "iteration: 507, loss: 0.4687647669612115\n",
      "iteration: 508, loss: 0.46852369459151594\n",
      "iteration: 509, loss: 0.46810937085309756\n",
      "iteration: 510, loss: 0.4678137833400029\n",
      "iteration: 511, loss: 0.46744350936239376\n",
      "iteration: 512, loss: 0.46716259881198685\n",
      "iteration: 513, loss: 0.4667666683632103\n",
      "iteration: 514, loss: 0.4665021323135425\n",
      "iteration: 515, loss: 0.46604457381443803\n",
      "iteration: 516, loss: 0.4657048689165167\n",
      "iteration: 517, loss: 0.4653429186867499\n",
      "iteration: 518, loss: 0.4651596537674396\n",
      "iteration: 519, loss: 0.46476303468253904\n",
      "iteration: 520, loss: 0.4643106351809696\n",
      "iteration: 521, loss: 0.46395372128099843\n",
      "iteration: 522, loss: 0.4637035802397253\n",
      "iteration: 523, loss: 0.46329111069711487\n",
      "iteration: 524, loss: 0.463024254003346\n",
      "iteration: 525, loss: 0.46276740516434794\n",
      "iteration: 526, loss: 0.46226716255289574\n",
      "iteration: 527, loss: 0.46201401840138184\n",
      "iteration: 528, loss: 0.46160466787753063\n",
      "iteration: 529, loss: 0.46132658244935487\n",
      "iteration: 530, loss: 0.46095330527354866\n",
      "iteration: 531, loss: 0.4606585069496707\n",
      "iteration: 532, loss: 0.46031481357917037\n",
      "iteration: 533, loss: 0.4599496048216182\n",
      "iteration: 534, loss: 0.45959638306929473\n",
      "iteration: 535, loss: 0.4592828109193602\n",
      "iteration: 536, loss: 0.45896555880381623\n",
      "iteration: 537, loss: 0.45866172888366846\n",
      "iteration: 538, loss: 0.4584350276098875\n",
      "iteration: 539, loss: 0.4579627134575818\n",
      "iteration: 540, loss: 0.45783075760116504\n",
      "iteration: 541, loss: 0.45731250604059515\n",
      "iteration: 542, loss: 0.4569990851228573\n",
      "iteration: 543, loss: 0.45672741962604824\n",
      "iteration: 544, loss: 0.45645490167854935\n",
      "iteration: 545, loss: 0.4559961213488602\n",
      "iteration: 546, loss: 0.45562408006298294\n",
      "iteration: 547, loss: 0.4554087088424339\n",
      "iteration: 548, loss: 0.45505795511050084\n",
      "iteration: 549, loss: 0.454674434090955\n",
      "iteration: 550, loss: 0.45438570487671337\n",
      "iteration: 551, loss: 0.4541478536561425\n",
      "iteration: 552, loss: 0.45378611865647794\n",
      "iteration: 553, loss: 0.4534194428677679\n",
      "iteration: 554, loss: 0.45315830279373986\n",
      "iteration: 555, loss: 0.45271593186019937\n",
      "iteration: 556, loss: 0.452447723201926\n",
      "iteration: 557, loss: 0.45213763752379393\n",
      "iteration: 558, loss: 0.4517960423740501\n",
      "iteration: 559, loss: 0.451506229943372\n",
      "iteration: 560, loss: 0.4511511678511688\n",
      "iteration: 561, loss: 0.4509216788969859\n",
      "iteration: 562, loss: 0.45047091153133745\n",
      "iteration: 563, loss: 0.4502248472146868\n",
      "iteration: 564, loss: 0.4498799728002507\n",
      "iteration: 565, loss: 0.44957811248143875\n",
      "iteration: 566, loss: 0.44927225178401015\n",
      "iteration: 567, loss: 0.4489915559072119\n",
      "iteration: 568, loss: 0.4487160137712851\n",
      "iteration: 569, loss: 0.4482878391635109\n",
      "iteration: 570, loss: 0.4480261643454764\n",
      "iteration: 571, loss: 0.44769434707790007\n",
      "iteration: 572, loss: 0.4473519121591388\n",
      "iteration: 573, loss: 0.4470406188964479\n",
      "iteration: 574, loss: 0.44674205315203375\n",
      "iteration: 575, loss: 0.44655907101855774\n",
      "iteration: 576, loss: 0.44612769192232316\n",
      "iteration: 577, loss: 0.44599765444482703\n",
      "iteration: 578, loss: 0.4455105400060821\n",
      "iteration: 579, loss: 0.4452168917929132\n",
      "iteration: 580, loss: 0.4449013701914732\n",
      "iteration: 581, loss: 0.44455917222429076\n",
      "iteration: 582, loss: 0.4442693299046656\n",
      "iteration: 583, loss: 0.4440319909753856\n",
      "iteration: 584, loss: 0.4435934938236583\n",
      "iteration: 585, loss: 0.4433640631948416\n",
      "iteration: 586, loss: 0.4430198335246038\n",
      "iteration: 587, loss: 0.4427229745186837\n",
      "iteration: 588, loss: 0.44241214906838416\n",
      "iteration: 589, loss: 0.44210256991719865\n",
      "iteration: 590, loss: 0.44176288344779313\n",
      "iteration: 591, loss: 0.44150948859160294\n",
      "iteration: 592, loss: 0.4412283523505402\n",
      "iteration: 593, loss: 0.4409348685944269\n",
      "iteration: 594, loss: 0.44056400574097576\n",
      "iteration: 595, loss: 0.4403073010819909\n",
      "iteration: 596, loss: 0.4400413174158738\n",
      "iteration: 597, loss: 0.43966732071090664\n",
      "iteration: 598, loss: 0.4393632052127306\n",
      "iteration: 599, loss: 0.43905395160241234\n",
      "iteration: 600, loss: 0.4389343680537037\n",
      "iteration: 601, loss: 0.4385106564625129\n",
      "iteration: 602, loss: 0.4381815173117661\n",
      "iteration: 603, loss: 0.43790781564395814\n",
      "iteration: 604, loss: 0.4376103527332306\n",
      "iteration: 605, loss: 0.43728346918127387\n",
      "iteration: 606, loss: 0.43701534593477137\n",
      "iteration: 607, loss: 0.4366521846668644\n",
      "iteration: 608, loss: 0.4363514352785973\n",
      "iteration: 609, loss: 0.43613346924679763\n",
      "iteration: 610, loss: 0.4358195070887564\n",
      "iteration: 611, loss: 0.43549120214684006\n",
      "iteration: 612, loss: 0.4352030418277953\n",
      "iteration: 613, loss: 0.4348468743628008\n",
      "iteration: 614, loss: 0.4345518059404507\n",
      "iteration: 615, loss: 0.43430914843008417\n",
      "iteration: 616, loss: 0.4339337932272392\n",
      "iteration: 617, loss: 0.43366080104076854\n",
      "iteration: 618, loss: 0.43334704855548917\n",
      "iteration: 619, loss: 0.4330475996089486\n",
      "iteration: 620, loss: 0.43284313092595367\n",
      "iteration: 621, loss: 0.43252629472134246\n",
      "iteration: 622, loss: 0.4322337332184757\n",
      "iteration: 623, loss: 0.4319161689073774\n",
      "iteration: 624, loss: 0.4316350446996556\n",
      "iteration: 625, loss: 0.4312760953369074\n",
      "iteration: 626, loss: 0.4310040939895089\n",
      "iteration: 627, loss: 0.4307431970819084\n",
      "iteration: 628, loss: 0.43042568999242004\n",
      "iteration: 629, loss: 0.43020824300469673\n",
      "iteration: 630, loss: 0.42984498043295905\n",
      "iteration: 631, loss: 0.429547137783932\n",
      "iteration: 632, loss: 0.42934605849044905\n",
      "iteration: 633, loss: 0.4289830018918596\n",
      "iteration: 634, loss: 0.4287155430198877\n",
      "iteration: 635, loss: 0.4284206704668741\n",
      "iteration: 636, loss: 0.4281236444750977\n",
      "iteration: 637, loss: 0.42778608598635526\n",
      "iteration: 638, loss: 0.4275388693459096\n",
      "iteration: 639, loss: 0.42724894686921583\n",
      "iteration: 640, loss: 0.4270179843033378\n",
      "iteration: 641, loss: 0.4266335169353308\n",
      "iteration: 642, loss: 0.42635300850324553\n",
      "iteration: 643, loss: 0.4261562739638153\n",
      "iteration: 644, loss: 0.4257743879649051\n",
      "iteration: 645, loss: 0.4255499650475605\n",
      "iteration: 646, loss: 0.42521790646212654\n",
      "iteration: 647, loss: 0.42494686177035673\n",
      "iteration: 648, loss: 0.42469936048615287\n",
      "iteration: 649, loss: 0.4243403354083589\n",
      "iteration: 650, loss: 0.42409131289451557\n",
      "iteration: 651, loss: 0.42377622174279594\n",
      "iteration: 652, loss: 0.42347179808589913\n",
      "iteration: 653, loss: 0.42318079972917766\n",
      "iteration: 654, loss: 0.4230142373731694\n",
      "iteration: 655, loss: 0.4226674421473112\n",
      "iteration: 656, loss: 0.4224012328815615\n",
      "iteration: 657, loss: 0.42216435357908694\n",
      "iteration: 658, loss: 0.4218266895344458\n",
      "iteration: 659, loss: 0.4215705626645914\n",
      "iteration: 660, loss: 0.4213439729832675\n",
      "iteration: 661, loss: 0.42095580834274837\n",
      "iteration: 662, loss: 0.42073530446229807\n",
      "iteration: 663, loss: 0.4204923294829507\n",
      "iteration: 664, loss: 0.42007280552888293\n",
      "iteration: 665, loss: 0.41987056446628845\n",
      "iteration: 666, loss: 0.4195878439802383\n",
      "iteration: 667, loss: 0.41927898117664764\n",
      "iteration: 668, loss: 0.41897620194208696\n",
      "iteration: 669, loss: 0.4187540812936531\n",
      "iteration: 670, loss: 0.4184163251005163\n",
      "iteration: 671, loss: 0.41815084662001334\n",
      "iteration: 672, loss: 0.4178843540180998\n",
      "iteration: 673, loss: 0.4175497207021218\n",
      "iteration: 674, loss: 0.4173410759204342\n",
      "iteration: 675, loss: 0.41703274163752585\n",
      "iteration: 676, loss: 0.41674455735926996\n",
      "iteration: 677, loss: 0.4164633350934866\n",
      "iteration: 678, loss: 0.4161842260047167\n",
      "iteration: 679, loss: 0.41597927392321465\n",
      "iteration: 680, loss: 0.4156489037650585\n",
      "iteration: 681, loss: 0.4153441056126372\n",
      "iteration: 682, loss: 0.41514374681521765\n",
      "iteration: 683, loss: 0.41484655524860325\n",
      "iteration: 684, loss: 0.4145960648452851\n",
      "iteration: 685, loss: 0.414313765886836\n",
      "iteration: 686, loss: 0.41395541602155367\n",
      "iteration: 687, loss: 0.4137751698764897\n",
      "iteration: 688, loss: 0.4134628093370638\n",
      "iteration: 689, loss: 0.41326590197985524\n",
      "iteration: 690, loss: 0.413032346744435\n",
      "iteration: 691, loss: 0.41264413603708927\n",
      "iteration: 692, loss: 0.4123550503305296\n",
      "iteration: 693, loss: 0.41210193374123244\n",
      "iteration: 694, loss: 0.4118624082829958\n",
      "iteration: 695, loss: 0.4115359808398056\n",
      "iteration: 696, loss: 0.41129259214949015\n",
      "iteration: 697, loss: 0.41103456593677395\n",
      "iteration: 698, loss: 0.4107568386911879\n",
      "iteration: 699, loss: 0.41054730904908115\n",
      "iteration: 700, loss: 0.41020641494852494\n",
      "iteration: 701, loss: 0.40990883596198885\n",
      "iteration: 702, loss: 0.4096220165148265\n",
      "iteration: 703, loss: 0.4093829682800945\n",
      "iteration: 704, loss: 0.4091176840165427\n",
      "iteration: 705, loss: 0.40888599301552353\n",
      "iteration: 706, loss: 0.408609260843057\n",
      "iteration: 707, loss: 0.40830279749823073\n",
      "iteration: 708, loss: 0.40799789455923186\n",
      "iteration: 709, loss: 0.40770676781502974\n",
      "iteration: 710, loss: 0.40748576743719545\n",
      "iteration: 711, loss: 0.4072291909896039\n",
      "iteration: 712, loss: 0.4069180968600361\n",
      "iteration: 713, loss: 0.40668063904314355\n",
      "iteration: 714, loss: 0.40644103432750617\n",
      "iteration: 715, loss: 0.40616973443076454\n",
      "iteration: 716, loss: 0.4058835479733598\n",
      "iteration: 717, loss: 0.4055924278640691\n",
      "iteration: 718, loss: 0.4053416836190624\n",
      "iteration: 719, loss: 0.4050883118217816\n",
      "iteration: 720, loss: 0.40480240770339365\n",
      "iteration: 721, loss: 0.40456308087997467\n",
      "iteration: 722, loss: 0.40433047166791647\n",
      "iteration: 723, loss: 0.4040120353408587\n",
      "iteration: 724, loss: 0.4036896352938947\n",
      "iteration: 725, loss: 0.40347629316194844\n",
      "iteration: 726, loss: 0.40322249530221765\n",
      "iteration: 727, loss: 0.40296908690580335\n",
      "iteration: 728, loss: 0.40271602133238\n",
      "iteration: 729, loss: 0.40247875149082485\n",
      "iteration: 730, loss: 0.40219035024028105\n",
      "iteration: 731, loss: 0.40189274462041324\n",
      "iteration: 732, loss: 0.4016985197411977\n",
      "iteration: 733, loss: 0.40137147522827576\n",
      "iteration: 734, loss: 0.40104720213400097\n",
      "iteration: 735, loss: 0.4009298566213779\n",
      "iteration: 736, loss: 0.40060286685701335\n",
      "iteration: 737, loss: 0.4003480969780913\n",
      "iteration: 738, loss: 0.40015566679013753\n",
      "iteration: 739, loss: 0.3998076509838357\n",
      "iteration: 740, loss: 0.3995170600366089\n",
      "iteration: 741, loss: 0.3992988522150469\n",
      "iteration: 742, loss: 0.398990112299846\n",
      "iteration: 743, loss: 0.39872273172360406\n",
      "iteration: 744, loss: 0.39865382734058796\n",
      "iteration: 745, loss: 0.39825444934487275\n",
      "iteration: 746, loss: 0.3979492639238993\n",
      "iteration: 747, loss: 0.397700437564469\n",
      "iteration: 748, loss: 0.39743870080261035\n",
      "iteration: 749, loss: 0.39729241436287727\n",
      "iteration: 750, loss: 0.3970174712717095\n",
      "iteration: 751, loss: 0.39670773609301835\n",
      "iteration: 752, loss: 0.3963992590795991\n",
      "iteration: 753, loss: 0.3962247509444294\n",
      "iteration: 754, loss: 0.39589696392773077\n",
      "iteration: 755, loss: 0.3956724522348313\n",
      "iteration: 756, loss: 0.3953970321529969\n",
      "iteration: 757, loss: 0.39518193045506345\n",
      "iteration: 758, loss: 0.3949520346377436\n",
      "iteration: 759, loss: 0.39463852674356487\n",
      "iteration: 760, loss: 0.39438772597881433\n",
      "iteration: 761, loss: 0.39409210251915505\n",
      "iteration: 762, loss: 0.393854170859662\n",
      "iteration: 763, loss: 0.3935842210159839\n",
      "iteration: 764, loss: 0.3933109368823057\n",
      "iteration: 765, loss: 0.3930749007243816\n",
      "iteration: 766, loss: 0.39282373272241333\n",
      "iteration: 767, loss: 0.3925474425846957\n",
      "iteration: 768, loss: 0.39231966535576257\n",
      "iteration: 769, loss: 0.39212335605568543\n",
      "iteration: 770, loss: 0.39175928890878625\n",
      "iteration: 771, loss: 0.3915506977358582\n",
      "iteration: 772, loss: 0.3913844732843572\n",
      "iteration: 773, loss: 0.39107708055323\n",
      "iteration: 774, loss: 0.3907554483199603\n",
      "iteration: 775, loss: 0.390502077470002\n",
      "iteration: 776, loss: 0.3902797651052513\n",
      "iteration: 777, loss: 0.39005594614011124\n",
      "iteration: 778, loss: 0.38983130464732646\n",
      "iteration: 779, loss: 0.38951123624233436\n",
      "iteration: 780, loss: 0.389252279332361\n",
      "iteration: 781, loss: 0.3890409312360091\n",
      "iteration: 782, loss: 0.38883061946664055\n",
      "iteration: 783, loss: 0.38855665109361714\n",
      "iteration: 784, loss: 0.388275489822041\n",
      "iteration: 785, loss: 0.38798394193589075\n",
      "iteration: 786, loss: 0.38775975045431443\n",
      "iteration: 787, loss: 0.3875705141013017\n",
      "iteration: 788, loss: 0.3872801552675714\n",
      "iteration: 789, loss: 0.38705692437382\n",
      "iteration: 790, loss: 0.386765840294632\n",
      "iteration: 791, loss: 0.3865223187181974\n",
      "iteration: 792, loss: 0.38642405476654496\n",
      "iteration: 793, loss: 0.3860986504548205\n",
      "iteration: 794, loss: 0.385787943256127\n",
      "iteration: 795, loss: 0.3855352800674694\n",
      "iteration: 796, loss: 0.3852188724502827\n",
      "iteration: 797, loss: 0.38507686243466643\n",
      "iteration: 798, loss: 0.38477742289538025\n",
      "iteration: 799, loss: 0.38446964106704895\n",
      "iteration: 800, loss: 0.3843461827561064\n",
      "iteration: 801, loss: 0.3840231257469387\n",
      "iteration: 802, loss: 0.38375633377285656\n",
      "iteration: 803, loss: 0.3835709481184802\n",
      "iteration: 804, loss: 0.38336106249694535\n",
      "iteration: 805, loss: 0.383083365669948\n",
      "iteration: 806, loss: 0.3828578274382228\n",
      "iteration: 807, loss: 0.382615562256019\n",
      "iteration: 808, loss: 0.38228747572971533\n",
      "iteration: 809, loss: 0.38206301258957276\n",
      "iteration: 810, loss: 0.381877933376079\n",
      "iteration: 811, loss: 0.381639128505158\n",
      "iteration: 812, loss: 0.3814015550625852\n",
      "iteration: 813, loss: 0.38105321254543845\n",
      "iteration: 814, loss: 0.38079704508941353\n",
      "iteration: 815, loss: 0.3805715199849647\n",
      "iteration: 816, loss: 0.3804368852415247\n",
      "iteration: 817, loss: 0.38018161408189644\n",
      "iteration: 818, loss: 0.37983784635628015\n",
      "iteration: 819, loss: 0.3796070676804965\n",
      "iteration: 820, loss: 0.3794329347566292\n",
      "iteration: 821, loss: 0.37911057379649926\n",
      "iteration: 822, loss: 0.3789386798435599\n",
      "iteration: 823, loss: 0.37870967528205285\n",
      "iteration: 824, loss: 0.3785222895717118\n",
      "iteration: 825, loss: 0.3781558822102891\n",
      "iteration: 826, loss: 0.37788622200060823\n",
      "iteration: 827, loss: 0.3776589521585221\n",
      "iteration: 828, loss: 0.37743453796172277\n",
      "iteration: 829, loss: 0.3772058364156662\n",
      "iteration: 830, loss: 0.37691844159467663\n",
      "iteration: 831, loss: 0.3766852054088953\n",
      "iteration: 832, loss: 0.3764946609291472\n",
      "iteration: 833, loss: 0.3761951268216559\n",
      "iteration: 834, loss: 0.3760316229117474\n",
      "iteration: 835, loss: 0.3757480687678532\n",
      "iteration: 836, loss: 0.37551440129701863\n",
      "iteration: 837, loss: 0.3753319122966684\n",
      "iteration: 838, loss: 0.3750064245152367\n",
      "iteration: 839, loss: 0.3747879075772991\n",
      "iteration: 840, loss: 0.3745317883765369\n",
      "iteration: 841, loss: 0.37428515914899346\n",
      "iteration: 842, loss: 0.3740634079185915\n",
      "iteration: 843, loss: 0.3738836875651411\n",
      "iteration: 844, loss: 0.373649802845315\n",
      "iteration: 845, loss: 0.37337677082798876\n",
      "iteration: 846, loss: 0.37310758568834973\n",
      "iteration: 847, loss: 0.37284704330161184\n",
      "iteration: 848, loss: 0.37268097451657434\n",
      "iteration: 849, loss: 0.3724023735085596\n",
      "iteration: 850, loss: 0.3722151107249285\n",
      "iteration: 851, loss: 0.3719351766900843\n",
      "iteration: 852, loss: 0.37166202781574537\n",
      "iteration: 853, loss: 0.37144128223193634\n",
      "iteration: 854, loss: 0.37125028630105517\n",
      "iteration: 855, loss: 0.37101254652968196\n",
      "iteration: 856, loss: 0.37076976084899005\n",
      "iteration: 857, loss: 0.3705738768751114\n",
      "iteration: 858, loss: 0.3703018293913805\n",
      "iteration: 859, loss: 0.37004115648047814\n",
      "iteration: 860, loss: 0.36985935178972573\n",
      "iteration: 861, loss: 0.36960773637360156\n",
      "iteration: 862, loss: 0.3693366983457091\n",
      "iteration: 863, loss: 0.36913326598481994\n",
      "iteration: 864, loss: 0.3689121237234703\n",
      "iteration: 865, loss: 0.3686460663505604\n",
      "iteration: 866, loss: 0.3685040758485635\n",
      "iteration: 867, loss: 0.36819895913962014\n",
      "iteration: 868, loss: 0.36796903887779053\n",
      "iteration: 869, loss: 0.3677151617111023\n",
      "iteration: 870, loss: 0.3675539258676282\n",
      "iteration: 871, loss: 0.36721606010775953\n",
      "iteration: 872, loss: 0.36705697171033125\n",
      "iteration: 873, loss: 0.36682378770753343\n",
      "iteration: 874, loss: 0.3665306382497031\n",
      "iteration: 875, loss: 0.36647677185386296\n",
      "iteration: 876, loss: 0.36607470293047695\n",
      "iteration: 877, loss: 0.3658452313893869\n",
      "iteration: 878, loss: 0.36566128942027415\n",
      "iteration: 879, loss: 0.36548376550929595\n",
      "iteration: 880, loss: 0.3651637281440828\n",
      "iteration: 881, loss: 0.36503110000878636\n",
      "iteration: 882, loss: 0.36475122448162794\n",
      "iteration: 883, loss: 0.3644759826114633\n",
      "iteration: 884, loss: 0.3642437833937091\n",
      "iteration: 885, loss: 0.36408041808179936\n",
      "iteration: 886, loss: 0.3638180407521317\n",
      "iteration: 887, loss: 0.36358700582310544\n",
      "iteration: 888, loss: 0.36333529509940987\n",
      "iteration: 889, loss: 0.3630877279912773\n",
      "iteration: 890, loss: 0.3628768320231916\n",
      "iteration: 891, loss: 0.3626725107979629\n",
      "iteration: 892, loss: 0.3623880861871047\n",
      "iteration: 893, loss: 0.36213432118119593\n",
      "iteration: 894, loss: 0.36199673716090186\n",
      "iteration: 895, loss: 0.3616989086625273\n",
      "iteration: 896, loss: 0.3615887288290173\n",
      "iteration: 897, loss: 0.36132857366159843\n",
      "iteration: 898, loss: 0.3610406350412363\n",
      "iteration: 899, loss: 0.3608127330200964\n",
      "iteration: 900, loss: 0.3606017655071377\n",
      "iteration: 901, loss: 0.36036938274176417\n",
      "iteration: 902, loss: 0.3601231719502285\n",
      "iteration: 903, loss: 0.3599073965219294\n",
      "iteration: 904, loss: 0.3596600937621429\n",
      "iteration: 905, loss: 0.35949187343613576\n",
      "iteration: 906, loss: 0.35924299182241637\n",
      "iteration: 907, loss: 0.35905068226192255\n",
      "iteration: 908, loss: 0.3588556732550039\n",
      "iteration: 909, loss: 0.3585537340342927\n",
      "iteration: 910, loss: 0.3584144797450071\n",
      "iteration: 911, loss: 0.3581849836208894\n",
      "iteration: 912, loss: 0.35786300281474126\n",
      "iteration: 913, loss: 0.3576688911467736\n",
      "iteration: 914, loss: 0.35745884222063906\n",
      "iteration: 915, loss: 0.35724797197258584\n",
      "iteration: 916, loss: 0.3569775566049399\n",
      "iteration: 917, loss: 0.3568175916496664\n",
      "iteration: 918, loss: 0.3565393999690951\n",
      "iteration: 919, loss: 0.3563322155879883\n",
      "iteration: 920, loss: 0.35617233723387337\n",
      "iteration: 921, loss: 0.3559091657111981\n",
      "iteration: 922, loss: 0.3556937164251955\n",
      "iteration: 923, loss: 0.3554131691620197\n",
      "iteration: 924, loss: 0.35519870157820366\n",
      "iteration: 925, loss: 0.35496859101240846\n",
      "iteration: 926, loss: 0.35474873947985397\n",
      "iteration: 927, loss: 0.35452560825614376\n",
      "iteration: 928, loss: 0.3543447813604339\n",
      "iteration: 929, loss: 0.3540932709471144\n",
      "iteration: 930, loss: 0.3539026140279146\n",
      "iteration: 931, loss: 0.3537287648718418\n",
      "iteration: 932, loss: 0.35349495902718786\n",
      "iteration: 933, loss: 0.3532157132919225\n",
      "iteration: 934, loss: 0.35297127992948396\n",
      "iteration: 935, loss: 0.35280425438387647\n",
      "iteration: 936, loss: 0.3526162902314981\n",
      "iteration: 937, loss: 0.352396769480958\n",
      "iteration: 938, loss: 0.3521324697983949\n",
      "iteration: 939, loss: 0.35188822725813845\n",
      "iteration: 940, loss: 0.3516845010315223\n",
      "iteration: 941, loss: 0.3514319866972307\n",
      "iteration: 942, loss: 0.35125715912770283\n",
      "iteration: 943, loss: 0.3512484319594599\n",
      "iteration: 944, loss: 0.35081664094765763\n",
      "iteration: 945, loss: 0.3506376376597168\n",
      "iteration: 946, loss: 0.3504110305791736\n",
      "iteration: 947, loss: 0.3501600470293821\n",
      "iteration: 948, loss: 0.34993427296820284\n",
      "iteration: 949, loss: 0.3497643203707834\n",
      "iteration: 950, loss: 0.34958291120881135\n",
      "iteration: 951, loss: 0.3492851949740988\n",
      "iteration: 952, loss: 0.34912323283085384\n",
      "iteration: 953, loss: 0.34887779618765935\n",
      "iteration: 954, loss: 0.3486437083716173\n",
      "iteration: 955, loss: 0.34842891684627697\n",
      "iteration: 956, loss: 0.34824154781057204\n",
      "iteration: 957, loss: 0.34814905268685165\n",
      "iteration: 958, loss: 0.34779532716070694\n",
      "iteration: 959, loss: 0.3476266082461763\n",
      "iteration: 960, loss: 0.34744192936763746\n",
      "iteration: 961, loss: 0.3471450847603107\n",
      "iteration: 962, loss: 0.34693466474597884\n",
      "iteration: 963, loss: 0.34671539266722085\n",
      "iteration: 964, loss: 0.34656684706695406\n",
      "iteration: 965, loss: 0.34637227895016925\n",
      "iteration: 966, loss: 0.34608350884187056\n",
      "iteration: 967, loss: 0.3458473548626105\n",
      "iteration: 968, loss: 0.3456241733141697\n",
      "iteration: 969, loss: 0.34542000327127353\n",
      "iteration: 970, loss: 0.34528152523698785\n",
      "iteration: 971, loss: 0.3450294250305216\n",
      "iteration: 972, loss: 0.3447908216108157\n",
      "iteration: 973, loss: 0.3446046973374343\n",
      "iteration: 974, loss: 0.3444254849416223\n",
      "iteration: 975, loss: 0.34417851241744457\n",
      "iteration: 976, loss: 0.34398446715335723\n",
      "iteration: 977, loss: 0.34371157869087426\n",
      "iteration: 978, loss: 0.34354681236992035\n",
      "iteration: 979, loss: 0.3433389223793329\n",
      "iteration: 980, loss: 0.3431057455374364\n",
      "iteration: 981, loss: 0.3428580924649712\n",
      "iteration: 982, loss: 0.3426875472959683\n",
      "iteration: 983, loss: 0.3424745408840036\n",
      "iteration: 984, loss: 0.34226003258402615\n",
      "iteration: 985, loss: 0.34209239463238106\n",
      "iteration: 986, loss: 0.3418323169828908\n",
      "iteration: 987, loss: 0.3416208045301884\n",
      "iteration: 988, loss: 0.34142347562075004\n",
      "iteration: 989, loss: 0.34121058000803234\n",
      "iteration: 990, loss: 0.3410663968869187\n",
      "iteration: 991, loss: 0.34078795092179615\n",
      "iteration: 992, loss: 0.3406333289136307\n",
      "iteration: 993, loss: 0.3404432647421597\n",
      "iteration: 994, loss: 0.34012043017291194\n",
      "iteration: 995, loss: 0.33994575888553435\n",
      "iteration: 996, loss: 0.33969468852500684\n",
      "iteration: 997, loss: 0.3395294201592699\n",
      "iteration: 998, loss: 0.3393252805170166\n",
      "iteration: 999, loss: 0.33910206605055526\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = MLP([\n",
    "    Linear(X_train.shape[1], 10),\n",
    "    Sigmoid(),\n",
    "    Linear(10, 30),\n",
    "    Sigmoid(),\n",
    "    Linear(30, 1)\n",
    "], learning_rate=0.001, max_iter=1000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6520ba1c-492d-481d-8f64-e9460b61eaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9543791425248009"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_exp = y_test.T[0]\n",
    "y_pred = model.predict(X_test).T[0]\n",
    "model.score(y_exp,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d23ba-2be4-4603-8a06-4072520b878b",
   "metadata": {},
   "source": [
    "# Перцептрон - Классификатор (грибы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4c00024-0af0-49aa-9e6f-e843a7b9f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = pd.read_csv(\"./mushroom/expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "990ab16f-135d-4b1c-9682-0a72853ed388",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.replace(\"?\", pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dbd2968-a815-4401-bff1-69467ea4963b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stalk-root    2480\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nans = gdf.isna().sum()\n",
    "print(nans[nans != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3ee3d5e-4251-48f0-bcd8-500f6787c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['stalk-root'].fillna(gdf['stalk-root'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4101510c-6d31-42de-b8a0-394e6d0ecc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "nans = gdf.isna().sum()\n",
    "print(nans[nans != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd8f24eb-7a45-4797-bc9c-08de9f03b593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class                                                     [EDIBLE, POISONOUS]\n",
       "cap-shape                      [CONVEX, FLAT, BELL, SUNKEN, KNOBBED, CONICAL]\n",
       "cap-surface                                 [SMOOTH, FIBROUS, SCALY, GROOVES]\n",
       "cap-color                   [WHITE, YELLOW, BROWN, GRAY, RED, PINK, PURPLE...\n",
       "brusies                                                         [BRUISES, NO]\n",
       "odor                        [ALMOND, ANISE, NONE, PUNGENT, CREOSOTE, FOUL,...\n",
       "gill-attachment                                              [FREE, ATTACHED]\n",
       "gill-spacing                                                 [CROWDED, CLOSE]\n",
       "gill-size                                                     [NARROW, BROAD]\n",
       "gill-color                  [WHITE, PINK, BROWN, GRAY, BLACK, CHOCOLATE, P...\n",
       "stalk-shape                                             [TAPERING, ENLARGING]\n",
       "stalk-root                                     [BULBOUS, CLUB, ROOTED, EQUAL]\n",
       "stalk-surface-above-ring                      [SMOOTH, FIBROUS, SILKY, SCALY]\n",
       "stalk-surface-below-ring                      [SMOOTH, SCALY, FIBROUS, SILKY]\n",
       "stalk-color-above-ring      [WHITE, PINK, GRAY, BUFF, BROWN, RED, CINNAMON...\n",
       "stalk-color-below-ring      [WHITE, PINK, GRAY, BUFF, BROWN, RED, YELLOW, ...\n",
       "veil-type                                                           [PARTIAL]\n",
       "veil-color                                     [WHITE, YELLOW, ORANGE, BROWN]\n",
       "ring-number                                                  [ONE, TWO, NONE]\n",
       "ring-type                         [PENDANT, EVANESCENT, LARGE, FLARING, NONE]\n",
       "spore-print-color           [PURPLE, BROWN, BLACK, CHOCOLATE, GREEN, WHITE...\n",
       "population                  [SEVERAL, SCATTERED, NUMEROUS, SOLITARY, ABUND...\n",
       "habitat                     [WOODS, MEADOWS, GRASSES, PATHS, URBAN, LEAVES...\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u =gdf.apply(lambda x: x.unique())\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df8529af-f2e7-4dc9-b85f-df8201c21017",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = gdf.drop(columns=['class'])\n",
    "y = gdf['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dabb26c0-57c4-44b7-9f40-afe26297431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [x for x in X.columns]\n",
    "ohe_features = [x for x in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed25501e-8299-4230-a746-163325c1dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_features:\n",
    "    if (len(X[col].unique()) == 2):\n",
    "        X[col] = (X[col] == X[col][0]).astype(int)\n",
    "        ohe_features.remove(col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ce15812-e40b-486b-897e-3052df7f0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "ring_mapping = {'NONE':0, 'ONE': 1, 'TWO': 2}\n",
    "X['ring-number'] = X['ring-number'].map(ring_mapping)\n",
    "ohe_features.remove('ring-number')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9d2daf9-79aa-42d3-a3b3-afa7301384cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>brusies</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>stalk-shape</th>\n",
       "      <th>stalk-root</th>\n",
       "      <th>stalk-surface-above-ring</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>0</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PINK</td>\n",
       "      <td>1</td>\n",
       "      <td>EQUAL</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>EVANESCENT</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>ABUNDANT</td>\n",
       "      <td>GRASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>PUNGENT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUAL</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>SCATTERED</td>\n",
       "      <td>GRASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6775</th>\n",
       "      <td>FLAT</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>RED</td>\n",
       "      <td>0</td>\n",
       "      <td>FISHY</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BUFF</td>\n",
       "      <td>1</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>PINK</td>\n",
       "      <td>PINK</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>EVANESCENT</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4264</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>0</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>0</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>BUFF</td>\n",
       "      <td>PINK</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>LARGE</td>\n",
       "      <td>CHOCOLATE</td>\n",
       "      <td>SOLITARY</td>\n",
       "      <td>GRASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>FLAT</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>1</td>\n",
       "      <td>ANISE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>PURPLE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7114</th>\n",
       "      <td>FLAT</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>0</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BUFF</td>\n",
       "      <td>1</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>EVANESCENT</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>0</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>1</td>\n",
       "      <td>EQUAL</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>EVANESCENT</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>ABUNDANT</td>\n",
       "      <td>GRASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4044</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>PINK</td>\n",
       "      <td>0</td>\n",
       "      <td>CREOSOTE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>0</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>SCATTERED</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>1</td>\n",
       "      <td>ALMOND</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>0</td>\n",
       "      <td>CLUB</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>NUMEROUS</td>\n",
       "      <td>GRASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>FLAT</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>1</td>\n",
       "      <td>ANISE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>0</td>\n",
       "      <td>ROOTED</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>SOLITARY</td>\n",
       "      <td>GRASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6466</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>RED</td>\n",
       "      <td>0</td>\n",
       "      <td>FISHY</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BUFF</td>\n",
       "      <td>1</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>EVANESCENT</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>FLAT</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>PUNGENT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUAL</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>URBAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6634</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>0</td>\n",
       "      <td>SPICY</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BUFF</td>\n",
       "      <td>1</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>EVANESCENT</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>0</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PINK</td>\n",
       "      <td>0</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>LARGE</td>\n",
       "      <td>CHOCOLATE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>PATHS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>FLAT</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>0</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CHOCOLATE</td>\n",
       "      <td>0</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>LARGE</td>\n",
       "      <td>CHOCOLATE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>PATHS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>BELL</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>ANISE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>0</td>\n",
       "      <td>CLUB</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>NUMEROUS</td>\n",
       "      <td>MEADOWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5766</th>\n",
       "      <td>FLAT</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>0</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>GREEN</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>MEADOWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1627</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>0</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>CHOCOLATE</td>\n",
       "      <td>1</td>\n",
       "      <td>EQUAL</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>EVANESCENT</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>ABUNDANT</td>\n",
       "      <td>GRASSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4508</th>\n",
       "      <td>CONVEX</td>\n",
       "      <td>SCALY</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>0</td>\n",
       "      <td>FOUL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PINK</td>\n",
       "      <td>0</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>SILKY</td>\n",
       "      <td>PINK</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>LARGE</td>\n",
       "      <td>CHOCOLATE</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>PATHS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>FLAT</td>\n",
       "      <td>FIBROUS</td>\n",
       "      <td>RED</td>\n",
       "      <td>1</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>BULBOUS</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>SMOOTH</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>PARTIAL</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>PENDANT</td>\n",
       "      <td>BROWN</td>\n",
       "      <td>SEVERAL</td>\n",
       "      <td>WOODS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cap-shape cap-surface cap-color  brusies      odor  gill-attachment  \\\n",
       "1481    CONVEX     FIBROUS      GRAY        0      NONE                1   \n",
       "965     CONVEX       SCALY     WHITE        1   PUNGENT                1   \n",
       "6775      FLAT      SMOOTH       RED        0     FISHY                1   \n",
       "4264    CONVEX     FIBROUS    YELLOW        0      FOUL                1   \n",
       "90        FLAT     FIBROUS    YELLOW        1     ANISE                1   \n",
       "7114      FLAT       SCALY     BROWN        0      FOUL                1   \n",
       "1341    CONVEX      SMOOTH      GRAY        0      NONE                1   \n",
       "4044    CONVEX     FIBROUS      PINK        0  CREOSOTE                1   \n",
       "443     CONVEX      SMOOTH    YELLOW        1    ALMOND                1   \n",
       "728       FLAT       SCALY    YELLOW        1     ANISE                1   \n",
       "6466    CONVEX       SCALY       RED        0     FISHY                1   \n",
       "1106      FLAT       SCALY     WHITE        1   PUNGENT                1   \n",
       "6634    CONVEX       SCALY     BROWN        0     SPICY                1   \n",
       "4364    CONVEX     FIBROUS    YELLOW        0      FOUL                1   \n",
       "5390      FLAT       SCALY    YELLOW        0      FOUL                1   \n",
       "150       BELL      SMOOTH     WHITE        1     ANISE                1   \n",
       "5766      FLAT      SMOOTH     WHITE        1      NONE                1   \n",
       "1627    CONVEX     FIBROUS     BROWN        0      NONE                1   \n",
       "4508    CONVEX       SCALY      GRAY        0      FOUL                1   \n",
       "3219      FLAT     FIBROUS       RED        1      NONE                1   \n",
       "\n",
       "      gill-spacing  gill-size gill-color  stalk-shape stalk-root  \\\n",
       "1481             1          0       PINK            1      EQUAL   \n",
       "965              0          1      WHITE            0      EQUAL   \n",
       "6775             0          1       BUFF            1    BULBOUS   \n",
       "4264             0          0       GRAY            0    BULBOUS   \n",
       "90               1          1      WHITE            1    BULBOUS   \n",
       "7114             0          1       BUFF            1    BULBOUS   \n",
       "1341             1          0      BROWN            1      EQUAL   \n",
       "4044             0          1      BROWN            0    BULBOUS   \n",
       "443              0          0      BROWN            0       CLUB   \n",
       "728              0          0      WHITE            0     ROOTED   \n",
       "6466             0          1       BUFF            1    BULBOUS   \n",
       "1106             0          1      BLACK            0      EQUAL   \n",
       "6634             0          1       BUFF            1    BULBOUS   \n",
       "4364             0          0       PINK            0    BULBOUS   \n",
       "5390             0          0  CHOCOLATE            0    BULBOUS   \n",
       "150              0          0      BLACK            0       CLUB   \n",
       "5766             0          0       GRAY            0    BULBOUS   \n",
       "1627             1          0  CHOCOLATE            1      EQUAL   \n",
       "4508             0          0       PINK            0    BULBOUS   \n",
       "3219             0          0      WHITE            1    BULBOUS   \n",
       "\n",
       "     stalk-surface-above-ring stalk-surface-below-ring stalk-color-above-ring  \\\n",
       "1481                  FIBROUS                   SMOOTH                  WHITE   \n",
       "965                    SMOOTH                   SMOOTH                  WHITE   \n",
       "6775                    SILKY                    SILKY                   PINK   \n",
       "4264                    SILKY                    SILKY                   BUFF   \n",
       "90                     SMOOTH                   SMOOTH                  WHITE   \n",
       "7114                   SMOOTH                   SMOOTH                  WHITE   \n",
       "1341                  FIBROUS                  FIBROUS                  WHITE   \n",
       "4044                   SMOOTH                   SMOOTH                  WHITE   \n",
       "443                    SMOOTH                   SMOOTH                  WHITE   \n",
       "728                    SMOOTH                    SCALY                  WHITE   \n",
       "6466                    SILKY                   SMOOTH                  WHITE   \n",
       "1106                   SMOOTH                   SMOOTH                  WHITE   \n",
       "6634                   SMOOTH                   SMOOTH                  WHITE   \n",
       "4364                    SILKY                    SILKY                  BROWN   \n",
       "5390                    SILKY                    SILKY                  BROWN   \n",
       "150                    SMOOTH                   SMOOTH                  WHITE   \n",
       "5766                   SMOOTH                   SMOOTH                  WHITE   \n",
       "1627                  FIBROUS                   SMOOTH                  WHITE   \n",
       "4508                    SILKY                    SILKY                   PINK   \n",
       "3219                   SMOOTH                   SMOOTH                   GRAY   \n",
       "\n",
       "     stalk-color-below-ring veil-type veil-color  ring-number   ring-type  \\\n",
       "1481                  WHITE   PARTIAL      WHITE            1  EVANESCENT   \n",
       "965                   WHITE   PARTIAL      WHITE            1     PENDANT   \n",
       "6775                   PINK   PARTIAL      WHITE            1  EVANESCENT   \n",
       "4264                   PINK   PARTIAL      WHITE            1       LARGE   \n",
       "90                    WHITE   PARTIAL      WHITE            1     PENDANT   \n",
       "7114                  WHITE   PARTIAL      WHITE            1  EVANESCENT   \n",
       "1341                  WHITE   PARTIAL      WHITE            1  EVANESCENT   \n",
       "4044                  WHITE   PARTIAL      WHITE            1     PENDANT   \n",
       "443                   WHITE   PARTIAL      WHITE            1     PENDANT   \n",
       "728                   WHITE   PARTIAL      WHITE            1     PENDANT   \n",
       "6466                  WHITE   PARTIAL      WHITE            1  EVANESCENT   \n",
       "1106                  WHITE   PARTIAL      WHITE            1     PENDANT   \n",
       "6634                  WHITE   PARTIAL      WHITE            1  EVANESCENT   \n",
       "4364                  BROWN   PARTIAL      WHITE            1       LARGE   \n",
       "5390                  BROWN   PARTIAL      WHITE            1       LARGE   \n",
       "150                   WHITE   PARTIAL      WHITE            1     PENDANT   \n",
       "5766                  WHITE   PARTIAL      WHITE            2     PENDANT   \n",
       "1627                  WHITE   PARTIAL      WHITE            1  EVANESCENT   \n",
       "4508                  BROWN   PARTIAL      WHITE            1       LARGE   \n",
       "3219                   GRAY   PARTIAL      WHITE            1     PENDANT   \n",
       "\n",
       "     spore-print-color population  habitat  \n",
       "1481             BLACK   ABUNDANT  GRASSES  \n",
       "965              BROWN  SCATTERED  GRASSES  \n",
       "6775             WHITE    SEVERAL    WOODS  \n",
       "4264         CHOCOLATE   SOLITARY  GRASSES  \n",
       "90              PURPLE    SEVERAL    WOODS  \n",
       "7114             WHITE    SEVERAL    WOODS  \n",
       "1341             BLACK   ABUNDANT  GRASSES  \n",
       "4044             BLACK  SCATTERED    WOODS  \n",
       "443              BLACK   NUMEROUS  GRASSES  \n",
       "728              BLACK   SOLITARY  GRASSES  \n",
       "6466             WHITE    SEVERAL    WOODS  \n",
       "1106             BLACK    SEVERAL    URBAN  \n",
       "6634             WHITE    SEVERAL    WOODS  \n",
       "4364         CHOCOLATE    SEVERAL    PATHS  \n",
       "5390         CHOCOLATE    SEVERAL    PATHS  \n",
       "150              BROWN   NUMEROUS  MEADOWS  \n",
       "5766             GREEN    SEVERAL  MEADOWS  \n",
       "1627             BROWN   ABUNDANT  GRASSES  \n",
       "4508         CHOCOLATE    SEVERAL    PATHS  \n",
       "3219             BROWN    SEVERAL    WOODS  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "X.sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1f1f5e8-eef6-4bdf-8723-8493738022b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, columns=ohe_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4440bc93-b0b9-408e-acce-f5abaf0fe7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8416, 109)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brusies</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>stalk-shape</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>cap-shape_BELL</th>\n",
       "      <th>cap-shape_CONICAL</th>\n",
       "      <th>cap-shape_CONVEX</th>\n",
       "      <th>cap-shape_FLAT</th>\n",
       "      <th>cap-shape_KNOBBED</th>\n",
       "      <th>cap-shape_SUNKEN</th>\n",
       "      <th>cap-surface_FIBROUS</th>\n",
       "      <th>cap-surface_GROOVES</th>\n",
       "      <th>cap-surface_SCALY</th>\n",
       "      <th>cap-surface_SMOOTH</th>\n",
       "      <th>cap-color_BROWN</th>\n",
       "      <th>cap-color_BUFF</th>\n",
       "      <th>cap-color_CINNAMON</th>\n",
       "      <th>cap-color_GRAY</th>\n",
       "      <th>cap-color_GREEN</th>\n",
       "      <th>cap-color_PINK</th>\n",
       "      <th>cap-color_PURPLE</th>\n",
       "      <th>cap-color_RED</th>\n",
       "      <th>cap-color_WHITE</th>\n",
       "      <th>cap-color_YELLOW</th>\n",
       "      <th>odor_ALMOND</th>\n",
       "      <th>odor_ANISE</th>\n",
       "      <th>odor_CREOSOTE</th>\n",
       "      <th>odor_FISHY</th>\n",
       "      <th>odor_FOUL</th>\n",
       "      <th>odor_MUSTY</th>\n",
       "      <th>odor_NONE</th>\n",
       "      <th>odor_PUNGENT</th>\n",
       "      <th>odor_SPICY</th>\n",
       "      <th>gill-color_BLACK</th>\n",
       "      <th>gill-color_BROWN</th>\n",
       "      <th>gill-color_BUFF</th>\n",
       "      <th>gill-color_CHOCOLATE</th>\n",
       "      <th>gill-color_GRAY</th>\n",
       "      <th>gill-color_GREEN</th>\n",
       "      <th>gill-color_ORANGE</th>\n",
       "      <th>gill-color_PINK</th>\n",
       "      <th>gill-color_PURPLE</th>\n",
       "      <th>gill-color_RED</th>\n",
       "      <th>gill-color_WHITE</th>\n",
       "      <th>gill-color_YELLOW</th>\n",
       "      <th>stalk-root_BULBOUS</th>\n",
       "      <th>stalk-root_CLUB</th>\n",
       "      <th>stalk-root_EQUAL</th>\n",
       "      <th>stalk-root_ROOTED</th>\n",
       "      <th>stalk-surface-above-ring_FIBROUS</th>\n",
       "      <th>stalk-surface-above-ring_SCALY</th>\n",
       "      <th>stalk-surface-above-ring_SILKY</th>\n",
       "      <th>stalk-surface-above-ring_SMOOTH</th>\n",
       "      <th>stalk-surface-below-ring_FIBROUS</th>\n",
       "      <th>stalk-surface-below-ring_SCALY</th>\n",
       "      <th>stalk-surface-below-ring_SILKY</th>\n",
       "      <th>stalk-surface-below-ring_SMOOTH</th>\n",
       "      <th>stalk-color-above-ring_BROWN</th>\n",
       "      <th>stalk-color-above-ring_BUFF</th>\n",
       "      <th>stalk-color-above-ring_CINNAMON</th>\n",
       "      <th>stalk-color-above-ring_GRAY</th>\n",
       "      <th>stalk-color-above-ring_ORANGE</th>\n",
       "      <th>stalk-color-above-ring_PINK</th>\n",
       "      <th>stalk-color-above-ring_RED</th>\n",
       "      <th>stalk-color-above-ring_WHITE</th>\n",
       "      <th>stalk-color-above-ring_YELLOW</th>\n",
       "      <th>stalk-color-below-ring_BROWN</th>\n",
       "      <th>stalk-color-below-ring_BUFF</th>\n",
       "      <th>stalk-color-below-ring_CINNAMON</th>\n",
       "      <th>stalk-color-below-ring_GRAY</th>\n",
       "      <th>stalk-color-below-ring_ORANGE</th>\n",
       "      <th>stalk-color-below-ring_PINK</th>\n",
       "      <th>stalk-color-below-ring_RED</th>\n",
       "      <th>stalk-color-below-ring_WHITE</th>\n",
       "      <th>stalk-color-below-ring_YELLOW</th>\n",
       "      <th>veil-type_PARTIAL</th>\n",
       "      <th>veil-color_BROWN</th>\n",
       "      <th>veil-color_ORANGE</th>\n",
       "      <th>veil-color_WHITE</th>\n",
       "      <th>veil-color_YELLOW</th>\n",
       "      <th>ring-type_EVANESCENT</th>\n",
       "      <th>ring-type_FLARING</th>\n",
       "      <th>ring-type_LARGE</th>\n",
       "      <th>ring-type_NONE</th>\n",
       "      <th>ring-type_PENDANT</th>\n",
       "      <th>spore-print-color_BLACK</th>\n",
       "      <th>spore-print-color_BROWN</th>\n",
       "      <th>spore-print-color_BUFF</th>\n",
       "      <th>spore-print-color_CHOCOLATE</th>\n",
       "      <th>spore-print-color_GREEN</th>\n",
       "      <th>spore-print-color_ORANGE</th>\n",
       "      <th>spore-print-color_PURPLE</th>\n",
       "      <th>spore-print-color_WHITE</th>\n",
       "      <th>spore-print-color_YELLOW</th>\n",
       "      <th>population_ABUNDANT</th>\n",
       "      <th>population_CLUSTERED</th>\n",
       "      <th>population_NUMEROUS</th>\n",
       "      <th>population_SCATTERED</th>\n",
       "      <th>population_SEVERAL</th>\n",
       "      <th>population_SOLITARY</th>\n",
       "      <th>habitat_GRASSES</th>\n",
       "      <th>habitat_LEAVES</th>\n",
       "      <th>habitat_MEADOWS</th>\n",
       "      <th>habitat_PATHS</th>\n",
       "      <th>habitat_URBAN</th>\n",
       "      <th>habitat_WASTE</th>\n",
       "      <th>habitat_WOODS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4933</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7476</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3490</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3129</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      brusies  gill-attachment  gill-spacing  gill-size  stalk-shape  \\\n",
       "4933        0                1             0          0            0   \n",
       "7476        0                1             0          1            1   \n",
       "3490        1                1             0          0            1   \n",
       "2897        1                1             0          0            1   \n",
       "3129        1                1             0          0            1   \n",
       "\n",
       "      ring-number  cap-shape_BELL  cap-shape_CONICAL  cap-shape_CONVEX  \\\n",
       "4933            1               0                  0                 0   \n",
       "7476            1               0                  0                 0   \n",
       "3490            1               0                  0                 0   \n",
       "2897            1               0                  0                 1   \n",
       "3129            1               0                  0                 0   \n",
       "\n",
       "      cap-shape_FLAT  cap-shape_KNOBBED  cap-shape_SUNKEN  \\\n",
       "4933               1                  0                 0   \n",
       "7476               0                  1                 0   \n",
       "3490               1                  0                 0   \n",
       "2897               0                  0                 0   \n",
       "3129               1                  0                 0   \n",
       "\n",
       "      cap-surface_FIBROUS  cap-surface_GROOVES  cap-surface_SCALY  \\\n",
       "4933                    1                    0                  0   \n",
       "7476                    0                    0                  0   \n",
       "3490                    0                    0                  1   \n",
       "2897                    0                    0                  1   \n",
       "3129                    1                    0                  0   \n",
       "\n",
       "      cap-surface_SMOOTH  cap-color_BROWN  cap-color_BUFF  cap-color_CINNAMON  \\\n",
       "4933                   0                0               0                   0   \n",
       "7476                   1                1               0                   0   \n",
       "3490                   0                0               0                   0   \n",
       "2897                   0                1               0                   0   \n",
       "3129                   0                0               0                   0   \n",
       "\n",
       "      cap-color_GRAY  cap-color_GREEN  cap-color_PINK  cap-color_PURPLE  \\\n",
       "4933               0                0               0                 0   \n",
       "7476               0                0               0                 0   \n",
       "3490               1                0               0                 0   \n",
       "2897               0                0               0                 0   \n",
       "3129               1                0               0                 0   \n",
       "\n",
       "      cap-color_RED  cap-color_WHITE  cap-color_YELLOW  odor_ALMOND  \\\n",
       "4933              0                0                 1            0   \n",
       "7476              0                0                 0            0   \n",
       "3490              0                0                 0            0   \n",
       "2897              0                0                 0            0   \n",
       "3129              0                0                 0            0   \n",
       "\n",
       "      odor_ANISE  odor_CREOSOTE  odor_FISHY  odor_FOUL  odor_MUSTY  odor_NONE  \\\n",
       "4933           0              0           0          1           0          0   \n",
       "7476           0              0           1          0           0          0   \n",
       "3490           0              0           0          0           0          1   \n",
       "2897           0              0           0          0           0          1   \n",
       "3129           0              0           0          0           0          1   \n",
       "\n",
       "      odor_PUNGENT  odor_SPICY  gill-color_BLACK  gill-color_BROWN  \\\n",
       "4933             0           0                 0                 0   \n",
       "7476             0           0                 0                 0   \n",
       "3490             0           0                 0                 0   \n",
       "2897             0           0                 0                 0   \n",
       "3129             0           0                 0                 0   \n",
       "\n",
       "      gill-color_BUFF  gill-color_CHOCOLATE  gill-color_GRAY  \\\n",
       "4933                0                     0                1   \n",
       "7476                1                     0                0   \n",
       "3490                0                     0                0   \n",
       "2897                0                     0                0   \n",
       "3129                0                     0                0   \n",
       "\n",
       "      gill-color_GREEN  gill-color_ORANGE  gill-color_PINK  gill-color_PURPLE  \\\n",
       "4933                 0                  0                0                  0   \n",
       "7476                 0                  0                0                  0   \n",
       "3490                 0                  0                0                  0   \n",
       "2897                 0                  0                0                  0   \n",
       "3129                 0                  0                0                  1   \n",
       "\n",
       "      gill-color_RED  gill-color_WHITE  gill-color_YELLOW  stalk-root_BULBOUS  \\\n",
       "4933               0                 0                  0                   1   \n",
       "7476               0                 0                  0                   1   \n",
       "3490               0                 1                  0                   1   \n",
       "2897               0                 1                  0                   1   \n",
       "3129               0                 0                  0                   1   \n",
       "\n",
       "      stalk-root_CLUB  stalk-root_EQUAL  stalk-root_ROOTED  \\\n",
       "4933                0                 0                  0   \n",
       "7476                0                 0                  0   \n",
       "3490                0                 0                  0   \n",
       "2897                0                 0                  0   \n",
       "3129                0                 0                  0   \n",
       "\n",
       "      stalk-surface-above-ring_FIBROUS  stalk-surface-above-ring_SCALY  \\\n",
       "4933                                 0                               0   \n",
       "7476                                 0                               0   \n",
       "3490                                 0                               0   \n",
       "2897                                 0                               0   \n",
       "3129                                 0                               0   \n",
       "\n",
       "      stalk-surface-above-ring_SILKY  stalk-surface-above-ring_SMOOTH  \\\n",
       "4933                               1                                0   \n",
       "7476                               1                                0   \n",
       "3490                               0                                1   \n",
       "2897                               0                                1   \n",
       "3129                               0                                1   \n",
       "\n",
       "      stalk-surface-below-ring_FIBROUS  stalk-surface-below-ring_SCALY  \\\n",
       "4933                                 0                               0   \n",
       "7476                                 0                               0   \n",
       "3490                                 0                               0   \n",
       "2897                                 0                               0   \n",
       "3129                                 0                               0   \n",
       "\n",
       "      stalk-surface-below-ring_SILKY  stalk-surface-below-ring_SMOOTH  \\\n",
       "4933                               1                                0   \n",
       "7476                               0                                1   \n",
       "3490                               0                                1   \n",
       "2897                               0                                1   \n",
       "3129                               0                                1   \n",
       "\n",
       "      stalk-color-above-ring_BROWN  stalk-color-above-ring_BUFF  \\\n",
       "4933                             0                            0   \n",
       "7476                             0                            0   \n",
       "3490                             0                            0   \n",
       "2897                             0                            0   \n",
       "3129                             0                            0   \n",
       "\n",
       "      stalk-color-above-ring_CINNAMON  stalk-color-above-ring_GRAY  \\\n",
       "4933                                0                            0   \n",
       "7476                                0                            0   \n",
       "3490                                0                            0   \n",
       "2897                                0                            0   \n",
       "3129                                0                            0   \n",
       "\n",
       "      stalk-color-above-ring_ORANGE  stalk-color-above-ring_PINK  \\\n",
       "4933                              0                            1   \n",
       "7476                              0                            0   \n",
       "3490                              0                            1   \n",
       "2897                              0                            0   \n",
       "3129                              0                            1   \n",
       "\n",
       "      stalk-color-above-ring_RED  stalk-color-above-ring_WHITE  \\\n",
       "4933                           0                             0   \n",
       "7476                           0                             1   \n",
       "3490                           0                             0   \n",
       "2897                           0                             1   \n",
       "3129                           0                             0   \n",
       "\n",
       "      stalk-color-above-ring_YELLOW  stalk-color-below-ring_BROWN  \\\n",
       "4933                              0                             0   \n",
       "7476                              0                             0   \n",
       "3490                              0                             0   \n",
       "2897                              0                             0   \n",
       "3129                              0                             0   \n",
       "\n",
       "      stalk-color-below-ring_BUFF  stalk-color-below-ring_CINNAMON  \\\n",
       "4933                            0                                0   \n",
       "7476                            0                                0   \n",
       "3490                            0                                0   \n",
       "2897                            0                                0   \n",
       "3129                            0                                0   \n",
       "\n",
       "      stalk-color-below-ring_GRAY  stalk-color-below-ring_ORANGE  \\\n",
       "4933                            0                              0   \n",
       "7476                            0                              0   \n",
       "3490                            0                              0   \n",
       "2897                            0                              0   \n",
       "3129                            0                              0   \n",
       "\n",
       "      stalk-color-below-ring_PINK  stalk-color-below-ring_RED  \\\n",
       "4933                            1                           0   \n",
       "7476                            1                           0   \n",
       "3490                            1                           0   \n",
       "2897                            0                           0   \n",
       "3129                            1                           0   \n",
       "\n",
       "      stalk-color-below-ring_WHITE  stalk-color-below-ring_YELLOW  \\\n",
       "4933                             0                              0   \n",
       "7476                             0                              0   \n",
       "3490                             0                              0   \n",
       "2897                             1                              0   \n",
       "3129                             0                              0   \n",
       "\n",
       "      veil-type_PARTIAL  veil-color_BROWN  veil-color_ORANGE  \\\n",
       "4933                  1                 0                  0   \n",
       "7476                  1                 0                  0   \n",
       "3490                  1                 0                  0   \n",
       "2897                  1                 0                  0   \n",
       "3129                  1                 0                  0   \n",
       "\n",
       "      veil-color_WHITE  veil-color_YELLOW  ring-type_EVANESCENT  \\\n",
       "4933                 1                  0                     0   \n",
       "7476                 1                  0                     1   \n",
       "3490                 1                  0                     0   \n",
       "2897                 1                  0                     0   \n",
       "3129                 1                  0                     0   \n",
       "\n",
       "      ring-type_FLARING  ring-type_LARGE  ring-type_NONE  ring-type_PENDANT  \\\n",
       "4933                  0                1               0                  0   \n",
       "7476                  0                0               0                  0   \n",
       "3490                  0                0               0                  1   \n",
       "2897                  0                0               0                  1   \n",
       "3129                  0                0               0                  1   \n",
       "\n",
       "      spore-print-color_BLACK  spore-print-color_BROWN  \\\n",
       "4933                        0                        0   \n",
       "7476                        0                        0   \n",
       "3490                        0                        1   \n",
       "2897                        1                        0   \n",
       "3129                        1                        0   \n",
       "\n",
       "      spore-print-color_BUFF  spore-print-color_CHOCOLATE  \\\n",
       "4933                       0                            1   \n",
       "7476                       0                            0   \n",
       "3490                       0                            0   \n",
       "2897                       0                            0   \n",
       "3129                       0                            0   \n",
       "\n",
       "      spore-print-color_GREEN  spore-print-color_ORANGE  \\\n",
       "4933                        0                         0   \n",
       "7476                        0                         0   \n",
       "3490                        0                         0   \n",
       "2897                        0                         0   \n",
       "3129                        0                         0   \n",
       "\n",
       "      spore-print-color_PURPLE  spore-print-color_WHITE  \\\n",
       "4933                         0                        0   \n",
       "7476                         0                        1   \n",
       "3490                         0                        0   \n",
       "2897                         0                        0   \n",
       "3129                         0                        0   \n",
       "\n",
       "      spore-print-color_YELLOW  population_ABUNDANT  population_CLUSTERED  \\\n",
       "4933                         0                    0                     0   \n",
       "7476                         0                    0                     0   \n",
       "3490                         0                    0                     0   \n",
       "2897                         0                    0                     0   \n",
       "3129                         0                    0                     0   \n",
       "\n",
       "      population_NUMEROUS  population_SCATTERED  population_SEVERAL  \\\n",
       "4933                    0                     0                   1   \n",
       "7476                    0                     0                   1   \n",
       "3490                    0                     0                   0   \n",
       "2897                    0                     0                   1   \n",
       "3129                    0                     0                   1   \n",
       "\n",
       "      population_SOLITARY  habitat_GRASSES  habitat_LEAVES  habitat_MEADOWS  \\\n",
       "4933                    0                1               0                0   \n",
       "7476                    0                0               0                0   \n",
       "3490                    1                0               0                0   \n",
       "2897                    0                0               0                0   \n",
       "3129                    0                0               0                0   \n",
       "\n",
       "      habitat_PATHS  habitat_URBAN  habitat_WASTE  habitat_WOODS  \n",
       "4933              0              0              0              0  \n",
       "7476              1              0              0              0  \n",
       "3490              0              0              0              1  \n",
       "2897              0              0              0              1  \n",
       "3129              0              0              0              1  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e9ddcf0-5d7f-4f3f-a60d-66950daf7d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {'EDIBLE': 1, 'POISONOUS': 0}\n",
    "y = y.map(class_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07861aa9-358e-4fb1-b28e-0513e7d7766f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1236    1\n",
       "6007    1\n",
       "1340    1\n",
       "5030    0\n",
       "4054    0\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1066bb44-89f3-463d-8a4d-974a902073ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values\n",
    "y = y.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a027fbc-f4c1-4d4e-a996-af6a9b639885",
   "metadata": {},
   "source": [
    "## sklearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5195de8f-01ea-456c-85ef-75157a0f06ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7fc2191c-c934-4190-843f-7464f6623537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5726468-14b4-4b1d-93b8-3c9840ab51d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X=X_test,y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1bc84e2-b479-4448-b707-7d8545f91cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[101].reshape(1,-1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "10a041fb-0dc5-45e7-847f-10471e266083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[101]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb8f5da-ca9a-4659-a29e-17659e5d7476",
   "metadata": {},
   "source": [
    "## Своя модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73b07a33-4eb6-467b-8b1b-33975531be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = pd.read_csv(\"./mushroom/expanded.csv\")\n",
    "gdf.replace(\"?\", pd.NA, inplace=True)\n",
    "gdf['stalk-root'].fillna(gdf['stalk-root'].mode()[0], inplace=True)\n",
    "u =gdf.apply(lambda x: x.unique())\n",
    "X = gdf.drop(columns=['class'])\n",
    "y = gdf['class']\n",
    "cat_features = [x for x in X.columns]\n",
    "ohe_features = [x for x in X.columns]\n",
    "for col in cat_features:\n",
    "    if (len(X[col].unique()) == 2):\n",
    "        X[col] = (X[col] == X[col][0]).astype(int)\n",
    "        ohe_features.remove(col)\n",
    "ring_mapping = {'NONE':0, 'ONE': 1, 'TWO': 2}\n",
    "X['ring-number'] = X['ring-number'].map(ring_mapping)\n",
    "ohe_features.remove('ring-number')\n",
    "X = pd.get_dummies(X, columns=ohe_features)\n",
    "class_mapping = {'EDIBLE': 1, 'POISONOUS': 0}\n",
    "y = y.map(class_mapping)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0a8ac0a-2452-464e-b6dc-d5b9a91b902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16872/2363718341.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['stalk-root'].fillna('m', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "mushrooms = fetch_ucirepo(id=73)\n",
    "df = mushrooms.data.features\n",
    "target = mushrooms.data.targets['poisonous']\n",
    "\n",
    "df['stalk-root'].fillna('m', inplace=True)\n",
    "X = df\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "X_encoded = encoder.fit_transform(X).toarray()\n",
    "y_encoded = target.apply(lambda x: 0 if x == 'p' else 1).values.reshape(-1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "565c5ffe-6d42-41c2-ba23-b53832151d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=65)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d571e004-e8e4-45b8-9135-4854dce42a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, loss: 20.266614852749907\n",
      "iteration: 1, loss: 14.78439984595087\n",
      "iteration: 2, loss: 11.151525208656643\n",
      "iteration: 3, loss: 8.67749092637354\n",
      "iteration: 4, loss: 6.943067880132247\n",
      "iteration: 5, loss: 5.711173377932786\n",
      "iteration: 6, loss: 4.832796701106078\n",
      "iteration: 7, loss: 4.190232022572929\n",
      "iteration: 8, loss: 3.7091214353071265\n",
      "iteration: 9, loss: 3.3372642010383307\n",
      "iteration: 10, loss: 3.0423071597627245\n",
      "iteration: 11, loss: 2.803449312204278\n",
      "iteration: 12, loss: 2.6061420962687887\n",
      "iteration: 13, loss: 2.4403700296579256\n",
      "iteration: 14, loss: 2.2993339919343305\n",
      "iteration: 15, loss: 2.1792598230905718\n",
      "iteration: 16, loss: 2.0755390891906584\n",
      "iteration: 17, loss: 1.9833469116087685\n",
      "iteration: 18, loss: 1.8996251780888176\n",
      "iteration: 19, loss: 1.826770942511411\n",
      "iteration: 20, loss: 1.7589323969783324\n",
      "iteration: 21, loss: 1.6972275086757853\n",
      "iteration: 22, loss: 1.6424829845152638\n",
      "iteration: 23, loss: 1.5892746259625028\n",
      "iteration: 24, loss: 1.5392386522207577\n",
      "iteration: 25, loss: 1.4934094264405942\n",
      "iteration: 26, loss: 1.4483987129992026\n",
      "iteration: 27, loss: 1.4091740615887554\n",
      "iteration: 28, loss: 1.368122851170983\n",
      "iteration: 29, loss: 1.3303216395476363\n",
      "iteration: 30, loss: 1.2951015979552172\n",
      "iteration: 31, loss: 1.259904807830881\n",
      "iteration: 32, loss: 1.2269796770881662\n",
      "iteration: 33, loss: 1.1947480402518058\n",
      "iteration: 34, loss: 1.1635812844585298\n",
      "iteration: 35, loss: 1.1347780208297513\n",
      "iteration: 36, loss: 1.1062623091695474\n",
      "iteration: 37, loss: 1.0788533067669894\n",
      "iteration: 38, loss: 1.052553476030825\n",
      "iteration: 39, loss: 1.0273366968704318\n",
      "iteration: 40, loss: 1.0030020288506056\n",
      "iteration: 41, loss: 0.9788685483332082\n",
      "iteration: 42, loss: 0.9565374666195396\n",
      "iteration: 43, loss: 0.9342105370214776\n",
      "iteration: 44, loss: 0.9135418747130432\n",
      "iteration: 45, loss: 0.8924049943530195\n",
      "iteration: 46, loss: 0.8730877469845383\n",
      "iteration: 47, loss: 0.8539667097488797\n",
      "iteration: 48, loss: 0.8355522223425829\n",
      "iteration: 49, loss: 0.8180620226374554\n",
      "iteration: 50, loss: 0.8001748404713348\n",
      "iteration: 51, loss: 0.7837743330348721\n",
      "iteration: 52, loss: 0.768306996123133\n",
      "iteration: 53, loss: 0.7524454740829949\n",
      "iteration: 54, loss: 0.7376206323041693\n",
      "iteration: 55, loss: 0.7230568456165918\n",
      "iteration: 56, loss: 0.708431606380267\n",
      "iteration: 57, loss: 0.6947677565457988\n",
      "iteration: 58, loss: 0.6820205075800166\n",
      "iteration: 59, loss: 0.6691882418234096\n",
      "iteration: 60, loss: 0.6567434272304216\n",
      "iteration: 61, loss: 0.6450265857169968\n",
      "iteration: 62, loss: 0.6326131498633485\n",
      "iteration: 63, loss: 0.6219158105829786\n",
      "iteration: 64, loss: 0.6110986852734333\n",
      "iteration: 65, loss: 0.6002401143001587\n",
      "iteration: 66, loss: 0.5899754622447022\n",
      "iteration: 67, loss: 0.5799514421506513\n",
      "iteration: 68, loss: 0.5702260523751633\n",
      "iteration: 69, loss: 0.5609837469502291\n",
      "iteration: 70, loss: 0.5514809784154827\n",
      "iteration: 71, loss: 0.5428816617038239\n",
      "iteration: 72, loss: 0.5339685806649154\n",
      "iteration: 73, loss: 0.525798952536986\n",
      "iteration: 74, loss: 0.5175089002598308\n",
      "iteration: 75, loss: 0.5091815583384129\n",
      "iteration: 76, loss: 0.5015413252202182\n",
      "iteration: 77, loss: 0.49405858799383195\n",
      "iteration: 78, loss: 0.4870103260256585\n",
      "iteration: 79, loss: 0.47958106516589727\n",
      "iteration: 80, loss: 0.47243929489059006\n",
      "iteration: 81, loss: 0.46569167391663513\n",
      "iteration: 82, loss: 0.45898183064027953\n",
      "iteration: 83, loss: 0.45258788824830704\n",
      "iteration: 84, loss: 0.44644130301793755\n",
      "iteration: 85, loss: 0.44001287967680386\n",
      "iteration: 86, loss: 0.434117825149265\n",
      "iteration: 87, loss: 0.42807249430574984\n",
      "iteration: 88, loss: 0.42243785328536704\n",
      "iteration: 89, loss: 0.41672310432254117\n",
      "iteration: 90, loss: 0.41114346171938554\n",
      "iteration: 91, loss: 0.4056154137370877\n",
      "iteration: 92, loss: 0.4002471294538238\n",
      "iteration: 93, loss: 0.3955810645426769\n",
      "iteration: 94, loss: 0.39065847985566177\n",
      "iteration: 95, loss: 0.3856558459360942\n",
      "iteration: 96, loss: 0.3806767353791114\n",
      "iteration: 97, loss: 0.3762977352380143\n",
      "iteration: 98, loss: 0.37159285938569275\n",
      "iteration: 99, loss: 0.36716651123156796\n"
     ]
    }
   ],
   "source": [
    "model = MLP([\n",
    "    Linear(X_train.shape[1], 100),\n",
    "    Sigmoid(),\n",
    "    Linear(100, 1),\n",
    "    Sigmoid()\n",
    "], learning_rate=0.05, max_iter=100)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bfc7076e-7942-4578-a5be-a8f495d5c463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709916909500949"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_exp = y_test.T[0]\n",
    "y_pred = model.predict(X_test).T[0]\n",
    "model.score(y_exp, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab3409-c341-4dec-8768-aa25a4a315a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
